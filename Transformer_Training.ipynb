{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5570e85a-9550-4d6d-829c-b540045e1d31",
   "metadata": {},
   "source": [
    "# Text Summarization with CNN/DailyMail Dataset: Data & Task Overview\r\n",
    "\r\n",
    "## The Original Task: Tweet Summarization\r\n",
    "The original goal of this project is to develop a transformer model from scratch for abstractive summarization of tweets. However, a significant challenge in this domain is the lack of large-scale, high-quality datasets pairing tweets with their summaries.\r\n",
    "\r\n",
    "## Using CNN/DailyMail as the Base Dataset\r\n",
    "Due to the unavailability of a large twitter summarization dataset, we've chosen the CNN/DailyMail dataset to train our base model. This dataset consists of news articles from CNN and Daily Mail websites paired with human-written summaries (called \"highlights\"). These summaries are bullet-point style abstracts created by professional editors.\r\n",
    "\r\n",
    "Key statistics:\r\n",
    "- Training set: 287,113 article-summary pairs\r\n",
    "- Validation set: 13,368 article-summary pairs\r\n",
    "- Test set: 11,490 article-summary pairs\r\n",
    "\r\n",
    "## The Task - Abstractive Text Summarization\r\n",
    "This project implements an **abstractive summarization** model using a Transformer architecture. Unlike extractive summarization (which selects sentences from the source text), abstractive summarization generates new text that captures the essential information from the source document.\r\n",
    "\r\n",
    "## Why CNN/DailyMail is Suitable\r\n",
    "1. **Real-world application**: News summarization is a practical task with commercial applications.\r\n",
    "2. **Well-defined task**: The highlights are professionally written and follow consistent patterns.\r\n",
    "3. **Appropriate length ratio**: Articles are typically several paragraphs (400-800 words), while summaries are 3-4 bullet points (around 40-70 words), providing a reasonable compression ratio.\r\n",
    "4. **Diverse topics**: The dataset covers a wide range of news topics, making models trained on it more generalizable.\r\n",
    "5. **Multiple references**: Each article has multiple highlight points, allowing evaluation of different aspects of summarization.\r\n",
    "\r\n",
    "Our plan is to first develop and train a strong base model on this dataset, which can later be adapted for the tweet summarization task. efficient and effective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acda0ac5-f24c-4409-96e0-159443deec82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\nisha\\anaconda3\\envs\\nlp\\lib\\site-packages (3.5.0)\n",
      "Requirement already satisfied: transformers in c:\\users\\nisha\\anaconda3\\envs\\nlp\\lib\\site-packages (4.50.3)\n",
      "Requirement already satisfied: tokenizers in c:\\users\\nisha\\anaconda3\\envs\\nlp\\lib\\site-packages (0.21.1)\n",
      "Requirement already satisfied: torch in c:\\users\\nisha\\anaconda3\\envs\\nlp\\lib\\site-packages (2.6.0+cu126)\n",
      "Requirement already satisfied: nltk in c:\\users\\nisha\\anaconda3\\envs\\nlp\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: rouge_score in c:\\users\\nisha\\anaconda3\\envs\\nlp\\lib\\site-packages (0.1.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\nisha\\anaconda3\\envs\\nlp\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\nisha\\anaconda3\\envs\\nlp\\lib\\site-packages (2.1.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\nisha\\anaconda3\\envs\\nlp\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: wandb in c:\\users\\nisha\\anaconda3\\envs\\nlp\\lib\\site-packages (0.19.9)\n",
      "Requirement already satisfied: tensorboardX in c:\\users\\nisha\\anaconda3\\envs\\nlp\\lib\\site-packages (2.6.2.2)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\nisha\\anaconda3\\envs\\nlp\\lib\\site-packages (0.2.0)\n",
      "Requirement already satisfied: einops in c:\\users\\nisha\\anaconda3\\envs\\nlp\\lib\\site-packages (0.8.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\nisha\\anaconda3\\envs\\nlp\\lib\\site-packages (3.10.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\nisha\\anaconda3\\envs\\nlp\\lib\\site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\nisha\\anaconda3\\envs\\nlp\\lib\\site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\nisha\\anaconda3\\envs\\nlp\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\nisha\\anaconda3\\envs\\nlp\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: xxhash in c:\\users\\nisha\\anaconda3\\envs\\nlp\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\nisha\\anaconda3\\envs\\nlp\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in c:\\users\\nisha\\anaconda3\\envs\\nlp\\lib\\site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\nisha\\anaconda3\\envs\\nlp\\lib\\site-packages (from datasets) (3.11.16)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in c:\\users\\nisha\\anaconda3\\envs\\nlp\\lib\\site-packages (from datasets) (0.30.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\nisha\\anaconda3\\envs\\nlp\\lib\\site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\nisha\\anaconda3\\envs\\nlp\\lib\\site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\nisha\\anaconda3\\envs\\nlp\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\nisha\\anaconda3\\envs\\nlp\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\nisha\\anaconda3\\envs\\nlp\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\nisha\\anaconda3\\envs\\nlp\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\nisha\\anaconda3\\envs\\nlp\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\nisha\\anaconda3\\envs\\nlp\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\nisha\\anaconda3\\envs\\nlp\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: click in c:\\users\\nisha\\anaconda3\\envs\\nlp\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\nisha\\anaconda3\\envs\\nlp\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: absl-py in c:\\users\\nisha\\anaconda3\\envs\\nlp\\lib\\site-packages (from rouge_score) (2.2.1)\n",
      "Requirement already satisfied: six>=1.14.0 in c:\\users\\nisha\\anaconda3\\envs\\nlp\\lib\\site-packages (from rouge_score) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\nisha\\anaconda3\\envs\\nlp\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\nisha\\anaconda3\\envs\\nlp\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\nisha\\anaconda3\\envs\\nlp\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\nisha\\anaconda3\\envs\\nlp\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in c:\\users\\nisha\\anaconda3\\envs\\nlp\\lib\\site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in c:\\users\\nisha\\anaconda3\\envs\\nlp\\lib\\site-packages (from wandb) (3.1.44)\n",
      "Requirement already satisfied: platformdirs in c:\\users\\nisha\\anaconda3\\envs\\nlp\\lib\\site-packages (from wandb) (3.10.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in c:\\users\\nisha\\anaconda3\\envs\\nlp\\lib\\site-packages (from wandb) (5.29.4)\n",
      "Requirement already satisfied: psutil>=5.0.0 in c:\\users\\nisha\\anaconda3\\envs\\nlp\\lib\\site-packages (from wandb) (5.9.0)\n",
      "Requirement already satisfied: pydantic<3 in c:\\users\\nisha\\anaconda3\\envs\\nlp\\lib\\site-packages (from wandb) (2.11.3)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in c:\\users\\nisha\\anaconda3\\envs\\nlp\\lib\\site-packages (from wandb) (2.26.1)\n",
      "Requirement already satisfied: setproctitle in c:\\users\\nisha\\anaconda3\\envs\\nlp\\lib\\site-packages (from wandb) (1.3.5)\n",
      "Requirement already satisfied: setuptools in c:\\users\\nisha\\anaconda3\\envs\\nlp\\lib\\site-packages (from wandb) (75.8.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\nisha\\anaconda3\\envs\\nlp\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\nisha\\anaconda3\\envs\\nlp\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\nisha\\anaconda3\\envs\\nlp\\lib\\site-packages (from matplotlib) (4.55.8)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\nisha\\anaconda3\\envs\\nlp\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\nisha\\anaconda3\\envs\\nlp\\lib\\site-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\nisha\\anaconda3\\envs\\nlp\\lib\\site-packages (from matplotlib) (3.2.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\nisha\\anaconda3\\envs\\nlp\\lib\\site-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\nisha\\anaconda3\\envs\\nlp\\lib\\site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\nisha\\anaconda3\\envs\\nlp\\lib\\site-packages (from aiohttp->datasets) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\nisha\\anaconda3\\envs\\nlp\\lib\\site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\nisha\\anaconda3\\envs\\nlp\\lib\\site-packages (from aiohttp->datasets) (6.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\nisha\\anaconda3\\envs\\nlp\\lib\\site-packages (from aiohttp->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\nisha\\anaconda3\\envs\\nlp\\lib\\site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\nisha\\anaconda3\\envs\\nlp\\lib\\site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\nisha\\anaconda3\\envs\\nlp\\lib\\site-packages (from pydantic<3->wandb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in c:\\users\\nisha\\anaconda3\\envs\\nlp\\lib\\site-packages (from pydantic<3->wandb) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\nisha\\anaconda3\\envs\\nlp\\lib\\site-packages (from pydantic<3->wandb) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\nisha\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nisha\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nisha\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nisha\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\nisha\\anaconda3\\envs\\nlp\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\nisha\\anaconda3\\envs\\nlp\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
      "Requirement already satisfied: accelerate in c:\\users\\nisha\\anaconda3\\envs\\nlp\\lib\\site-packages (1.6.0)\n",
      "Requirement already satisfied: rouge in c:\\users\\nisha\\anaconda3\\envs\\nlp\\lib\\site-packages (1.0.1)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in c:\\users\\nisha\\anaconda3\\envs\\nlp\\lib\\site-packages (from accelerate) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\nisha\\anaconda3\\envs\\nlp\\lib\\site-packages (from accelerate) (24.2)\n",
      "Requirement already satisfied: psutil in c:\\users\\nisha\\anaconda3\\envs\\nlp\\lib\\site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\nisha\\anaconda3\\envs\\nlp\\lib\\site-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in c:\\users\\nisha\\anaconda3\\envs\\nlp\\lib\\site-packages (from accelerate) (2.6.0+cu126)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in c:\\users\\nisha\\anaconda3\\envs\\nlp\\lib\\site-packages (from accelerate) (0.30.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\nisha\\anaconda3\\envs\\nlp\\lib\\site-packages (from accelerate) (0.5.3)\n",
      "Requirement already satisfied: six in c:\\users\\nisha\\anaconda3\\envs\\nlp\\lib\\site-packages (from rouge) (1.16.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\nisha\\anaconda3\\envs\\nlp\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\nisha\\anaconda3\\envs\\nlp\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (2024.6.1)\n",
      "Requirement already satisfied: requests in c:\\users\\nisha\\anaconda3\\envs\\nlp\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\nisha\\anaconda3\\envs\\nlp\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\nisha\\anaconda3\\envs\\nlp\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\nisha\\anaconda3\\envs\\nlp\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\nisha\\anaconda3\\envs\\nlp\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\nisha\\anaconda3\\envs\\nlp\\lib\\site-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\nisha\\anaconda3\\envs\\nlp\\lib\\site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\nisha\\anaconda3\\envs\\nlp\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.21.0->accelerate) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\nisha\\anaconda3\\envs\\nlp\\lib\\site-packages (from jinja2->torch>=2.0.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\nisha\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nisha\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nisha\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nisha\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2025.1.31)\n"
     ]
    }
   ],
   "source": [
    "# Install Required Packages\n",
    "!pip install datasets transformers tokenizers torch nltk rouge_score pandas numpy tqdm wandb tensorboardX sentencepiece einops matplotlib\n",
    "!pip install accelerate rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "847223a3-8b9f-4cb9-bd2d-452c4c7c675f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Setup\n",
    "import os\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from tokenizers import ByteLevelBPETokenizer, Tokenizer\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "import wandb  # Optional for experiment tracking\n",
    "from tensorboardX import SummaryWriter\n",
    "import logging\n",
    "import regex as re\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.cuda.amp import autocast, GradScaler  # For mixed precision training\n",
    "from torch.nn.utils import clip_grad_norm_  # For gradient clipping\n",
    "import torch.cuda.amp as amp  # For mixed precision\n",
    "from torch.utils.checkpoint import checkpoint  # For gradient checkpointing\n",
    "import time  # For timing training\n",
    "import json  # For saving configs\n",
    "import matplotlib.pyplot as plt  # For plotting\n",
    "import shutil  # For disk usage information\n",
    "import random  # For seed setting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510d7943-fe41-450a-b817-7d996f161abc",
   "metadata": {},
   "source": [
    "## Random Seed Initialization\n",
    "\n",
    "The code below sets fixed random seeds across all libraries used in this project. This is critical for:\n",
    "\n",
    "- **Reproducibility**: Ensures the same results can be obtained across different runs\n",
    "- **Consistent evaluation**: Guarantees that the generated summaries remain consistent for proper analysis and comparison\n",
    "- **Reliable generation**: With our temperature-based sampling approach, fixed seeds ensure consistent token selection during text generation\n",
    "- **Deterministic behavior**: Makes debugging and validation possible by eliminating randomness as a variable\n",
    "\n",
    "For academic and research contexts, reproducibility is a fundamental requirement. Without these seeds, the model would produce different summaries each time, making proper analysis and comparison impossible.\n",
    "\n",
    "The `deterministic` and `benchmark` settings specifically configure CUDA operations to prioritize consistent results over performance optimizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8637b9d9-bd3d-41c5-9519-52900a115845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU available: NVIDIA GeForce RTX 3060 Laptop GPU\n",
      "Total GPU memory: 6.44 GB\n",
      "Disk space - Total: 372.8 GB, Used: 338.8 GB, Free: 34.0 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nisha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Set random seeds \n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Check GPU specs if available\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Total GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "\n",
    "# Check disk space\n",
    "total, used, free = shutil.disk_usage(\"/\")\n",
    "print(f\"Disk space - Total: {total/1e9:.1f} GB, Used: {used/1e9:.1f} GB, Free: {free/1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbc1e5e3-093e-4aa9-96e8-ed00107f5934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully!\n",
      "Dataset structure: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['article', 'highlights', 'id'],\n",
      "        num_rows: 287113\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['article', 'highlights', 'id'],\n",
      "        num_rows: 13368\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['article', 'highlights', 'id'],\n",
      "        num_rows: 11490\n",
      "    })\n",
      "})\n",
      "Number of training examples: 287113\n",
      "Number of validation examples: 13368\n",
      "Number of test examples: 11490\n",
      "\n",
      "Sample article (first 300 chars):\n",
      "LONDON, England (Reuters) -- Harry Potter star Daniel Radcliffe gains access to a reported £20 million ($41.1 million) fortune as he turns 18 on Monday, but he insists the money won't cast a spell on him. Daniel Radcliffe as Harry Potter in \"Harry Potter and the Order of the Phoenix\" To the disappoi...\n",
      "\n",
      "Sample highlights (summary):\n",
      "Harry Potter star Daniel Radcliffe gets £20M fortune as he turns 18 Monday .\n",
      "Young actor says he has no plans to fritter his cash away .\n",
      "Radcliffe's earnings from first five Potter films have been held in trust fund .\n"
     ]
    }
   ],
   "source": [
    "# Load Dataset\n",
    "# Load CNN/DailyMail dataset\n",
    "cnn_dailymail = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
    "\n",
    "# Print info\n",
    "print(\"Dataset loaded successfully!\")\n",
    "print(f\"Dataset structure: {cnn_dailymail}\")\n",
    "print(f\"Number of training examples: {len(cnn_dailymail['train'])}\")\n",
    "print(f\"Number of validation examples: {len(cnn_dailymail['validation'])}\")\n",
    "print(f\"Number of test examples: {len(cnn_dailymail['test'])}\")\n",
    "\n",
    "# Sample \n",
    "sample = cnn_dailymail['train'][0]\n",
    "print(\"\\nSample article (first 300 chars):\")\n",
    "print(sample['article'][:300] + \"...\")\n",
    "print(\"\\nSample highlights (summary):\")\n",
    "print(sample['highlights'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2d4efe-be7f-47a0-b559-50cb2adec59b",
   "metadata": {},
   "source": [
    "# Custom Tokenization Strategy for Abstractive Summarization\r\n",
    "\r\n",
    "## The Tokenization Architecture\r\n",
    "\r\n",
    "The project implements a custom `OptimizedTokenizer` class that wraps around Hugging Face's Rust-based tokenizers library instead of the more commonly used SentencePiece. This choice was deliberate and offers several advantages for our summarization task.\r\n",
    "\r\n",
    "### Key Components of the Implementation:\r\n",
    "\r\n",
    "1. **ByteLevelBPE Tokenizer**: The implementation uses Byte-Level Byte-Pair Encoding (BPE) algorithm for tokenization, which works by iteratively merging the most frequent pairs of bytes in the text.\r\n",
    "\r\n",
    "2. **Special Tokens Management**: Four special tokens are explicitly defined with assigned IDs:\r\n",
    "   - `<pad>` (ID 0): For padding sequences to uniform length\r\n",
    "   - `<sos>` (ID 1): Start of sequence marker\r\n",
    "   - `<eos>` (ID 2): End of sequence marker\r\n",
    "   - `<unk>` (ID 3): For handling unknown tokens\r\n",
    "\r\n",
    "3. **Fast Training Process**: The tokenizer is trained on a sampled subset of the data (around 200,000 texts including both articles and summaries).\r\n",
    "\r\n",
    "4. **Vocabulary Size**: A vocabulary size of 32,000 tokens is used, which is large enough to capture the diversity of news language.\r\n",
    "\r\n",
    "## Rationale for Avoiding SentencePiece\r\n",
    "\r\n",
    "The implementation deliberately avoids SentencePiece for several reasons:\r\n",
    "\r\n",
    "1. **Performance**: ByteLevelBPE with the Rust implementation provides significant performance gains during both training and inference compared to SentencePiece.\r\n",
    "\r\n",
    "2. **Efficiency**: Training the tokenizer takes minutes rather than hours, making development iterations faster.\r\n",
    "\r\n",
    "3. **Memory Usage**: The Rust-based implementation is more memory-efficient, allowing for processing larger batches during training.\r\n",
    "\r\n",
    "4. **Fine-grained Control**: The implementation provides explicit control over special token IDs, which is important for the transformer architecture.\r\n",
    "\r\n",
    "5. **HuggingFace Integration**: Using `PreTrainedTokenizerFast` wrapper ensures compatibility with the broader ecosystem.\r\n",
    "\r\n",
    "## Implementation Details\r\n",
    "\r\n",
    "The implementation includes methods for:\r\n",
    "- Training the tokenizer on a corpus (`train`)\r\n",
    "- Encoding single texts (`encode`)\r\n",
    "- Batch encoding multiple texts (`batch_encode`)\r\n",
    "- Decoding token IDs back to text (`decode`)\r\n",
    "- Saving and loading the tokenizer (`save`, `load`)\r\n",
    "\r\n",
    "When used in the pipeline, the tokenizer truncates inputs to 512 tokens and targets to 128 tokens, which aligns with the characteristics of CNN/DailyMail articles and summaries.\r\n",
    "\r\n",
    "For a text summarization task like ours, having an efficient tokenization process is crucial due to the large amount of text data being processed. The chosen approach optimizes for both training speed and runtime performance. both training speed and runtime performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8593af0c-2e2d-442c-a459-ba5d6ee33f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimized Tokenizer Class\n",
    "class OptimizedTokenizer:\n",
    "    def __init__(self, vocab_size=32000):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.tokenizer = None\n",
    "        self.special_tokens = [\"<pad>\", \"<sos>\", \"<eos>\", \"<unk>\"]\n",
    "        self.special_token_ids = {\n",
    "            \"<pad>\": 0,\n",
    "            \"<sos>\": 1,\n",
    "            \"<eos>\": 2,\n",
    "            \"<unk>\": 3\n",
    "        }\n",
    "        \n",
    "    def train(self, texts, model_prefix=\"tokenizer\", num_samples=None):\n",
    "        \"\"\"Train a ByteLevelBPE tokenizer (much faster than SentencePiece)\"\"\"\n",
    "        os.makedirs(model_prefix, exist_ok=True)\n",
    "        \n",
    "        # Sample texts to speed up training if needed\n",
    "        if num_samples and len(texts) > num_samples:\n",
    "            import random\n",
    "            random.seed(42)\n",
    "            texts = random.sample(texts, num_samples)\n",
    "        \n",
    "        # Write sample texts to file\n",
    "        corpus_path = \"corpus.txt\"\n",
    "        print(f\"Writing {len(texts)} texts to file...\")\n",
    "        with open(corpus_path, 'w', encoding='utf-8') as f:\n",
    "            for text in texts:\n",
    "                f.write(text + '\\n')\n",
    "        \n",
    "        # Initialize and train the tokenizer (Rust implementation - very fast)\n",
    "        print(\"Training tokenizer (this should take minutes, not hours)...\")\n",
    "        from tokenizers import Tokenizer\n",
    "        from tokenizers.models import BPE\n",
    "        from tokenizers.trainers import BpeTrainer\n",
    "        from tokenizers.pre_tokenizers import Whitespace\n",
    "        \n",
    "        # Create a new BPE tokenizer\n",
    "        tokenizer = Tokenizer(BPE(unk_token=\"<unk>\"))\n",
    "        tokenizer.pre_tokenizer = Whitespace()\n",
    "        \n",
    "        # Prepare the trainer\n",
    "        trainer = BpeTrainer(\n",
    "            vocab_size=self.vocab_size,\n",
    "            special_tokens=self.special_tokens,\n",
    "            min_frequency=2\n",
    "        )\n",
    "        \n",
    "        # Train the tokenizer\n",
    "        tokenizer.train(files=[corpus_path], trainer=trainer)\n",
    "        \n",
    "        # Save the tokenizer\n",
    "        tokenizer_path = os.path.join(model_prefix, \"tokenizer.json\")\n",
    "        tokenizer.save(tokenizer_path)\n",
    "        print(f\"Tokenizer saved to {tokenizer_path}\")\n",
    "        \n",
    "        # Load the tokenizer\n",
    "        from transformers import PreTrainedTokenizerFast\n",
    "        self.tokenizer = PreTrainedTokenizerFast(\n",
    "            tokenizer_file=tokenizer_path,\n",
    "            bos_token=\"<sos>\",\n",
    "            eos_token=\"<eos>\",\n",
    "            pad_token=\"<pad>\",\n",
    "            unk_token=\"<unk>\"\n",
    "        )\n",
    "        \n",
    "        # Set special token IDs explicitly\n",
    "        self.tokenizer.pad_token_id = 0\n",
    "        self.tokenizer.bos_token_id = 1\n",
    "        self.tokenizer.eos_token_id = 2\n",
    "        self.tokenizer.unk_token_id = 3\n",
    "        \n",
    "        # Clean up\n",
    "        if os.path.exists(corpus_path):\n",
    "            os.remove(corpus_path)\n",
    "            \n",
    "        print(f\"Tokenizer training complete!\")\n",
    "        print(f\"Vocabulary size: {self.tokenizer.vocab_size}\")\n",
    "        \n",
    "    def encode(self, text, max_length=None, padding=\"max_length\", truncation=True):\n",
    "        if self.tokenizer is None:\n",
    "            raise ValueError(\"Tokenizer not trained. Call train() first.\")\n",
    "        \n",
    "        # Use the HuggingFace tokenizer\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_length,\n",
    "            padding=padding,\n",
    "            truncation=truncation,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors=None\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"],\n",
    "            \"attention_mask\": encoding[\"attention_mask\"]\n",
    "        }\n",
    "    \n",
    "    def batch_encode(self, texts, max_length=None, padding=\"max_length\", truncation=True):\n",
    "        if self.tokenizer is None:\n",
    "            raise ValueError(\"Tokenizer not trained. Call train() first.\")\n",
    "        \n",
    "        # Batch encode\n",
    "        encodings = self.tokenizer(\n",
    "            texts,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_length,\n",
    "            padding=padding,\n",
    "            truncation=truncation,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors=None\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": encodings[\"input_ids\"],\n",
    "            \"attention_mask\": encodings[\"attention_mask\"]\n",
    "        }\n",
    "    \n",
    "    def decode(self, token_ids, skip_special_tokens=True):\n",
    "        if self.tokenizer is None:\n",
    "            raise ValueError(\"Tokenizer not trained. Call train() first.\")\n",
    "            \n",
    "        if isinstance(token_ids, torch.Tensor):\n",
    "            token_ids = token_ids.cpu().tolist()\n",
    "            \n",
    "        return self.tokenizer.decode(token_ids, skip_special_tokens=skip_special_tokens)\n",
    "    \n",
    "    def save(self, path):\n",
    "        if self.tokenizer is None:\n",
    "            raise ValueError(\"Tokenizer not trained. Call train() first.\")\n",
    "        \n",
    "        os.makedirs(path, exist_ok=True)\n",
    "        self.tokenizer.save_pretrained(path)\n",
    "        \n",
    "        # Save the special token mapping separately\n",
    "        with open(os.path.join(path, \"special_tokens.json\"), \"w\") as f:\n",
    "            json.dump(self.special_token_ids, f)\n",
    "    \n",
    "    def load(self, path):\n",
    "        from transformers import PreTrainedTokenizerFast\n",
    "        \n",
    "        tokenizer_path = os.path.join(path, \"tokenizer.json\")\n",
    "        if os.path.exists(tokenizer_path):\n",
    "            self.tokenizer = PreTrainedTokenizerFast(\n",
    "                tokenizer_file=tokenizer_path,\n",
    "                bos_token=\"<sos>\",\n",
    "                eos_token=\"<eos>\",\n",
    "                pad_token=\"<pad>\",\n",
    "                unk_token=\"<unk>\"\n",
    "            )\n",
    "        else:\n",
    "            # Try loading as a pretrained tokenizer\n",
    "            self.tokenizer = PreTrainedTokenizerFast.from_pretrained(path)\n",
    "        \n",
    "        # Load special token mapping if it exists\n",
    "        special_tokens_path = os.path.join(path, \"special_tokens.json\")\n",
    "        if os.path.exists(special_tokens_path):\n",
    "            with open(special_tokens_path, \"r\") as f:\n",
    "                self.special_token_ids = json.load(f)\n",
    "        \n",
    "        # Ensure the tokenizer has the correct special tokens\n",
    "        self.tokenizer.pad_token = \"<pad>\"\n",
    "        self.tokenizer.bos_token = \"<sos>\"\n",
    "        self.tokenizer.eos_token = \"<eos>\"\n",
    "        self.tokenizer.unk_token = \"<unk>\"\n",
    "        \n",
    "        # Set special token IDs explicitly\n",
    "        self.tokenizer.pad_token_id = self.special_token_ids[\"<pad>\"]\n",
    "        self.tokenizer.bos_token_id = self.special_token_ids[\"<sos>\"]\n",
    "        self.tokenizer.eos_token_id = self.special_token_ids[\"<eos>\"]\n",
    "        self.tokenizer.unk_token_id = self.special_token_ids[\"<unk>\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2dd0e2-4142-4d2d-ac1a-d3b94ab0fc2d",
   "metadata": {},
   "source": [
    "# Tokenizer Training: Balancing Coverage and Efficiency\r\n",
    "\r\n",
    "## Training Process Overview\r\n",
    "\r\n",
    "The tokenizer training section implements a carefully balanced approach to building a vocabulary that can handle the complexities of news articles while maintaining computational efficiency. The code first checks if a previously trained tokenizer exists, and if not, proceeds to train a new one using a substantial sample of the dataset.\r\n",
    "\r\n",
    "## Why a 32,000 Token Vocabulary?\r\n",
    "\r\n",
    "The choice of a 32,000 token vocabulary represents a deliberate balance between several competing factors:\r\n",
    "\r\n",
    "1. **Linguistic Coverage**: News articles contain diverse vocabulary including named entities, domain-specific terminology, and rare words. A larger vocabulary ensures better coverage of these elements without excessive use of unknown tokens.\r\n",
    "\r\n",
    "2. **Model Size Considerations**: Each additional token in the vocabulary increases the size of the embedding matrices in the transformer model. At 32,000 tokens, we achieve good coverage while keeping the model parameters manageable.\r\n",
    "\r\n",
    "3. **Training Efficiency**: Larger vocabularies require more computation during both training and inference. The 32,000 size allows for efficient training while preserving linguistic nuance.\r\n",
    "\r\n",
    "4. **Industry Standard Alignment**: This size is in line with other successful language models - slightly larger than BERT (30,000) but smaller than GPT-2 (50,000).\r\n",
    "\r\n",
    "5. **Subword Efficiency**: With BPE tokenization, 32,000 tokens provides sufficient granularity to reconstruct most words while capturing common subword patterns.\r\n",
    "\r\n",
    "## Corpus Selection Strategy\r\n",
    "\r\n",
    "The implementation samples 100,000 articles but also includes their summaries, effectively creating a 200,000 text corpus. This ensures the tokenizer learns patterns from both input articles and target summaries, which is crucial for the seq2seq nature of the summarization task.\r\n",
    "\r\n",
    "The sample size was increased from an earlier version (from 50,000 to 100,000), indicating an empirical decision to improve vocabulary coverage based on observed performance during development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b42411ac-5618-45f7-b1ce-1751dd173d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing tokenizer...\n",
      "Tokenizer loaded with vocabulary size: 32000\n"
     ]
    }
   ],
   "source": [
    "# Train Tokenizer\n",
    "# Initialize tokenizer\n",
    "MAX_VOCAB_SIZE = 32000\n",
    "tokenizer = OptimizedTokenizer(vocab_size=MAX_VOCAB_SIZE)\n",
    "\n",
    "# Check if tokenizer already exists\n",
    "if os.path.exists(r\"C:\\Users\\nisha\\Downloads\\tokenizer\\tokenizer.json\"):\n",
    "    print(\"Loading existing tokenizer...\")\n",
    "    tokenizer.load(r\"C:\\Users\\nisha\\Downloads\\tokenizer\")\n",
    "    print(f\"Tokenizer loaded with vocabulary size: {tokenizer.tokenizer.vocab_size}\")\n",
    "else:\n",
    "    print(\"Preparing texts for tokenizer training...\")\n",
    "    # Use a larger subset of the dataset for better vocabulary coverage\n",
    "    sample_size = 100000  \n",
    "    train_samples = cnn_dailymail['train'].select(range(sample_size))\n",
    "    \n",
    "    texts = []\n",
    "    for example in tqdm(train_samples):\n",
    "        texts.append(example['article'])\n",
    "        texts.append(example['highlights'])\n",
    "    \n",
    "    print(f\"Training tokenizer on {len(texts)} texts...\")\n",
    "    start_time = time.time()\n",
    "    tokenizer.train(texts, model_prefix=\"tokenizer\", num_samples=200000)  # Increased sample size\n",
    "    end_time = time.time()\n",
    "    print(f\"Tokenizer training completed in {end_time - start_time:.2f} seconds\")\n",
    "    \n",
    "    # Save the tokenizer\n",
    "    tokenizer.save(\"tokenizer\")\n",
    "    print(\"Tokenizer saved to 'tokenizer' directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d2489e-57c2-4d09-b62a-be28c0635f96",
   "metadata": {},
   "source": [
    "# Transformer Architecture: Advanced Components for Summarization\r\n",
    "\r\n",
    "## Positional Encoding\r\n",
    "\r\n",
    "The positional encoding component addresses a fundamental limitation of transformer models: they have no inherent understanding of sequence order. Unlike RNNs, which process tokens sequentially, transformers process all tokens in parallel.\r\n",
    "\r\n",
    "This implementation uses sinusoidal positional encodings, which add position-dependent patterns to each embedding. The mathematical properties of these sine/cosine functions allow the model to attend to relative positions, making it possible to understand the sequential nature of text while retaining the benefits of parallel processing.\r\n",
    "\r\n",
    "## Enhanced Multi-Head Attention\r\n",
    "\r\n",
    "The `ImprovedMultiHeadAttention` class represents a refined implementation of the attention mechanism that forms the core of the transformer. Key enhancements include:\r\n",
    "\r\n",
    "- **Numerical Stability**: Uses a smaller negative value (-1e4 instead of -1e9) for masked positions to prevent overflow in mixed precision training\r\n",
    "- **Proper Initialization**: Weight matrices are initialized with Xavier uniform distribution to ensure stable gradient flow\r\n",
    "- **Flexible Masking**: Supports multiple mask dimensions for different attention patterns\r\n",
    "\r\n",
    "Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. This enables capturing different types of dependencies in the text - some heads might focus on local syntactic patterns while others capture document-level semantic relationships.\r\n",
    "\r\n",
    "## Position-wise Feed-Forward Networks\r\n",
    "\r\n",
    "The feed-forward networks apply two linear transformations with a GELU activation in between. This creates a component that can model complex token-level transformations. Key features:\r\n",
    "\r\n",
    "- **GELU Activation**: Uses Gaussian Error Linear Unit instead of ReLU, providing smoother gradients\r\n",
    "- **Proper Dropout**: Applied after activation to improve regularization\r\n",
    "- **Careful Initialization**: Parameter initialization designed to prevent vanishing/exploding gradients\r\n",
    "\r\n",
    "## Encoder and Decoder Architecture\r\n",
    "\r\n",
    "The encoder and decoder follow the classic transformer design but with several optimizations:\r\n",
    "\r\n",
    "- **Pre-Layer Normalization**: Unlike the original transformer's post-layer norm, this implementation applies normalization before each sub-layer, significantly improving training stability\r\n",
    "- **Residual Connections**: Carefully implemented skip connections help maintain gradient flow through deep networks\r\n",
    "- **Shared Embeddings**: Input and output embeddings are shared to reduce parameters and improve regularization\r\n",
    "\r\n",
    "## Complete Transformer Model\r\n",
    "\r\n",
    "The `ImprovedTransformer` class brings everything together with several enhancements:\r\n",
    "\r\n",
    "- **Efficient Masking**: Optimized logic for creating source and target masks\r\n",
    "- **Three-Way Weight Tying**: Shares weights between encoder embeddings, decoder embeddings, and the output projection layer\r\n",
    "- **Beam Search Generation**: Implements a sophisticated beam search algorithm with top-k sampling for better summary quality\r\n",
    "- **Temperature Control**: Allows controlling the randomness in the generation process\r\n",
    "\r\n",
    "These architectural choices reflect both the original transformer design principles and more recent improvements developed by the NLP community, creating a model particularly well-suited for abstractive summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3ebbe20-0d54-4961-9971-a81f99eb325f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer Components\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length=5000, dropout=0.1):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Create positional encoding matrix\n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        \n",
    "        # Register buffer (not a parameter, but should be part of the module's state)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len, d_model]\n",
    "        x = x + self.pe[:, :x.size(1), :].to(x.device)\n",
    "        return self.dropout(x)\n",
    "\n",
    "class ImprovedMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super(ImprovedMultiHeadAttention, self).__init__()\n",
    "        \n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        # Linear projections with weight initialization\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # Initialize weights properly\n",
    "        for module in [self.W_q, self.W_k, self.W_v, self.W_o]:\n",
    "            nn.init.xavier_uniform_(module.weight)\n",
    "            nn.init.zeros_(module.bias)\n",
    "            \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None, return_attention=False):\n",
    "        batch_size = query.size(0)\n",
    "        \n",
    "        # Linear projections\n",
    "        Q = self.W_q(query)\n",
    "        K = self.W_k(key)\n",
    "        V = self.W_v(value)\n",
    "        \n",
    "        # Reshape for multi-head attention\n",
    "        Q = Q.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)  # [B, h, L_q, d_k]\n",
    "        K = K.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)  # [B, h, L_k, d_k]\n",
    "        V = V.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)  # [B, h, L_v, d_k]\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)  # [B, h, L_q, L_k]\n",
    "        \n",
    "        if mask is not None:\n",
    "            # Fix mask dimensions to match scores\n",
    "            if mask.dim() == 2:  # [B, L]\n",
    "                mask = mask.unsqueeze(1).unsqueeze(2)  # [B, 1, 1, L]\n",
    "            elif mask.dim() == 3:  # [B, L_q, L_k]\n",
    "                mask = mask.unsqueeze(1)  # [B, 1, L_q, L_k]\n",
    "            \n",
    "            # Use a smaller negative value for numerical stability in mixed precision\n",
    "            # -1e4 is safe for float16, while -1e9 causes overflow\n",
    "            scores = scores.masked_fill(mask == 0, -1e4)\n",
    "        \n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        # Apply attention weights to values\n",
    "        context = torch.matmul(attn_weights, V)  # [B, h, L_q, d_k]\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)  # [B, L_q, d_model]\n",
    "        \n",
    "        # Final projection\n",
    "        output = self.W_o(context)\n",
    "        \n",
    "        if return_attention:\n",
    "            return output, attn_weights\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1, activation='gelu'):\n",
    "        super(FeedForward, self).__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Choose activation function\n",
    "        if activation == 'relu':\n",
    "            self.activation = F.relu\n",
    "        elif activation == 'gelu':\n",
    "            self.activation = F.gelu\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation: {activation}\")\n",
    "        \n",
    "        # Initialize weights\n",
    "        nn.init.xavier_uniform_(self.linear1.weight)\n",
    "        nn.init.zeros_(self.linear1.bias)\n",
    "        nn.init.xavier_uniform_(self.linear2.weight)\n",
    "        nn.init.zeros_(self.linear2.bias)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear2(self.dropout(self.activation(self.linear1(x))))\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1, activation='gelu'):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        \n",
    "        # Self-attention mechanism\n",
    "        self.self_attn = ImprovedMultiHeadAttention(d_model, num_heads, dropout)\n",
    "        \n",
    "        # Feed-forward network\n",
    "        self.feed_forward = FeedForward(d_model, d_ff, dropout, activation)\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.norm1 = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        self.norm2 = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        # Pre-LayerNorm architecture (more stable for training)\n",
    "        norm_x = self.norm1(x)\n",
    "        attn_output = self.self_attn(norm_x, norm_x, norm_x, mask)\n",
    "        x = x + self.dropout1(attn_output)\n",
    "        \n",
    "        norm_x = self.norm2(x)\n",
    "        ff_output = self.feed_forward(norm_x)\n",
    "        x = x + self.dropout2(ff_output)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1, activation='gelu'):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        \n",
    "        # Self-attention mechanism\n",
    "        self.self_attn = ImprovedMultiHeadAttention(d_model, num_heads, dropout)\n",
    "        \n",
    "        # Cross-attention mechanism (for encoder-decoder attention)\n",
    "        self.cross_attn = ImprovedMultiHeadAttention(d_model, num_heads, dropout)\n",
    "        \n",
    "        # Feed-forward network\n",
    "        self.feed_forward = FeedForward(d_model, d_ff, dropout, activation)\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.norm1 = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        self.norm2 = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        self.norm3 = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, enc_output, src_mask=None, tgt_mask=None):\n",
    "        # Pre-LayerNorm architecture\n",
    "        # Self-attention block\n",
    "        norm_x = self.norm1(x)\n",
    "        self_attn_output = self.self_attn(norm_x, norm_x, norm_x, tgt_mask)\n",
    "        x = x + self.dropout1(self_attn_output)\n",
    "        \n",
    "        # Cross-attention block\n",
    "        norm_x = self.norm2(x)\n",
    "        cross_attn_output = self.cross_attn(norm_x, enc_output, enc_output, src_mask)\n",
    "        x = x + self.dropout2(cross_attn_output)\n",
    "        \n",
    "        # Feed-forward block\n",
    "        norm_x = self.norm3(x)\n",
    "        ff_output = self.feed_forward(norm_x)\n",
    "        x = x + self.dropout3(ff_output)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e588b2-5b0f-4cac-8c86-8fa048bc6fb1",
   "metadata": {},
   "source": [
    "# Encoder-Decoder Architecture: The Core of Sequence Transformation\r\n",
    "\r\n",
    "## Encoder: Processing Source Documents\r\n",
    "\r\n",
    "The Encoder component is responsible for transforming the input article into a rich contextual representation. Its key characteristics include:\r\n",
    "\r\n",
    "1. **Token and Positional Embedding Combination**: \r\n",
    "   - Input tokens are first embedded into a continuous vector space\r\n",
    "   - These embeddings are scaled by √d_model to balance their magnitude with positional encodings\r\n",
    "   - Positional encodings are added to provide sequence order information\r\n",
    "\r\n",
    "2. **Stacked Processing Layers**:\r\n",
    "   - Multiple identical encoder layers process the input sequentially\r\n",
    "   - Each layer has self-attention and feed-forward sub-layers\r\n",
    "   - Layer normalization and residual connections maintain gradient flow\r\n",
    "   \r\n",
    "3. **Final Normalization**: \r\n",
    "   - A layer normalization is applied to the output, producing the encoded representation\r\n",
    "   - This stabilizes the encoder output before it's passed to the decoder\r\n",
    "\r\n",
    "4. **Attention Masking**:\r\n",
    "   - The encoder uses masking to ignore padding tokens, ensuring attention is only computed over actual content\r\n",
    "   - This is crucial for processing variable-length articles efficiently\r\n",
    "\r\n",
    "## Decoder: Generating Summaries\r\n",
    "\r\n",
    "The Decoder takes the encoder output and autoregressively generates the summary. Its architecture includes:\r\n",
    "\r\n",
    "1. **Embeddings with Positional Information**:\r\n",
    "   - Like the encoder, it embeds tokens and adds positional encodings\r\n",
    "   - During training, it processes the entire target sequence in parallel with causal masking\r\n",
    "   - During inference, it generates tokens one by one\r\n",
    "\r\n",
    "2. **Additional Complexity**:\r\n",
    "   - Each decoder layer has three sub-components rather than two\r\n",
    "   - Self-attention over previously generated tokens\r\n",
    "   - Cross-attention to access the encoder's representation\r\n",
    "   - Feed-forward network for transformation\r\n",
    "\r\n",
    "3. **Causal Masking**:\r\n",
    "   - A triangular attention mask ensures the decoder can only attend to positions that come before the current one\r\n",
    "   - This prevents information leakage from future tokens during training\r\n",
    "\r\n",
    "4. **Cross-Attention Mechanism**:\r\n",
    "   - The critical component where the decoder attends to the encoded source\r\n",
    "   - Allows the model to focus on relevant parts of the article when generating each summary token\r\n",
    "\r\n",
    "## Encoder-Decoder Interaction\r\n",
    "\r\n",
    "The interaction between these components creates a powerful mechanism for transforming articles into summaries:\r\n",
    "\r\n",
    "1. **Information Flow**: \r\n",
    "   - The encoder processes the entire article at once\r\n",
    "   - The decoder accesses this encoded information through cross-attention\r\n",
    "   - This allows selective focusing on relevant article sections\r\n",
    "\r\n",
    "2. **Parameter Sharing**:\r\n",
    "   - The implementation shares embedding weights between encoder and decoder\r\n",
    "   - This weight tying reduces model size and acts as a regularization technique\r\n",
    "\r\n",
    "3. **Model Scaling**:\r\n",
    "   - The architecture can be scaled by adjusting the number of layers, attention heads, and model dimension\r\n",
    "   - The implementation uses 6 layers, giving sufficient depth for learning complex patterns without excessive computational requirements\r\n",
    "\r\n",
    "This encoder-decoder architecture is particularly well-suited for summarization because it can process long documents, identify salient information, and generate fluent summaries that capture the essence of the source text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27c0bcb0-4448-426e-a400-e974cf095978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder and Decoder Implementation\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, d_ff, num_layers, dropout=0.1, activation='gelu'):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        # Token embedding + positional encoding\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=0)\n",
    "        self.pos_encoding = PositionalEncoding(d_model, dropout=dropout)\n",
    "        \n",
    "        # Stack of encoder layers\n",
    "        self.layers = nn.ModuleList([\n",
    "            EncoderLayer(d_model, num_heads, d_ff, dropout, activation)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Final layer normalization\n",
    "        self.norm = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        \n",
    "        # Initialize embeddings\n",
    "        nn.init.normal_(self.embedding.weight, mean=0, std=d_model**-0.5)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        # x: [batch_size, seq_len]\n",
    "        \n",
    "        # Embed tokens and add positional encoding\n",
    "        x = self.embedding(x) * math.sqrt(self.embedding.embedding_dim)\n",
    "        x = self.pos_encoding(x)\n",
    "        \n",
    "        # Apply encoder layers\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "            \n",
    "        # Apply final normalization\n",
    "        x = self.norm(x)\n",
    "            \n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, d_ff, num_layers, dropout=0.1, activation='gelu'):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        # Token embedding + positional encoding\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=0)\n",
    "        self.pos_encoding = PositionalEncoding(d_model, dropout=dropout)\n",
    "        \n",
    "        # Stack of decoder layers\n",
    "        self.layers = nn.ModuleList([\n",
    "            DecoderLayer(d_model, num_heads, d_ff, dropout, activation)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Final layer normalization\n",
    "        self.norm = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        \n",
    "        # Initialize embeddings\n",
    "        nn.init.normal_(self.embedding.weight, mean=0, std=d_model**-0.5)\n",
    "        \n",
    "    def forward(self, x, enc_output, src_mask=None, tgt_mask=None):\n",
    "        # x: [batch_size, seq_len]\n",
    "        \n",
    "        # Embed tokens and add positional encoding\n",
    "        x = self.embedding(x) * math.sqrt(self.embedding.embedding_dim)\n",
    "        x = self.pos_encoding(x)\n",
    "        \n",
    "        # Apply decoder layers\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, enc_output, src_mask, tgt_mask)\n",
    "            \n",
    "        # Apply final normalization\n",
    "        x = self.norm(x)\n",
    "            \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5be5eb4-133d-4ab4-a805-87f8b1d81fb8",
   "metadata": {},
   "source": [
    "# Full Transformer Model Implementation\r\n",
    "\r\n",
    "## ImprovedTransformer Class\r\n",
    "\r\n",
    "The `ImprovedTransformer` class in Cell 8 implements a complete sequence-to-sequence transformer designed specifically for abstractive summarization. This class integrates the previously defined encoder and decoder components into a cohesive model.\r\n",
    "\r\n",
    "## Core Components and Design Decisions\r\n",
    "\r\n",
    "1. **Initialization Parameters**:\r\n",
    "   - The model accepts configuration for vocabulary sizes, dimensions, heads, feed-forward size, and number of layers\r\n",
    "   - Default values (d_model=512, num_heads=8, d_ff=2048, num_layers=6) follow the original transformer paper with modest adjustments\r\n",
    "\r\n",
    "2. **Special Token Management**:\r\n",
    "   - The class explicitly stores token IDs (pad=0, sos=1, eos=2) needed for masking and generation\r\n",
    "   - These IDs align with the tokenizer's special tokens\r\n",
    "\r\n",
    "3. **Encoder-Decoder Integration**:\r\n",
    "   - The encoder processes the entire source text in parallel\r\n",
    "   - The decoder works with the encoder outputs through cross-attention mechanisms\r\n",
    "\r\n",
    "4. **Parameter Sharing Strategy**:\r\n",
    "   - The `share_embeddings` parameter enables weight sharing between encoder and decoder embeddings\r\n",
    "   - Three-way weight tying links the output projection layer with the decoder embeddings\r\n",
    "   - This significantly reduces model parameters and improves regularization\r\n",
    "\r\n",
    "5. **Masking Utilities**:\r\n",
    "   - `create_src_mask`: Generates masks for handling padding in source sequences\r\n",
    "   - `create_tgt_mask`: Creates combined causal and padding masks for target sequences\r\n",
    "   - These ensure proper attention behavior during both training and inference\r\n",
    "\r\n",
    "6. **Forward Method**:\r\n",
    "   - Handles the complete sequence-to-sequence transformation\r\n",
    "   - Connects encoder and decoder with appropriate masking\r\n",
    "   - Returns logits over the vocabulary for each target position\r\n",
    "\r\n",
    "7. **Generation Method**:\r\n",
    "   - Implements beam search with additional controls (beam size, top-k, temperature)\r\n",
    "   - Uses helper `Beam` class to track multiple generation hypotheses\r\n",
    "   - Incorporates early stopping for efficiency\r\n",
    "\r\n",
    "This implementation combines architectural best practices with practical optimizations for the summarization task, striking a balance between model expressiveness and computational efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb68f00d-e257-4e8f-9134-bcea96d1bf19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full Transformer Model\n",
    "class ImprovedTransformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=512, num_heads=8,\n",
    "                 d_ff=2048, num_layers=6, dropout=0.1, activation='gelu',\n",
    "                 share_embeddings=True):\n",
    "        super(ImprovedTransformer, self).__init__()\n",
    "        \n",
    "        # Store special token IDs for mask creation and generation\n",
    "        self.pad_token_id = 0\n",
    "        self.sos_token_id = 1\n",
    "        self.eos_token_id = 2\n",
    "        \n",
    "        # Encoder and decoder\n",
    "        self.encoder = Encoder(src_vocab_size, d_model, num_heads, d_ff, num_layers, dropout, activation)\n",
    "        self.decoder = Decoder(tgt_vocab_size, d_model, num_heads, d_ff, num_layers, dropout, activation)\n",
    "        \n",
    "        # Final projection to vocabulary\n",
    "        self.final_layer = nn.Linear(d_model, tgt_vocab_size, bias=False)\n",
    "        \n",
    "        # Share embeddings between encoder and decoder (parameter sharing)\n",
    "        if share_embeddings:\n",
    "            self.encoder.embedding.weight = self.decoder.embedding.weight\n",
    "            \n",
    "        # Share embeddings with final output layer (three-way weight tying)\n",
    "        self.final_layer.weight = self.decoder.embedding.weight\n",
    "        \n",
    "    def create_src_mask(self, src):\n",
    "        # src: [batch_size, src_len]\n",
    "        # Create source padding mask: 1 for tokens, 0 for padding\n",
    "        src_mask = (src != self.pad_token_id).unsqueeze(1).unsqueeze(2)\n",
    "        # src_mask shape: [batch_size, 1, 1, src_len]\n",
    "        return src_mask\n",
    "    \n",
    "    def create_tgt_mask(self, tgt):\n",
    "        # tgt: [batch_size, tgt_len]\n",
    "        # Create target padding mask\n",
    "        tgt_pad_mask = (tgt != self.pad_token_id).unsqueeze(1).unsqueeze(3)\n",
    "        # tgt_pad_mask shape: [batch_size, 1, tgt_len, 1]\n",
    "        \n",
    "        # Create causal mask to prevent attending to future positions\n",
    "        tgt_len = tgt.size(1)\n",
    "        tgt_sub_mask = torch.tril(torch.ones((tgt_len, tgt_len), device=tgt.device)).bool()\n",
    "        tgt_sub_mask = tgt_sub_mask.unsqueeze(0).unsqueeze(1)\n",
    "        # tgt_sub_mask shape: [1, 1, tgt_len, tgt_len]\n",
    "        \n",
    "        # Combine padding mask and causal mask\n",
    "        tgt_mask = tgt_pad_mask & tgt_sub_mask\n",
    "        # tgt_mask shape: [batch_size, 1, tgt_len, tgt_len]\n",
    "        return tgt_mask\n",
    "    \n",
    "    def forward(self, src, tgt):\n",
    "        # src: [batch_size, src_len]\n",
    "        # tgt: [batch_size, tgt_len]\n",
    "        \n",
    "        # Create masks\n",
    "        src_mask = self.create_src_mask(src)\n",
    "        tgt_mask = self.create_tgt_mask(tgt)\n",
    "        \n",
    "        # Encoder forward pass\n",
    "        enc_output = self.encoder(src, src_mask)\n",
    "        \n",
    "        # Decoder forward pass\n",
    "        dec_output = self.decoder(tgt, enc_output, src_mask, tgt_mask)\n",
    "        \n",
    "        # Final projection to vocabulary\n",
    "        output = self.final_layer(dec_output)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def generate(self, src, max_length=128, beam_size=5, top_k=50, temperature=1.0, early_stopping=True):\n",
    "        \"\"\"\n",
    "        Generate sequence using beam search.\n",
    "        \n",
    "        Args:\n",
    "            src: Tensor of shape [batch_size, src_len]\n",
    "            max_length: Maximum length of generated sequence\n",
    "            beam_size: Number of beams for beam search\n",
    "            top_k: Sample from top k most likely tokens for diversity\n",
    "            temperature: Temperature for sampling\n",
    "            early_stopping: Whether to stop when all beams have generated EOS\n",
    "            \n",
    "        Returns:\n",
    "            Tensor of shape [batch_size, max_length]\n",
    "        \"\"\"\n",
    "        batch_size = src.size(0)\n",
    "        \n",
    "        # Process source sequence\n",
    "        src_mask = self.create_src_mask(src)\n",
    "        encoder_output = self.encoder(src, src_mask)\n",
    "        \n",
    "        # Initialize beam for each batch item\n",
    "        beams = [Beam(beam_size, self.pad_token_id, self.sos_token_id, self.eos_token_id, device=src.device)\n",
    "                for _ in range(batch_size)]\n",
    "        \n",
    "        # Run beam search for each batch item\n",
    "        for i in range(batch_size):\n",
    "            # Extract encoder output for this batch item\n",
    "            encoder_output_i = encoder_output[i:i+1].expand(beam_size, -1, -1)\n",
    "            src_mask_i = src_mask[i:i+1].expand(beam_size, -1, -1, -1)\n",
    "            \n",
    "            # This will store the decoder input tokens\n",
    "            input_ids = torch.full((beam_size, 1), self.sos_token_id, dtype=torch.long, device=src.device)\n",
    "            \n",
    "            # Generate tokens step by step\n",
    "            for step in range(max_length):\n",
    "                # Create tgt_mask for the current step\n",
    "                tgt_mask = self.create_tgt_mask(input_ids)\n",
    "                \n",
    "                # Compute decoder output\n",
    "                decoder_output = self.decoder(input_ids, encoder_output_i, src_mask_i, tgt_mask)\n",
    "                \n",
    "                # Get output for the last position\n",
    "                last_token_output = decoder_output[:, -1]\n",
    "                \n",
    "                # Project to vocabulary\n",
    "                logits = self.final_layer(last_token_output)\n",
    "                \n",
    "                # Apply temperature\n",
    "                logits = logits / temperature\n",
    "                \n",
    "                # Apply top-k sampling\n",
    "                if top_k > 0:\n",
    "                    top_k_logits, top_k_indices = torch.topk(logits, top_k)\n",
    "                    logits = torch.full_like(logits, float('-inf'))\n",
    "                    logits.scatter_(1, top_k_indices, top_k_logits)\n",
    "                \n",
    "                # Convert to probabilities\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "                \n",
    "                # Update beam state\n",
    "                beams[i].advance(probs)\n",
    "                \n",
    "                # Prepare for next step\n",
    "                # Get the indices of the top beam_size hypotheses\n",
    "                beam_indices = beams[i].get_current_state()\n",
    "                \n",
    "                # Select the next tokens and update the input for next step\n",
    "                next_tokens = beams[i].get_current_tokens().unsqueeze(1)\n",
    "                \n",
    "                # Create new input by concatenating current input_ids with next tokens\n",
    "                input_ids = torch.cat([input_ids[beam_indices], next_tokens], dim=1)\n",
    "                \n",
    "                # Check if all beams have generated EOS\n",
    "                if early_stopping and beams[i].is_done():\n",
    "                    break\n",
    "            \n",
    "        # Return the best hypothesis for each batch item\n",
    "        output_ids = []\n",
    "        for i in range(batch_size):\n",
    "            output_ids.append(beams[i].get_best_hypothesis())\n",
    "            \n",
    "        # Pad sequences to max_length if needed\n",
    "        max_len = max(len(ids) for ids in output_ids)\n",
    "        padded_output = torch.full((batch_size, max_len), self.pad_token_id, dtype=torch.long, device=src.device)\n",
    "        \n",
    "        for i, ids in enumerate(output_ids):\n",
    "            padded_output[i, :len(ids)] = torch.tensor(ids, dtype=torch.long, device=src.device)\n",
    "            \n",
    "        return padded_output\n",
    "\n",
    "# Helper classes for beam search generation\n",
    "class Beam:\n",
    "    def __init__(self, beam_size, pad_token_id, sos_token_id, eos_token_id, device):\n",
    "        self.beam_size = beam_size\n",
    "        self.pad_token_id = pad_token_id\n",
    "        self.sos_token_id = sos_token_id\n",
    "        self.eos_token_id = eos_token_id\n",
    "        self.device = device\n",
    "        \n",
    "        # Scores for each sequence\n",
    "        self.scores = torch.zeros(beam_size, device=device)\n",
    "        self.scores[1:] = -1e9  # Start with only one hypothesis\n",
    "        \n",
    "        # Backpointers at each step\n",
    "        self.backpointers = []\n",
    "        \n",
    "        # Sequence tokens at each step\n",
    "        self.tokens = torch.full((beam_size, 1), self.sos_token_id, dtype=torch.long, device=device)\n",
    "        \n",
    "        # Flags to indicate if a beam has reached EOS\n",
    "        self.eos_top = False\n",
    "        self.finished = []\n",
    "        \n",
    "    def get_current_state(self):\n",
    "        \"\"\"Get current output token indices\"\"\"\n",
    "        return torch.arange(self.beam_size, device=self.device)\n",
    "    \n",
    "    def get_current_tokens(self):\n",
    "        \"\"\"Get the tokens for the current step\"\"\"\n",
    "        return self.tokens[:, -1]\n",
    "    \n",
    "    def advance(self, word_probs):\n",
    "        \"\"\"Update beam state given log probabilities for the next token\"\"\"\n",
    "        vocab_size = word_probs.size(-1)\n",
    "        \n",
    "        # Add current scores to the log probabilities\n",
    "        scores = word_probs + self.scores.unsqueeze(1)\n",
    "        \n",
    "        # Flatten to find top beam_size candidates\n",
    "        flat_scores = scores.view(-1)\n",
    "        best_scores, best_scores_idx = flat_scores.topk(self.beam_size, largest=True, sorted=True)\n",
    "        \n",
    "        # Get beam indices and token indices\n",
    "        beam_idx = best_scores_idx // vocab_size\n",
    "        token_idx = best_scores_idx % vocab_size\n",
    "        \n",
    "        # Update backpointers\n",
    "        self.backpointers.append(beam_idx)\n",
    "        \n",
    "        # Update tokens\n",
    "        self.tokens = torch.cat([self.tokens[beam_idx], token_idx.unsqueeze(1)], dim=1)\n",
    "        \n",
    "        # Update scores\n",
    "        self.scores = best_scores\n",
    "        \n",
    "        # Check if any hypothesis reached EOS\n",
    "        eos_indices = torch.where(token_idx == self.eos_token_id)[0]\n",
    "        if len(eos_indices) > 0:\n",
    "            for idx in eos_indices:\n",
    "                if not self.eos_top:\n",
    "                    # Record the first EOS in the beam\n",
    "                    self.eos_top = True\n",
    "                    \n",
    "                # Add finished hypothesis\n",
    "                self.finished.append((self.scores[idx].item(), len(self.tokens[idx]) - 1, idx))\n",
    "        \n",
    "    def is_done(self):\n",
    "        \"\"\"Check if at least one beam has reached EOS\"\"\"\n",
    "        return self.eos_top and len(self.finished) >= self.beam_size\n",
    "    \n",
    "    def get_best_hypothesis(self):\n",
    "        \"\"\"Get the best hypothesis\"\"\"\n",
    "        if self.finished:\n",
    "            # Sort finished hypotheses by score\n",
    "            self.finished.sort(key=lambda x: -x[0])\n",
    "            score, length, idx = self.finished[0]\n",
    "            # Return best finished hypothesis\n",
    "            return self.tokens[idx, 1:length+1].tolist()\n",
    "        else:\n",
    "            # Return best unfinished hypothesis\n",
    "            return self.tokens[0, 1:].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1707bcc4-2c38-40ef-ae3b-f8331f2d999c",
   "metadata": {},
   "source": [
    "# Data Preprocessing: Efficient Processing for Large-Scale Training\r\n",
    "\r\n",
    "## Preprocessing Strategy\r\n",
    "\r\n",
    "The data preprocessing implementation addresses the challenges of efficiently handling the large CNN/DailyMail dataset while preparing it for transformer training. The preprocessing pipeline includes several key optimizations:\r\n",
    "\r\n",
    "1. **Length Constraints**:\r\n",
    "   - Articles are capped at 512 tokens, capturing the majority of article content while keeping sequences manageable\r\n",
    "   - Summaries are limited to 128 tokens, providing ample space for comprehensive summaries\r\n",
    "\r\n",
    "2. **Label Processing for Training**:\r\n",
    "   - Padding tokens in target sequences are replaced with -100, which PyTorch's loss functions automatically ignore\r\n",
    "   - This prevents the model from learning to predict padding tokens\r\n",
    "   - Token IDs are also verified to be within vocabulary size to prevent index errors\r\n",
    "\r\n",
    "3. **Chunked Processing**:\r\n",
    "   - The 287,113 training examples are processed in 50,000 example chunks\r\n",
    "   - This chunking prevents memory overflow issues that would occur when processing the entire dataset at once\r\n",
    "   - Each chunk is processed and then concatenated into the final dataset\r\n",
    "\r\n",
    "4. **Parallelization**:\r\n",
    "   - Multiple processes (num_proc=4) are used to accelerate tokenization\r\n",
    "   - This leverages multicore processing capabilities, significantly reducing preprocessing time\r\n",
    "\r\n",
    "5. **Persistence Strategy**:\r\n",
    "   - Processed datasets are saved to disk after creation\r\n",
    "   - The code checks for existing processed data before beginning the expensive preprocessing\r\n",
    "   - This \"process once, use many times\" approach saves substantial time during development iterations\r\n",
    "\r\n",
    "6. **Progress Monitoring**:\r\n",
    "   - Detailed progress bars and time tracking provide visibility into the preprocessing status\r\n",
    "   - This is particularly important given the time-intensive nature of processing large text datasets\r\n",
    "\r\n",
    "## Implementation Details\r\n",
    "\r\n",
    "The preprocessing handles several technical challenges:\r\n",
    "\r\n",
    "1. **Memory Management**: By processing in chunks and using efficient data structures, the code avoids out-of-memory errors that would occur when naively processing the full dataset.\r\n",
    "\r\n",
    "2. **Attention Mask Generation**: The tokenization process generates attention masks that identify which tokens are real content versus padding, essential for the transformer's attention mechanisms.\r\n",
    "\r\n",
    "3. **Dataset Transformation**: The original dataset format is converted to a training-ready format with input_ids, attention_masks, and properly formatted labels.\r\n",
    "\r\n",
    "This preprocessing approach balances thoroughness with efficiency, ensuring that the entire dataset is properly prepared for training while minimizing computational overhead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df0c8540-a044-4728-9b3f-920e6a69874f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing training dataset...\n",
      "Loading pre-processed datasets...\n",
      "Loaded 287113 training examples and 13368 validation examples.\n",
      "Processed train dataset size: 287113\n",
      "Processed validation dataset size: 13368\n",
      "Preprocessing complete!\n"
     ]
    }
   ],
   "source": [
    "# Data Preprocessing\n",
    "# Define preprocessing parameters\n",
    "MAX_INPUT_LENGTH = 512\n",
    "MAX_TARGET_LENGTH = 128\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    \"\"\"\n",
    "    Preprocess data examples for training:\n",
    "    - Tokenize inputs (articles)\n",
    "    - Tokenize targets (summaries)\n",
    "    - Truncate to max length\n",
    "    \"\"\"\n",
    "    # Tokenize the articles\n",
    "    model_inputs = tokenizer.batch_encode(\n",
    "        examples[\"article\"], \n",
    "        max_length=MAX_INPUT_LENGTH, \n",
    "        padding=\"max_length\", \n",
    "        truncation=True\n",
    "    )\n",
    "    \n",
    "    # Tokenize the summaries\n",
    "    labels = tokenizer.batch_encode(\n",
    "        examples[\"highlights\"], \n",
    "        max_length=MAX_TARGET_LENGTH, \n",
    "        padding=\"max_length\", \n",
    "        truncation=True\n",
    "    )\n",
    "    \n",
    "    # Set the labels\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    \n",
    "    # Replace padding token id with -100 for labels (PyTorch will ignore these in loss calculation)\n",
    "    new_labels = []\n",
    "    for label in labels[\"input_ids\"]:\n",
    "        # Important: Ensure label tokens don't exceed vocab size\n",
    "        new_label = [l if l != tokenizer.tokenizer.pad_token_id and l < MAX_VOCAB_SIZE else -100 for l in label]\n",
    "        new_labels.append(new_label)\n",
    "    \n",
    "    model_inputs[\"labels\"] = new_labels\n",
    "    \n",
    "    return model_inputs\n",
    "\n",
    "# Process datasets with progress tracking and multiple processes\n",
    "print(\"Processing training dataset...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Process all training data\n",
    "train_dataset = cnn_dailymail[\"train\"]\n",
    "processed_data_dir = r\"D:\\NLP-Project\\processed_text_summarization_data\\processed_data\"\n",
    "train_data_path = os.path.join(processed_data_dir, \"train\")\n",
    "val_data_path = os.path.join(processed_data_dir, \"validation\")\n",
    "\n",
    "# Check if processed data already exists\n",
    "if os.path.exists(train_data_path) and os.path.exists(val_data_path):\n",
    "    print(\"Loading pre-processed datasets...\")\n",
    "    try:\n",
    "        from datasets import load_from_disk\n",
    "        tokenized_train_dataset = load_from_disk(train_data_path)\n",
    "        tokenized_val_dataset = load_from_disk(val_data_path)\n",
    "        print(f\"Loaded {len(tokenized_train_dataset)} training examples and {len(tokenized_val_dataset)} validation examples.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading pre-processed data: {e}\")\n",
    "        raise e\n",
    "else:\n",
    "    print(\"Processing datasets from scratch...\")\n",
    "    os.makedirs(processed_data_dir, exist_ok=True)\n",
    "    \n",
    "    # Process training data in manageable chunks to avoid memory issues\n",
    "    chunk_size = 50000\n",
    "    num_chunks = math.ceil(len(train_dataset) / chunk_size)\n",
    "    \n",
    "    all_train_datasets = []\n",
    "    for i in range(num_chunks):\n",
    "        start_idx = i * chunk_size\n",
    "        end_idx = min((i + 1) * chunk_size, len(train_dataset))\n",
    "        \n",
    "        print(f\"Processing chunk {i+1}/{num_chunks} (examples {start_idx} to {end_idx})...\")\n",
    "        chunk = train_dataset.select(range(start_idx, end_idx))\n",
    "        \n",
    "        tokenized_chunk = chunk.map(\n",
    "            preprocess_function,\n",
    "            batched=True,\n",
    "            batch_size=1000,\n",
    "            remove_columns=train_dataset.column_names,\n",
    "            desc=f\"Processing chunk {i+1}/{num_chunks}\",\n",
    "            num_proc=4  # Use multiple processes\n",
    "        )\n",
    "        \n",
    "        all_train_datasets.append(tokenized_chunk)\n",
    "    \n",
    "    # Combine all chunks\n",
    "    from datasets import concatenate_datasets\n",
    "    tokenized_train_dataset = concatenate_datasets(all_train_datasets)\n",
    "    \n",
    "    print(\"Processing validation dataset...\")\n",
    "    tokenized_val_dataset = cnn_dailymail[\"validation\"].map(\n",
    "        preprocess_function,\n",
    "        batched=True,\n",
    "        batch_size=1000,\n",
    "        remove_columns=cnn_dailymail[\"validation\"].column_names,\n",
    "        desc=\"Processing validation data\",\n",
    "        num_proc=4\n",
    "    )\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"Data preprocessing completed in {end_time - start_time:.2f} seconds\")\n",
    "    \n",
    "    # Save processed datasets\n",
    "    print(\"Saving processed datasets to disk...\")\n",
    "    tokenized_train_dataset.save_to_disk(train_data_path)\n",
    "    tokenized_val_dataset.save_to_disk(val_data_path)\n",
    "\n",
    "# Calculate dataset sizes\n",
    "print(f\"Processed train dataset size: {len(tokenized_train_dataset)}\")\n",
    "print(f\"Processed validation dataset size: {len(tokenized_val_dataset)}\")\n",
    "print(\"Preprocessing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3fe89d-36b6-40b8-b2d1-f59a507fb7eb",
   "metadata": {},
   "source": [
    "# Hyperparameter Selection: Balancing Performance and Efficiency\r\n",
    "\r\n",
    "## Model Hyperparameters\r\n",
    "\r\n",
    "The implementation carefully selects hyperparameters that balance modeling capacity with computational efficiency:\r\n",
    "\r\n",
    "1. **Model Dimensions**:\r\n",
    "   - `D_MODEL = 768`: Larger than the original transformer paper (512) to increase model capacity for handling complex news articles\r\n",
    "   - `NUM_HEADS = 12`: Increased from the standard 8 heads to allow finer-grained attention patterns\r\n",
    "   - `D_FF = 3072`: Feed-forward dimension set to 4x the model dimension, providing sufficient transformation capacity\r\n",
    "   - `NUM_LAYERS = 6`: Using 6 encoder and decoder layers offers good depth without excessive computation\r\n",
    "\r\n",
    "2. **Regularization Controls**:\r\n",
    "   - `DROPOUT = 0.1`: Standard dropout rate that prevents overfitting while maintaining training signal\r\n",
    "   - `ACTIVATION = 'gelu'`: Gaussian Error Linear Unit provides smoother gradients than ReLU\r\n",
    "\r\n",
    "## Training Hyperparameters\r\n",
    "\r\n",
    "The training configuration incorporates several advanced techniques:\r\n",
    "\r\n",
    "1. **Batch Processing Strategy**:\r\n",
    "   - `BATCH_SIZE = 16`: Direct batch size constrained by GPU memory\r\n",
    "   - `GRADIENT_ACCUMULATION_STEPS = 4`: Accumulates gradients across 4 batches\r\n",
    "   - `EFFECTIVE_BATCH_SIZE = 64`: The resulting effective batch size provides more stable gradients\r\n",
    "\r\n",
    "2. **Optimization Settings**:\r\n",
    "   - `LEARNING_RATE = 3e-4`: Slightly lower than typical 5e-4 for improved stability\r\n",
    "   - `WEIGHT_DECAY = 0.01`: L2 regularization to prevent overfitting\r\n",
    "   - `NUM_EPOCHS = 10`: Sufficient for convergence on this dataset\r\n",
    "   - `WARMUP_RATIO = 0.1`: Gradual warmup for 10% of total steps prevents early instability\r\n",
    "   - `MAX_GRAD_NORM = 1.0`: Gradient clipping to prevent exploding gradients\r\n",
    "   - `MIXED_PRECISION = True`: Enables FP16 computation for speed while maintaining numerical stability\r\n",
    "\r\n",
    "## Data Processing Optimizations\r\n",
    "\r\n",
    "The implementation includes a specialized collate function that:\r\n",
    "\r\n",
    "1. Prepares target inputs for teacher forcing during training\r\n",
    "2. Handles padding and masking efficiently\r\n",
    "3. Properly marks invalid positions using the -100 label for PyTorch's loss functions\r\n",
    "\r\n",
    "This careful balance of hyperparameters enables effective training of a powerful abstractive summarization model within reasonable computational constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fcd96169-204d-4460-87b9-5879136ccfc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training steps: 44862\n",
      "Warmup steps: 4486\n"
     ]
    }
   ],
   "source": [
    "# Training Setup and Hyperparameters\n",
    "# Model hyperparameters - optimized values\n",
    "VOCAB_SIZE = 32000  # Ensure this matches the tokenizer vocabulary size\n",
    "D_MODEL = 768       # Larger model dimension\n",
    "NUM_HEADS = 12      # More attention heads\n",
    "D_FF = 3072         # Larger feed-forward dimension\n",
    "NUM_LAYERS = 6      # Enough layers for good performance without overfitting\n",
    "DROPOUT = 0.1       # Standard dropout rate\n",
    "ACTIVATION = 'gelu' # Better activation function\n",
    "\n",
    "# Training hyperparameters\n",
    "BATCH_SIZE = 16     # Increased batch size\n",
    "GRADIENT_ACCUMULATION_STEPS = 4  # Accumulate gradients for stable training\n",
    "EFFECTIVE_BATCH_SIZE = BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS\n",
    "LEARNING_RATE = 3e-4  # Slightly lower learning rate for stability\n",
    "WEIGHT_DECAY = 0.01   # L2 regularization\n",
    "NUM_EPOCHS = 10\n",
    "WARMUP_RATIO = 0.1   # Warmup for 10% of total steps\n",
    "MAX_GRAD_NORM = 1.0\n",
    "MIXED_PRECISION = True  # Enable mixed precision training for speed\n",
    "\n",
    "# Create dataloaders with optimized collate function\n",
    "def collate_fn(batch):\n",
    "    input_ids = torch.tensor([example['input_ids'] for example in batch])\n",
    "    attention_mask = torch.tensor([example['attention_mask'] for example in batch])\n",
    "    labels = torch.tensor([example['labels'] for example in batch])\n",
    "    \n",
    "    # Calculate target input (for teacher forcing) and target output\n",
    "    # Target input is the labels shifted right with SOS token at the beginning\n",
    "    target_input = torch.zeros_like(labels)\n",
    "    target_input[:, 0] = tokenizer.tokenizer.bos_token_id  # Start with SOS token\n",
    "    \n",
    "    # Fill in the rest of the target input\n",
    "    valid_positions = (labels != -100)[:, :-1]\n",
    "    target_input[:, 1:][valid_positions] = labels[:, :-1][valid_positions]\n",
    "    \n",
    "    # Set padding for invalid positions\n",
    "    invalid_positions = ~valid_positions\n",
    "    target_input[:, 1:][invalid_positions] = tokenizer.tokenizer.pad_token_id\n",
    "    \n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'target_input': target_input,\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# Create training dataloader\n",
    "train_sampler = RandomSampler(tokenized_train_dataset)\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_train_dataset,\n",
    "    sampler=train_sampler,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# Create validation dataloader\n",
    "val_sampler = SequentialSampler(tokenized_val_dataset)\n",
    "val_dataloader = DataLoader(\n",
    "    tokenized_val_dataset,\n",
    "    sampler=val_sampler,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# Calculate total training steps and warmup steps\n",
    "total_steps = len(train_dataloader) * NUM_EPOCHS // GRADIENT_ACCUMULATION_STEPS\n",
    "warmup_steps = int(total_steps * WARMUP_RATIO)\n",
    "\n",
    "print(f\"Total training steps: {total_steps}\")\n",
    "print(f\"Warmup steps: {warmup_steps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790c9a4b-c41d-44da-9802-b1d2ab4013b9",
   "metadata": {},
   "source": [
    "# Loss Function and Learning Rate Scheduler: Optimizing Training Dynamics\r\n",
    "\r\n",
    "## Label Smoothing Loss\r\n",
    "\r\n",
    "The implementation uses a custom `LabelSmoothingLoss` class that incorporates label smoothing, a technique that improves model generalization and confidence calibration:\r\n",
    "\r\n",
    "1. **Regularization Mechanism**:\r\n",
    "   - Rather than training the model to predict exactly 1.0 for the correct class and 0.0 for all others, label smoothing redistributes some probability to other tokens\r\n",
    "   - The implementation uses a smoothing factor of 0.1, reserving 90% probability for the correct token and distributing 10% across all other tokens\r\n",
    "\r\n",
    "2. **Overconfidence Prevention**:\r\n",
    "   - Without smoothing, models tend to become overconfident in their predictions\r\n",
    "   - By introducing uncertainty into the training targets, the model learns to be appropriately cautious\r\n",
    "   - This is particularly important for summarization, where multiple valid phrasings may exist\r\n",
    "\r\n",
    "3. **Handling of Padding Tokens**:\r\n",
    "   - The loss function carefully handles padding with an `ignore_index` parameter (set to -100)\r\n",
    "   - This prevents the model from wasting capacity learning to predict padding tokens\r\n",
    "   - The implementation includes safeguards to prevent index errors by clamping target values\r\n",
    "\r\n",
    "4. **Numerical Stability**:\r\n",
    "   - The implementation uses stable computation patterns to avoid underflow/overflow\r\n",
    "   - This is especially important when training with mixed precision\r\n",
    "\r\n",
    "## Warmup Cosine Scheduler\r\n",
    "\r\n",
    "The `WarmupCosineScheduler` class implements an advanced learning rate schedule combining initial warmup with cosine decay:\r\n",
    "\r\n",
    "1. **Warmup Phase**:\r\n",
    "   - Learning rate increases linearly from 0 to the base learning rate during the first 10% of training steps\r\n",
    "   - This prevents unstable gradient updates at the beginning of training when weights are randomly initialized\r\n",
    "   - Warmup is especially important for transformer models due to their complex gradient flow through attention mechanisms\r\n",
    "\r\n",
    "2. **Cosine Annealing Phase**:\r\n",
    "   - After warmup, learning rate follows a cosine curve that gradually decreases to a minimum\r\n",
    "   - This smooth decay allows the model to settle into better minima than step-based schedules\r\n",
    "   - The cosine shape provides initially slow decay that accelerates in the middle of training and slows again near the end\r\n",
    "\r\n",
    "3. **Implementation Approach**:\r\n",
    "   - The scheduler updates the optimizer's learning rate after each step\r\n",
    "   - It tracks its state through the `current_step` variable\r\n",
    "   - The schedule calculation accounts for both warmup and annealing phases with appropriate transitions\r\n",
    "\r\n",
    "This combination of label smoothing and advanced learning rate scheduling contributes significantly to training stability and final model quality, addressing common challenges in transformer training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "568b705b-82d3-469a-af4e-56fd307a4ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function and Scheduler\n",
    "# Label smoothing loss function for better regularization\n",
    "class LabelSmoothingLoss(nn.Module):\n",
    "    def __init__(self, smoothing=0.1, ignore_index=-100, vocab_size=None):\n",
    "        super(LabelSmoothingLoss, self).__init__()\n",
    "        self.smoothing = smoothing\n",
    "        self.ignore_index = ignore_index\n",
    "        self.vocab_size = vocab_size\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        \n",
    "    def forward(self, output, target):\n",
    "        # output: [batch_size, seq_len, vocab_size]\n",
    "        # target: [batch_size, seq_len]\n",
    "        \n",
    "        batch_size, seq_len, vocab_size = output.size()\n",
    "        output = output.reshape(-1, vocab_size)\n",
    "        target = target.reshape(-1)\n",
    "        \n",
    "        # Create mask for ignored indices (padding)\n",
    "        non_pad_mask = (target != self.ignore_index)\n",
    "        \n",
    "        # Get only valid targets\n",
    "        target = target[non_pad_mask]\n",
    "        output = output[non_pad_mask]\n",
    "        \n",
    "        # Count valid targets\n",
    "        n_valid = target.size(0)\n",
    "        if n_valid == 0:\n",
    "            return torch.tensor(0.0, device=output.device, requires_grad=True)\n",
    "        \n",
    "        # Clamp target values to stay within vocabulary range\n",
    "        target = torch.clamp(target, 0, vocab_size-1)\n",
    "        \n",
    "        # Create smoothed target distribution\n",
    "        smooth_target = torch.zeros_like(output)\n",
    "        smooth_target.fill_(self.smoothing / (vocab_size - 1))\n",
    "        smooth_target.scatter_(1, target.unsqueeze(1), self.confidence)\n",
    "        \n",
    "        # Calculate loss using cross-entropy\n",
    "        log_probs = F.log_softmax(output, dim=1)\n",
    "        loss = -(smooth_target * log_probs).sum(dim=1).mean()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "# Improved learning rate scheduler with warmup\n",
    "class WarmupCosineScheduler:\n",
    "    def __init__(self, optimizer, warmup_steps, total_steps):\n",
    "        self.optimizer = optimizer\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.total_steps = total_steps\n",
    "        self.current_step = 0\n",
    "        \n",
    "    def step(self):\n",
    "        self.current_step += 1\n",
    "        lr = self.get_lr()\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "            \n",
    "    def get_lr(self):\n",
    "        # Linear warmup\n",
    "        if self.current_step < self.warmup_steps:\n",
    "            return LEARNING_RATE * (self.current_step / max(1, self.warmup_steps))\n",
    "        \n",
    "        # Cosine annealing\n",
    "        progress = (self.current_step - self.warmup_steps) / max(1, self.total_steps - self.warmup_steps)\n",
    "        return LEARNING_RATE * 0.5 * (1.0 + math.cos(math.pi * progress))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a93003c-769c-4635-9d0d-bace8b12ef9d",
   "metadata": {},
   "source": [
    "# Model Initialization and Training Functions: Implementation Details\r\n",
    "\r\n",
    "## Model Initialization Strategy\r\n",
    "\r\n",
    "The model initialization code demonstrates a robust approach to creating and configuring the transformer:\r\n",
    "\r\n",
    "1. **Model Creation with Fallback Mechanism**:\r\n",
    "   - The implementation first attempts to create the model with the optimal hyperparameters\r\n",
    "   - Error handling detects potential memory issues and provides a fallback to a smaller model configuration\r\n",
    "   - This graceful degradation ensures training can proceed even with memory constraints\r\n",
    "\r\n",
    "2. **Weight Initialization and Parameter Sharing**:\r\n",
    "   - The model uses weight sharing between encoder and decoder embeddings\r\n",
    "   - The output projection layer reuses decoder embedding weights (three-way weight tying)\r\n",
    "   - Model size statistics are calculated to verify the expected parameter count (123.8M parameters)\r\n",
    "\r\n",
    "3. **Optimizer Configuration**:\r\n",
    "   - AdamW optimizer is used with parameter-specific weight decay\r\n",
    "   - Parameters are grouped to apply weight decay only to appropriate tensors:\r\n",
    "     - Weight decay applied to most parameters\r\n",
    "     - No weight decay for bias terms and LayerNorm weights\r\n",
    "   - This selective weight decay follows best practices for transformer models\r\n",
    "\r\n",
    "4. **Mixed Precision Setup**:\r\n",
    "   - The code tests mixed precision capability with a small forward pass\r\n",
    "   - GradScaler is initialized when mixed precision is enabled\r\n",
    "   - This proactive testing prevents potential runtime errors during training\r\n",
    "\r\n",
    "5. **Monitoring and Logging Infrastructure**:\r\n",
    "   - TensorBoard writer is initialized for tracking metrics\r\n",
    "   - Checkpoint directory is cng instability, and the need for robust error handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c5d7fd9-4dad-4f9e-a16d-a76e5bf1a3c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating improved transformer model...\n",
      "Model successfully moved to cuda\n",
      "Model created with 123,816,960 parameters\n",
      "Model size in memory: 0.50 GB\n",
      "Using AdamW optimizer with weight decay\n",
      "Mixed precision training enabled: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nisha\\AppData\\Local\\Temp\\ipykernel_20392\\1919511013.py:81: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler() if MIXED_PRECISION else None\n"
     ]
    }
   ],
   "source": [
    "# Model Initialization\n",
    "# Create model with optimized architecture\n",
    "print(\"Creating improved transformer model...\")\n",
    "try:\n",
    "    # Initialize the model\n",
    "    model = ImprovedTransformer(\n",
    "        src_vocab_size=VOCAB_SIZE,\n",
    "        tgt_vocab_size=VOCAB_SIZE,\n",
    "        d_model=D_MODEL,\n",
    "        num_heads=NUM_HEADS,\n",
    "        d_ff=D_FF,\n",
    "        num_layers=NUM_LAYERS,\n",
    "        dropout=DROPOUT,\n",
    "        activation=ACTIVATION,\n",
    "        share_embeddings=True  # Enable weight sharing for efficiency\n",
    "    )\n",
    "    \n",
    "    # Move model to device\n",
    "    model = model.to(device)\n",
    "    print(f\"Model successfully moved to {device}\")\n",
    "    \n",
    "except RuntimeError as e:\n",
    "    print(f\"Error during model initialization: {e}\")\n",
    "    print(\"Reducing model size and trying again...\")\n",
    "    \n",
    "    # Try with smaller model if we hit memory issues\n",
    "    D_MODEL = 512\n",
    "    NUM_HEADS = 8\n",
    "    D_FF = 2048\n",
    "    NUM_LAYERS = 4\n",
    "    \n",
    "    model = ImprovedTransformer(\n",
    "        src_vocab_size=VOCAB_SIZE,\n",
    "        tgt_vocab_size=VOCAB_SIZE,\n",
    "        d_model=D_MODEL,\n",
    "        num_heads=NUM_HEADS,\n",
    "        d_ff=D_FF,\n",
    "        num_layers=NUM_LAYERS,\n",
    "        dropout=DROPOUT,\n",
    "        activation=ACTIVATION,\n",
    "        share_embeddings=True\n",
    "    ).to(device)\n",
    "\n",
    "# Calculate model size\n",
    "model_size = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model created with {model_size:,} parameters\")\n",
    "model_size_bytes = sum(p.numel() * p.element_size() for p in model.parameters())\n",
    "print(f\"Model size in memory: {model_size_bytes / 1e9:.2f} GB\")\n",
    "\n",
    "# Initialize optimizer with weight decay\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay': WEIGHT_DECAY},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay': 0.0}\n",
    "]\n",
    "\n",
    "optimizer = AdamW(\n",
    "    optimizer_grouped_parameters,\n",
    "    lr=LEARNING_RATE,\n",
    "    betas=(0.9, 0.999),\n",
    "    eps=1e-8\n",
    ")\n",
    "print(\"Using AdamW optimizer with weight decay\")\n",
    "\n",
    "# Initialize scheduler\n",
    "scheduler = WarmupCosineScheduler(optimizer, warmup_steps, total_steps)\n",
    "\n",
    "# Initialize loss function\n",
    "criterion = LabelSmoothingLoss(smoothing=0.1, ignore_index=-100, vocab_size=VOCAB_SIZE)\n",
    "\n",
    "# Check if mixed precision is causing issues, and disable if needed\n",
    "try:\n",
    "    # Test if half precision works with a small forward pass\n",
    "    with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n",
    "        test_input = torch.ones(1, 10, dtype=torch.long).to(device)\n",
    "        test_output = model(test_input, test_input)\n",
    "    \n",
    "    # If it works, use mixed precision\n",
    "    scaler = GradScaler() if MIXED_PRECISION else None\n",
    "    print(f\"Mixed precision training enabled: {MIXED_PRECISION}\")\n",
    "except RuntimeError as e:\n",
    "    print(f\"Mixed precision test failed: {e}\")\n",
    "    print(\"Disabling mixed precision training\")\n",
    "    MIXED_PRECISION = False\n",
    "    scaler = None\n",
    "\n",
    "# Initialize tensorboard for logging\n",
    "writer = SummaryWriter(log_dir=\"runs/improved_transformer\")\n",
    "\n",
    "# Create directory for checkpoints\n",
    "os.makedirs(\"checkpoints\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91758e9b-e103-4c0a-9201-deee6f18f491",
   "metadata": {},
   "source": [
    "## Training Functions Implementation\n",
    "\n",
    "The training implementation includes several sophisticated functions:\n",
    "\n",
    "1. **Checkpoint Management**:\n",
    "   - `save_checkpoint`: Captures complete training state (model, optimizer, scheduler) along with metadata\n",
    "   - `load_checkpoint`: Restores training from a saved state, enabling seamless resumption\n",
    "   - Includes model configuration in checkpoints for accurate reconstruction\n",
    "\n",
    "2. **Training Epoch Function**:\n",
    "   - Implements a complete training loop with progress tracking\n",
    "   - Handles mixed precision training with appropriate context managers\n",
    "   - Implements gradient accumulation for effective larger batch sizes\n",
    "   - Applies gradient clipping to prevent gradient explosions\n",
    "   - Includes sophisticated error handling for numerical issues\n",
    "   - Implements regular checkpointing during training\n",
    "   - Logs detailed metrics for monitoring\n",
    "\n",
    "3. **Evaluation Function**:\n",
    "   - Clean implementation for validation that disables gradients\n",
    "   - Uses the same batching and processing logic as training for consistency\n",
    "   - Returns validation loss for model selection and early stopping decisions\n",
    "\n",
    "These implementations incorporate numerous best practices for training large-scale transformer models, addressing common challenges like memory constraints, training instability, and the need for robust error handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ecbee486-e5b1-4ec2-a10c-d06c8a5ad168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Functions\n",
    "def save_checkpoint(model, optimizer, scheduler, epoch, step, loss, filename):\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'global_step': step,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state': {\n",
    "            'current_step': scheduler.current_step,\n",
    "            'warmup_steps': scheduler.warmup_steps,\n",
    "            'total_steps': scheduler.total_steps\n",
    "        },\n",
    "        'loss': loss,\n",
    "        'model_config': {\n",
    "            'vocab_size': VOCAB_SIZE,\n",
    "            'd_model': D_MODEL,\n",
    "            'num_heads': NUM_HEADS,\n",
    "            'd_ff': D_FF,\n",
    "            'num_layers': NUM_LAYERS,\n",
    "            'dropout': DROPOUT,\n",
    "            'activation': ACTIVATION\n",
    "        }\n",
    "    }, filename)\n",
    "    print(f\"Checkpoint saved: {filename}\")\n",
    "\n",
    "def load_checkpoint(filename, model, optimizer, scheduler):\n",
    "    if os.path.exists(filename):\n",
    "        print(f\"Loading checkpoint: {filename}\")\n",
    "        checkpoint = torch.load(filename, map_location=device)\n",
    "        \n",
    "        # Load model state\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        \n",
    "        # Load optimizer state\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        \n",
    "        # Restore scheduler state if available\n",
    "        if 'scheduler_state' in checkpoint:\n",
    "            scheduler.current_step = checkpoint['scheduler_state']['current_step']\n",
    "            scheduler.warmup_steps = checkpoint['scheduler_state']['warmup_steps']\n",
    "            scheduler.total_steps = checkpoint['scheduler_state']['total_steps']\n",
    "        \n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        global_step = checkpoint['global_step']\n",
    "        print(f\"Resuming from epoch {start_epoch}, step {global_step}\")\n",
    "        return start_epoch, global_step\n",
    "    else:\n",
    "        print(\"No checkpoint found, starting from scratch\")\n",
    "        return 0, 0\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, criterion, scheduler, scaler=None, \n",
    "               gradient_accumulation_steps=1, max_grad_norm=1.0, epoch=0, global_step=0):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    steps_per_epoch = len(dataloader)\n",
    "    progress_bar = tqdm(enumerate(dataloader), total=steps_per_epoch, \n",
    "                       desc=f\"Training Epoch {epoch+1}\")\n",
    "    \n",
    "    # Reset gradients at the beginning of epoch\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    for step, batch in progress_bar:\n",
    "        try:\n",
    "            # Move batch to device\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            target_input = batch['target_input'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            # Forward pass with mixed precision if enabled\n",
    "            if scaler is not None:\n",
    "                # Use torch.amp.autocast('cuda') instead of torch.cuda.amp.autocast()\n",
    "                with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n",
    "                    outputs = model(input_ids, target_input)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    loss = loss / gradient_accumulation_steps\n",
    "                    \n",
    "                # Backward pass with scaling\n",
    "                scaler.scale(loss).backward()\n",
    "                \n",
    "                # Step optimizer and scaler after accumulation\n",
    "                if (step + 1) % gradient_accumulation_steps == 0 or step == steps_per_epoch - 1:\n",
    "                    # Unscale before gradient clipping\n",
    "                    scaler.unscale_(optimizer)\n",
    "                    clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "                    \n",
    "                    # Update parameters with scaler\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                    \n",
    "                    # Step scheduler\n",
    "                    scheduler.step()\n",
    "                    \n",
    "                    # Reset gradients\n",
    "                    optimizer.zero_grad()\n",
    "                    \n",
    "                    # Update global step\n",
    "                    global_step += 1\n",
    "            else:\n",
    "                # Standard forward pass\n",
    "                outputs = model(input_ids, target_input)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss = loss / gradient_accumulation_steps\n",
    "                \n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "                \n",
    "                # Step optimizer after accumulation\n",
    "                if (step + 1) % gradient_accumulation_steps == 0 or step == steps_per_epoch - 1:\n",
    "                    clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "                    optimizer.step()\n",
    "                    scheduler.step()\n",
    "                    optimizer.zero_grad()\n",
    "                    global_step += 1\n",
    "            \n",
    "            # Track loss\n",
    "            epoch_loss += loss.item() * gradient_accumulation_steps\n",
    "            \n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix({\n",
    "                'loss': epoch_loss / (step + 1),\n",
    "                'lr': optimizer.param_groups[0]['lr']\n",
    "            })\n",
    "            \n",
    "            # Log to tensorboard every 100 steps\n",
    "            if global_step % 100 == 0:\n",
    "                writer.add_scalar('train/loss', loss.item() * gradient_accumulation_steps, global_step)\n",
    "                writer.add_scalar('train/learning_rate', optimizer.param_groups[0]['lr'], global_step)\n",
    "            \n",
    "            # Save checkpoint periodically\n",
    "            if global_step % 1000 == 0:\n",
    "                save_checkpoint(\n",
    "                    model, optimizer, scheduler, epoch, global_step,\n",
    "                    loss.item() * gradient_accumulation_steps,\n",
    "                    f\"checkpoints/checkpoint_step_{global_step}.pt\"\n",
    "                )\n",
    "                \n",
    "        except RuntimeError as e:\n",
    "            if \"overflow\" in str(e) or \"underflow\" in str(e) or \"out of range\" in str(e):\n",
    "                print(f\"Numerical error in batch (skipping): {e}\")\n",
    "                # Skip this batch and continue with next one\n",
    "                optimizer.zero_grad()\n",
    "                if scaler is not None:\n",
    "                    # If we have overflow in mixed precision, reduce the scale\n",
    "                    scaler.update()\n",
    "                continue\n",
    "            else:\n",
    "                # For other runtime errors, re-raise\n",
    "                raise e\n",
    "    \n",
    "    # Return the average loss for the epoch and the updated global step\n",
    "    return epoch_loss / steps_per_epoch, global_step\n",
    "\n",
    "def evaluate(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    eval_loss = 0.0\n",
    "    steps = len(dataloader)\n",
    "    progress_bar = tqdm(enumerate(dataloader), total=steps, desc=\"Evaluating\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for step, batch in progress_bar:\n",
    "            # Move batch to device\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            target_input = batch['target_input'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(input_ids, target_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Track loss\n",
    "            eval_loss += loss.item()\n",
    "            \n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix({'loss': eval_loss / (step + 1)})\n",
    "    \n",
    "    # Return the average loss\n",
    "    return eval_loss / steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699f69ab-9f8a-44cf-8be1-08378585ea41",
   "metadata": {},
   "source": [
    "# The Training Loop: Comprehensive Implementation with Monitoring\r\n",
    "\r\n",
    "## Training Loop Architecture\r\n",
    "\r\n",
    "The training loop implementation in Cell 14 represents a comprehensive approach to transformer training, combining several advanced techniques:\r\n",
    "\r\n",
    "1. **Checkpoint Recovery Mechanism**:\r\n",
    "   - Training begins by attempting to load the latest checkpoint\r\n",
    "   - If found, training resumes from the saved epoch and step\r\n",
    "   - If not, training starts from scratch\r\n",
    "   - This enables resilience against interruptions and allows for training continuation\r\n",
    "\r\n",
    "2. **Progress Tracking and Metrics**:\r\n",
    "   - Training statistics are stored in a JSON file that persists across sessions\r\n",
    "   - The loop maintains and updates best validation loss for model selection\r\n",
    "   - Detailed logging includes per-epoch metrics like training time and learning rates\r\n",
    "\r\n",
    "3. **Main Training Procedure**:\r\n",
    "   - Structured as a double loop: outer loop over epochs, inner loop via the `train_epoch` function\r\n",
    "   - Each epoch follows a train-then-evaluate pattern\r\n",
    "   - The results are logged to TensorBoard for visualization\r\n",
    "   - Progress is displayed to the user with detailed metrics\r\n",
    "\r\n",
    "4. **Model Persistence Strategy**:\r\n",
    "   - Regular checkpoints save model state after each epoch\r\n",
    "   - Special \"best model\" checkpoint preserves the model with lowest validation loss\r\n",
    "   - The final model is saved regardless of performance\r\n",
    "   - Emergency checkpoints are created if training is interrupted\r\n",
    "\r\n",
    "5. **Comprehensive Error Handling**:\r\n",
    "   - Try/except blocks capture both user interruptions and unexpected errors\r\n",
    "   - Upon interruption, an emergency checkpoint is saved\r\n",
    "   - Errors trigger diagnostics and attempt to preserve training progress\r\n",
    "   - TensorBoard writer is properly closed in the finally block\r\n",
    "\r\n",
    "## Implementation Details\r\n",
    "\r\n",
    "The implementation addresses several practical challenges:\r\n",
    "\r\n",
    "1. **Training Duration Tracking**:\r\n",
    "   - Timestamps record the start of training and each epoch\r\n",
    "   - Elapsed time is calculated and reported\r\n",
    "   - This helps with planning and resource allocation\r\n",
    "\r\n",
    "2. **Model Selection**:\r\n",
    "   - Validation loss determines the \"best\" model\r\n",
    "   - The loop tracks and updates this metric after each epoch\r\n",
    "   - Clear logging signals when a new best model is found\r\n",
    "\r\n",
    "3. **Resource Management**:\r\n",
    "   - The loop implements a structured approach to manage memory and computation\r\n",
    "   - Regular state saving prevents excessive resource waste on interruption\r\n",
    "   - Properly closes resources with the finally block\r\n",
    "\r\n",
    "This carefully structured training loop creates a robust training procedure capable of handling the lengthy process of training a large transformer model on the substantial CNN/DailyMail dataset, with appropriate safeguards against common failure modes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "65489f41-9d91-487a-8feb-93a01163247c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training from epoch 1 with 17945 batches per epoch\n",
      "Gradient accumulation steps: 4 (effective batch size: 64)\n",
      "Using mixed precision: True\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb839b6ff015404888d5d6b71fd2292a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 1:   0%|          | 0/17945 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved: checkpoints/checkpoint_step_0.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_0.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_0.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_1000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_1000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_1000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_1000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_2000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_2000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_2000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_2000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_3000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_3000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_3000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_3000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_4000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_4000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_4000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_4000.pt\n",
      "Epoch 1/10 completed in 1634.70s - Loss: 6.4273\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75d1d8a77ce647f39adb0bded32ebc4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/836 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 5.1591\n",
      "Checkpoint saved: checkpoints/latest_checkpoint.pt\n",
      "New best validation loss: 5.1591 - Saving model...\n",
      "Checkpoint saved: checkpoints/best_model.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12b2dd9d0fd347d79409786a23447e0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 2:   0%|          | 0/17945 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved: checkpoints/checkpoint_step_5000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_5000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_5000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_5000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_6000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_6000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_6000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_6000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_7000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_7000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_7000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_7000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_8000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_8000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_8000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_8000.pt\n",
      "Epoch 2/10 completed in 1632.03s - Loss: 4.8443\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f0ae9497ac84f52b827d004decc5dda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/836 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 4.1304\n",
      "Checkpoint saved: checkpoints/latest_checkpoint.pt\n",
      "New best validation loss: 4.1304 - Saving model...\n",
      "Checkpoint saved: checkpoints/best_model.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb1c64e7b38a481bab459b7d706b52dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 3:   0%|          | 0/17945 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved: checkpoints/checkpoint_step_9000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_9000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_9000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_9000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_10000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_10000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_10000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_10000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_11000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_11000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_11000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_11000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_12000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_12000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_12000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_12000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_13000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_13000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_13000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_13000.pt\n",
      "Epoch 3/10 completed in 1642.77s - Loss: 4.0675\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d3151af554d41e9b96e0f00abed1d18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/836 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 3.7069\n",
      "Checkpoint saved: checkpoints/latest_checkpoint.pt\n",
      "New best validation loss: 3.7069 - Saving model...\n",
      "Checkpoint saved: checkpoints/best_model.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d439360293aa4118ae5f321b87892e6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 4:   0%|          | 0/17945 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved: checkpoints/checkpoint_step_14000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_14000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_14000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_14000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_15000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_15000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_15000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_15000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_16000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_16000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_16000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_16000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_17000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_17000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_17000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_17000.pt\n",
      "Epoch 4/10 completed in 1631.56s - Loss: 3.7231\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf3c2d431b374a908fa6a7a97cdd6f1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/836 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 3.5773\n",
      "Checkpoint saved: checkpoints/latest_checkpoint.pt\n",
      "New best validation loss: 3.5773 - Saving model...\n",
      "Checkpoint saved: checkpoints/best_model.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b7dc48951c34e5eb0c43dd880216371",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 5:   0%|          | 0/17945 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved: checkpoints/checkpoint_step_18000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_18000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_18000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_18000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_19000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_19000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_19000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_19000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_20000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_20000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_20000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_20000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_21000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_21000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_21000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_21000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_22000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_22000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_22000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_22000.pt\n",
      "Epoch 5/10 completed in 1648.66s - Loss: 3.5320\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa698ff3ea6c4ff6b6e2dbc4066f873f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/836 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 3.4989\n",
      "Checkpoint saved: checkpoints/latest_checkpoint.pt\n",
      "New best validation loss: 3.4989 - Saving model...\n",
      "Checkpoint saved: checkpoints/best_model.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0842e611b92a4a9f810006d15a6d0fa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 6:   0%|          | 0/17945 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved: checkpoints/checkpoint_step_23000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_23000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_23000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_23000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_24000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_24000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_24000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_24000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_25000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_25000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_25000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_25000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_26000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_26000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_26000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_26000.pt\n",
      "Epoch 6/10 completed in 1652.61s - Loss: 3.3816\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a4844e84d0840139b2a26314eba0204",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/836 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 3.4541\n",
      "Checkpoint saved: checkpoints/latest_checkpoint.pt\n",
      "New best validation loss: 3.4541 - Saving model...\n",
      "Checkpoint saved: checkpoints/best_model.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "015ccb0b0917452ab9f371e657ac2e2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 7:   0%|          | 0/17945 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved: checkpoints/checkpoint_step_27000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_27000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_27000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_27000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_28000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_28000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_28000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_28000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_29000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_29000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_29000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_29000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_30000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_30000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_30000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_30000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_31000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_31000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_31000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_31000.pt\n",
      "Epoch 7/10 completed in 1645.90s - Loss: 3.2534\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e57de519845f4279a3a9aa8f485e5f75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/836 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 3.4361\n",
      "Checkpoint saved: checkpoints/latest_checkpoint.pt\n",
      "New best validation loss: 3.4361 - Saving model...\n",
      "Checkpoint saved: checkpoints/best_model.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4052ff0215ba49aab6b87b3f4582e098",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 8:   0%|          | 0/17945 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved: checkpoints/checkpoint_step_32000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_32000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_32000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_32000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_33000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_33000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_33000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_33000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_34000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_34000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_34000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_34000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_35000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_35000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_35000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_35000.pt\n",
      "Epoch 8/10 completed in 1638.47s - Loss: 3.1483\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90f6a72029d84447841cfe0da021e8e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/836 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 3.4274\n",
      "Checkpoint saved: checkpoints/latest_checkpoint.pt\n",
      "New best validation loss: 3.4274 - Saving model...\n",
      "Checkpoint saved: checkpoints/best_model.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "194c74f667324421a789e2e1d8b31503",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 9:   0%|          | 0/17945 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved: checkpoints/checkpoint_step_36000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_36000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_36000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_36000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_37000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_37000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_37000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_37000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_38000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_38000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_38000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_38000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_39000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_39000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_39000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_39000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_40000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_40000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_40000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_40000.pt\n",
      "Epoch 9/10 completed in 1661.82s - Loss: 3.0758\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb40e7d361e04aa6a41d16f0d6a7f351",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/836 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 3.4357\n",
      "Checkpoint saved: checkpoints/latest_checkpoint.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4156581aacd49068e3b931b4a3a09ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 10:   0%|          | 0/17945 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved: checkpoints/checkpoint_step_41000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_41000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_41000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_41000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_42000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_42000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_42000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_42000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_43000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_43000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_43000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_43000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_44000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_44000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_44000.pt\n",
      "Checkpoint saved: checkpoints/checkpoint_step_44000.pt\n",
      "Epoch 10/10 completed in 1644.76s - Loss: 3.0394\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "499d0df1b3d74161a76d4c7c0c4ec1dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/836 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 3.4382\n",
      "Checkpoint saved: checkpoints/latest_checkpoint.pt\n",
      "Training completed in 281.77 minutes\n",
      "Best validation loss: 3.4274\n",
      "Checkpoint saved: checkpoints/final_model.pt\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "# Auto-resume from checkpoint\n",
    "checkpoint_path = \"checkpoints/latest_checkpoint.pt\"\n",
    "start_epoch, global_step = load_checkpoint(checkpoint_path, model, optimizer, scheduler) if os.path.exists(checkpoint_path) else (0, 0)\n",
    "\n",
    "# Track best model\n",
    "best_val_loss = float('inf')\n",
    "training_stats = []\n",
    "\n",
    "# Load existing stats if available\n",
    "if os.path.exists('training_stats.json'):\n",
    "    with open('training_stats.json', 'r') as f:\n",
    "        training_stats = json.load(f)\n",
    "\n",
    "# Training start time\n",
    "train_start_time = time.time()\n",
    "\n",
    "print(f\"Starting training from epoch {start_epoch+1} with {len(train_dataloader)} batches per epoch\")\n",
    "print(f\"Gradient accumulation steps: {GRADIENT_ACCUMULATION_STEPS} (effective batch size: {EFFECTIVE_BATCH_SIZE})\")\n",
    "print(f\"Using mixed precision: {MIXED_PRECISION}\")\n",
    "\n",
    "# Training loop\n",
    "try:\n",
    "    for epoch in range(start_epoch, NUM_EPOCHS):\n",
    "        epoch_start_time = time.time()\n",
    "        \n",
    "        # Train one epoch\n",
    "        train_loss, global_step = train_epoch(\n",
    "            model=model,\n",
    "            dataloader=train_dataloader,\n",
    "            optimizer=optimizer,\n",
    "            criterion=criterion,\n",
    "            scheduler=scheduler,\n",
    "            scaler=scaler,\n",
    "            gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "            max_grad_norm=MAX_GRAD_NORM,\n",
    "            epoch=epoch,\n",
    "            global_step=global_step\n",
    "        )\n",
    "        \n",
    "        # Calculate epoch time\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{NUM_EPOCHS} completed in {epoch_time:.2f}s - Loss: {train_loss:.4f}\")\n",
    "        \n",
    "        # Evaluate\n",
    "        val_loss = evaluate(model, val_dataloader, criterion)\n",
    "        print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "        \n",
    "        # Log to tensorboard\n",
    "        writer.add_scalar('epoch/train_loss', train_loss, epoch)\n",
    "        writer.add_scalar('epoch/val_loss', val_loss, epoch)\n",
    "        writer.add_scalar('epoch/time', epoch_time, epoch)\n",
    "        \n",
    "        # Save checkpoint after each epoch\n",
    "        save_checkpoint(\n",
    "            model, optimizer, scheduler, epoch, global_step, val_loss,\n",
    "            checkpoint_path\n",
    "        )\n",
    "        \n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            print(f\"New best validation loss: {best_val_loss:.4f} - Saving model...\")\n",
    "            save_checkpoint(\n",
    "                model, optimizer, scheduler, epoch, global_step, val_loss,\n",
    "                \"checkpoints/best_model.pt\"\n",
    "            )\n",
    "        \n",
    "        # Track training stats\n",
    "        training_stats.append({\n",
    "            'epoch': epoch + 1,\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss,\n",
    "            'epoch_time': epoch_time,\n",
    "            'learning_rate': optimizer.param_groups[0]['lr']\n",
    "        })\n",
    "        \n",
    "        # Save training stats\n",
    "        with open('training_stats.json', 'w') as f:\n",
    "            json.dump(training_stats, f)\n",
    "    \n",
    "    # Training complete\n",
    "    total_training_time = time.time() - train_start_time\n",
    "    print(f\"Training completed in {total_training_time/60:.2f} minutes\")\n",
    "    print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "    \n",
    "    # Save final model\n",
    "    save_checkpoint(\n",
    "        model, optimizer, scheduler, NUM_EPOCHS-1, global_step, val_loss,\n",
    "        \"checkpoints/final_model.pt\"\n",
    "    )\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Training interrupted by user!\")\n",
    "    # Save emergency checkpoint\n",
    "    try:\n",
    "        save_checkpoint(\n",
    "            model, optimizer, scheduler, epoch, global_step, train_loss,\n",
    "            \"checkpoints/interrupted_checkpoint.pt\"\n",
    "        )\n",
    "        print(\"Emergency checkpoint saved\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not save emergency checkpoint: {e}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error during training: {e}\")\n",
    "    # Try to save emergency checkpoint\n",
    "    try:\n",
    "        save_checkpoint(\n",
    "            model, optimizer, scheduler, epoch, global_step, train_loss,\n",
    "            \"checkpoints/error_checkpoint.pt\"\n",
    "        )\n",
    "        print(\"Emergency checkpoint saved\")\n",
    "    except Exception as e2:\n",
    "        print(f\"Could not save emergency checkpoint: {e2}\")\n",
    "\n",
    "finally:\n",
    "    # Close tensorboard writer\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70124b46-b3c5-4406-9c28-d03e7476cde4",
   "metadata": {},
   "source": [
    "## Approach 1: Built-in Generate Method\n",
    "\n",
    "### Design Philosophy\n",
    "This approach leverages the model's native generation capabilities, prioritizing implementation cleanliness and code maintainability over specialized controls for summarization.\n",
    "\n",
    "### Key Implementation Features\n",
    "\n",
    "- **Architectural Integration**: Directly uses the model's generation pipeline without additional custom logic\n",
    "\n",
    "- **Simplified Parameter Control**: Offers configuration of standard generation parameters (beam size, temperature, top-k)\n",
    "\n",
    "- **Minimal Post-Processing**: Applies only basic cleanup to the generated text\n",
    "\n",
    "### Performance Analysis\n",
    "The built-in approach achieves lower scores across all metrics:\n",
    "- ROUGE-1: 0.2315\n",
    "- ROUGE-2: 0.0716\n",
    "- ROUGE-L: 0.2192\n",
    "\n",
    "The generated summaries tend to be significantly longer (62.76 words) than references (34.98 words), which negatively impacts precision and overall quality.\n",
    "\n",
    "## Technical Implications\n",
    "\n",
    "The performance gap between these approaches demonstrates that text generation for specialized tasks like summarization benefits significantly from task-specific constraints and controls that go beyond general-purpose generation mechanisms.\n",
    "\n",
    "The custom approach shows that:\n",
    "1. **Length control is critical** for summarization quality\n",
    "2. **Repetition prevention** substantially improves readability\n",
    "3. **Task-specific beam search scoring** leads to more appropriate summary selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "954dcd55-d165-47dd-8e41-0e76db2aa04c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading best model...\n",
      "\n",
      "Sample Article (first 200 chars):\n",
      "(CNN)The Palestinian Authority officially became the 123rd member of the International Criminal Court on Wednesday, a step that gives the court jurisdiction over alleged crimes in Palestinian territor...\n",
      "\n",
      "Reference Summary:\n",
      "Membership gives the ICC jurisdiction over alleged crimes committed in Palestinian territories since last June .\n",
      "Israel and the United States opposed the move, which could open the door to war crimes investigations against Israelis .\n",
      "\n",
      "Generated Summary:\n",
      "Palestinian Authority officially becomes a 12 3rd member of the International Criminal Court . Palestinians signed Rome Stat ute in January , when they also accepted its jurisdiction over alleged crimes . Palestinian Foreign Minister Netanyahu says . ad al - M alk i ' s efforts to join the body . The Netherlands ' efforts to join the body . Palestinians may be.\n",
      "\n",
      "ROUGE Scores for sample:\n",
      "ROUGE-1: 0.2000\n",
      "ROUGE-2: 0.0690\n",
      "ROUGE-L: 0.1750\n",
      "\n",
      "=== Running full evaluation ===\n",
      "Generating summaries for 50 examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/50 [00:01<00:51,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 1:\n",
      "Article (truncated): (CNN)The Palestinian Authority officially became the 123rd member of the International Criminal Court on Wednesday, a step that gives the court jurisdiction over alleged crimes in Palestinian territor...\n",
      "Reference: Membership gives the ICC jurisdiction over alleged crimes committed in Palestinian territories since last June .\n",
      "Israel and the United States opposed the move, which could open the door to war crimes investigations against Israelis .\n",
      "Generated: NEW : Palestinian Authority officially a State Party member of Palestine member of the International Criminal Court . Palestinians may be subject to counter - charges as well . Palestinians may be subject to counter - charges as well . Palestinian Foreign Minister Ri ad al - M alk i.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 11/50 [00:11<00:40,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 11:\n",
      "Article (truncated): London (CNN)A 19-year-old man was charged Wednesday with terror offenses after he was arrested as he returned to Britain from Turkey, London's Metropolitan Police said. Yahya Rashid, a UK national fro...\n",
      "Reference: London's Metropolitan Police say the man was arrested at Luton airport after landing on a flight from Istanbul .\n",
      "He's been charged with terror offenses allegedly committed since the start of November .\n",
      "Generated: Yah ya Rashid , 19 , is due to appear in Westminster Magistrates ' Court on Wednesday . He was arrested at Westminster Magistrates ' Court on Wednesday . Rashid says . is due to appear in court on Wednesday . Rashid is due to appear in Westminster Magistrates ' Court on Wednesday . He is due to appear in Westminster Magistrates ' Court.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 21/50 [00:21<00:30,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 21:\n",
      "Article (truncated): Norfolk, Virginia (CNN)The second mate of the Houston Express probably couldn't believe what he was seeing. Hundreds of miles from land there was a small boat nearby. At first it looked abandoned. It ...\n",
      "Reference: Father: \"I know he went through what he went through\"\n",
      "Louis Jordan was found on his sailboat, which was listing and in bad shape, rescuer says .\n",
      "He appears to be in good shape, physically and mentally .\n",
      "Generated: The 1 , \" He \" looked good condition \" It took so long \" so he couldn ' d been drifting on the 35 - foot - foot capsized ' drifting on the 35 - foot Pearson sail boat for more than two months ' The weather wouldn ' father : \" He looked good . Had n ' t lost too much.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 31/50 [00:32<00:21,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 31:\n",
      "Article (truncated): (CNN)Police in the Indian city of Malegaon, in the western state of Maharashtra, are requiring identity cards for an unusual group of residents: Cattle. Following a recent state-wide ban on the sale a...\n",
      "Reference: Authorities in the Indian city of Malegaon have asked residents to take a 'mugshot' of their cattle .\n",
      "Cows are revered by the majority Hindu population, and many parts of the country have laws banning the slaughter of cattle .\n",
      "Officials in Malegaon believe this is the best way to solve cow slaughter cases and enforce the law .\n",
      "Generated: Mah ar ash tra are requiring identity cards for an unusual group of residents . Mah ar ash tra is the only way to solve cow slaughter cases and enforce the law . C ows are considered holy and revered by that state ' s majority Hindu population . Ban on the government doesn ' t have a right to interfere in an individual.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 41/50 [00:46<00:12,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 41:\n",
      "Article (truncated): (CNN)A high temperature of 63.5 degrees Fahrenheit might sound like a pleasant day in early spring -- unless you're in Antarctica. The chilly continent recorded the temperature (15.5 degrees Celsius) ...\n",
      "Reference: High temperatures are recorded on the northern tip of the Antarctica Peninsula .\n",
      "The World Meteorological Organization will make the final determination .\n",
      "Generated: The temperature was recorded at Argentina ' s Es per anza Base on the northern tip of the Antarctica Peninsula . The World Meteorological Organization , a specialized United Nations agency , is in the process of setting up an ad - hoc committee of about 10 blue - ribbon clim at ologists . The committee will examine the equipment used to measure the.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:59<00:00,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculating ROUGE scores...\n",
      "\n",
      "ROUGE Scores:\n",
      "ROUGE-1: 0.2315\n",
      "ROUGE-2: 0.0716\n",
      "ROUGE-L: 0.2192\n",
      "\n",
      "Length Statistics:\n",
      "Average Reference Length: 34.98 words\n",
      "Average Summary Length: 62.76 words\n",
      "Results saved to model_evaluation_results.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'rouge-1': {'r': 0.2973948197047714,\n",
       "  'p': 0.19780151505118926,\n",
       "  'f': 0.23153300918980477},\n",
       " 'rouge-2': {'r': 0.10313627817083003,\n",
       "  'p': 0.057368809503389996,\n",
       "  'f': 0.0716117097415556},\n",
       " 'rouge-l': {'r': 0.28218548934238874,\n",
       "  'p': 0.18721430976169418,\n",
       "  'f': 0.21923602877855036}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generating with inbuilt generate() in the model\n",
    "def generate_with_model(model, tokenizer, article, max_length=64, beam_size=5, \n",
    "                       top_k=50, temperature=0.7, early_stopping=True):\n",
    "    \"\"\"Generate a summary using the model's built-in generate() method\"\"\"\n",
    "    # Tokenize input\n",
    "    encoding = tokenizer.encode(\n",
    "        article, \n",
    "        max_length=MAX_INPUT_LENGTH, \n",
    "        padding='max_length', \n",
    "        truncation=True\n",
    "    )\n",
    "    \n",
    "    # Convert to tensor and move to device\n",
    "    input_ids = torch.tensor(encoding['input_ids']).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Generate summary using the model's built-in method\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(\n",
    "            input_ids,\n",
    "            max_length=max_length,\n",
    "            beam_size=beam_size,\n",
    "            top_k=top_k,\n",
    "            temperature=temperature,\n",
    "            early_stopping=early_stopping\n",
    "        )\n",
    "    \n",
    "    # Decode the generated tokens\n",
    "    generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Basic post-processing\n",
    "    generated_text = re.sub(r'\\s+', ' ', generated_text).strip()\n",
    "    if generated_text and not generated_text[-1] in '.!?':\n",
    "        generated_text += '.'\n",
    "    \n",
    "    return generated_text\n",
    "\n",
    "def evaluate_rouge_scores(model, tokenizer, test_dataset, num_examples=100):\n",
    "    \"\"\"Evaluate model performance using ROUGE metrics\"\"\"\n",
    "    # Import required libraries\n",
    "    from rouge import Rouge\n",
    "    rouge = Rouge()\n",
    "    \n",
    "    # Prepare lists to store results\n",
    "    references = []\n",
    "    summaries = []\n",
    "    \n",
    "    # Process examples\n",
    "    print(f\"Generating summaries for {num_examples} examples...\")\n",
    "    test_subset = test_dataset.select(range(min(num_examples, len(test_dataset))))\n",
    "    \n",
    "    for i, example in enumerate(tqdm(test_subset)):\n",
    "        article = example['article']\n",
    "        reference = example['highlights']\n",
    "        \n",
    "        # Generate summary\n",
    "        generated_summary = generate_with_model(\n",
    "            model, \n",
    "            tokenizer, \n",
    "            article,\n",
    "            max_length=64,  # Adjust as needed\n",
    "            beam_size=4,\n",
    "            top_k=50,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        \n",
    "        references.append(reference)\n",
    "        summaries.append(generated_summary)\n",
    "        \n",
    "        # Display progress examples\n",
    "        if i % 10 == 0 or i == 0:\n",
    "            print(f\"\\nExample {i+1}:\")\n",
    "            print(f\"Article (truncated): {article[:200]}...\")\n",
    "            print(f\"Reference: {reference}\")\n",
    "            print(f\"Generated: {generated_summary}\")\n",
    "    \n",
    "    # Calculate ROUGE scores\n",
    "    print(\"\\nCalculating ROUGE scores...\")\n",
    "    scores = rouge.get_scores(summaries, references, avg=True)\n",
    "    \n",
    "    # Print scores\n",
    "    print(\"\\nROUGE Scores:\")\n",
    "    print(f\"ROUGE-1: {scores['rouge-1']['f']:.4f}\")\n",
    "    print(f\"ROUGE-2: {scores['rouge-2']['f']:.4f}\")\n",
    "    print(f\"ROUGE-L: {scores['rouge-l']['f']:.4f}\")\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    ref_lengths = [len(r.split()) for r in references]\n",
    "    summary_lengths = [len(s.split()) for s in summaries]\n",
    "    \n",
    "    print(\"\\nLength Statistics:\")\n",
    "    print(f\"Average Reference Length: {sum(ref_lengths)/len(ref_lengths):.2f} words\")\n",
    "    print(f\"Average Summary Length: {sum(summary_lengths)/len(summary_lengths):.2f} words\")\n",
    "    \n",
    "    # Save results to CSV\n",
    "    results_df = pd.DataFrame({\n",
    "        'article': [example['article'][:500] + '...' for example in test_subset],\n",
    "        'reference': references,\n",
    "        'generated': summaries,\n",
    "        'rouge1': [scores['rouge-1']['f']] * len(summaries),\n",
    "        'rouge2': [scores['rouge-2']['f']] * len(summaries),\n",
    "        'rougeL': [scores['rouge-l']['f']] * len(summaries)\n",
    "    })\n",
    "    \n",
    "    results_df.to_csv('model_evaluation_results.csv', index=False)\n",
    "    print(\"Results saved to model_evaluation_results.csv\")\n",
    "    \n",
    "    return scores\n",
    "\n",
    "# Load model and run evaluation\n",
    "print(\"Loading best model...\")\n",
    "best_model_path = r\"C:\\Users\\nisha\\Downloads\\best_model (1).pt\"\n",
    "checkpoint = torch.load(best_model_path, map_location=device)\n",
    "\n",
    "if 'model_config' in checkpoint:\n",
    "    config = checkpoint['model_config']\n",
    "    model = ImprovedTransformer(\n",
    "        src_vocab_size=config['vocab_size'],\n",
    "        tgt_vocab_size=config['vocab_size'],\n",
    "        d_model=config['d_model'],\n",
    "        num_heads=config['num_heads'],\n",
    "        d_ff=config['d_ff'],\n",
    "        num_layers=config['num_layers'],\n",
    "        dropout=config['dropout'],\n",
    "        activation=config['activation']\n",
    "    ).to(device)\n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "# Single example test\n",
    "sample_article = cnn_dailymail['test'][0]['article']\n",
    "reference = cnn_dailymail['test'][0]['highlights']\n",
    "\n",
    "print(\"\\nSample Article (first 200 chars):\")\n",
    "print(sample_article[:200] + \"...\")\n",
    "\n",
    "print(\"\\nReference Summary:\")\n",
    "print(reference)\n",
    "\n",
    "print(\"\\nGenerated Summary:\")\n",
    "summary = generate_with_model(model, tokenizer, sample_article)\n",
    "print(summary)\n",
    "\n",
    "# Calculate ROUGE for single example\n",
    "rouge = Rouge()\n",
    "scores = rouge.get_scores(summary, reference)[0]\n",
    "print(\"\\nROUGE Scores for sample:\")\n",
    "print(f\"ROUGE-1: {scores['rouge-1']['f']:.4f}\")\n",
    "print(f\"ROUGE-2: {scores['rouge-2']['f']:.4f}\")\n",
    "print(f\"ROUGE-L: {scores['rouge-l']['f']:.4f}\")\n",
    "\n",
    "# Full evaluation\n",
    "print(\"\\n=== Running full evaluation ===\")\n",
    "evaluate_rouge_scores(model, tokenizer, cnn_dailymail['test'], num_examples=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665e07e2-4796-4a7b-855e-e33e80bc8041",
   "metadata": {},
   "source": [
    "## Approach 2: Custom Beam Search Implementation\n",
    "\n",
    "### Design Philosophy\n",
    "The custom beam search algorithm provides fine-grained control over the generation process through several specialized mechanisms designed specifically for summarization. This approach prioritizes quality and readability over implementation simplicity.\n",
    "\n",
    "### Key Implementation Features\n",
    "\n",
    "- **Dynamic Length Control**: Adjusts generation length based on reference summary length, with configurable penalties to favor concise outputs\n",
    "  \n",
    "- **N-gram Repetition Prevention**: Actively blocks 3-gram repetitions that commonly occur in transformer outputs, dramatically improving readability\n",
    "\n",
    "- **Sophisticated Beam Scoring**: Implements length normalization and adaptive scoring to balance between fluency and conciseness\n",
    "\n",
    "- **Comprehensive Post-Processing Pipeline**: Includes sentence deduplication, proper ending punctuation, and removal of common generation artifacts\n",
    "\n",
    "### Performance Analysis\n",
    "The custom approach achieves superior results across all ROUGE metrics:\n",
    "- ROUGE-1: 0.2483\n",
    "- ROUGE-2: 0.0820\n",
    "- ROUGE-L: 0.2310\n",
    "\n",
    "Most notably, the generated summaries maintain lengths (average 42.62 words) much closer to reference summaries (34.46 words), contributing to higher precision scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e797cdf7-e267-4855-a812-6f319f66c72f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading best model for evaluation...\n",
      "Recreating model from saved config: {'vocab_size': 32000, 'd_model': 768, 'num_heads': 12, 'd_ff': 3072, 'num_layers': 6, 'dropout': 0.1, 'activation': 'gelu'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Using default tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n",
      "Generating summaries for 100 examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/100 [00:09<15:38,  9.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 1:\n",
      "Article (truncated): (CNN)The Palestinian Authority officially became the 123rd member of the International Criminal Court on Wednesday, a step that gives the court jurisdiction over alleged crimes in Palestinian territor...\n",
      "Reference: Membership gives the ICC jurisdiction over alleged crimes committed in Palestinian territories since last June .\n",
      "Israel and the United States opposed the move, which could open the door to war crimes investigations against Israelis .\n",
      "Generated: The ICC officially becomes the 12 3rd member of the International Criminal Court . The formal access ion was marked with a ceremony at The Hague , in the Netherlands . Palestinians may be subject to counter - charges as well . Palestinian Foreign Minister.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 11/100 [00:41<04:17,  2.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 11:\n",
      "Article (truncated): London (CNN)A 19-year-old man was charged Wednesday with terror offenses after he was arrested as he returned to Britain from Turkey, London's Metropolitan Police said. Yahya Rashid, a UK national fro...\n",
      "Reference: London's Metropolitan Police say the man was arrested at Luton airport after landing on a flight from Istanbul .\n",
      "He's been charged with terror offenses allegedly committed since the start of November .\n",
      "Generated: Yah ya Rashid , 19 , was arrested at Luton airport on Tuesday . He is due to appear in Westminster Magistrates ' Court on Wednesday . Rashid is due in court on Wednesday , police say . He ' s been.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 21/100 [01:12<04:08,  3.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 21:\n",
      "Article (truncated): Norfolk, Virginia (CNN)The second mate of the Houston Express probably couldn't believe what he was seeing. Hundreds of miles from land there was a small boat nearby. At first it looked abandoned. It ...\n",
      "Reference: Father: \"I know he went through what he went through\"\n",
      "Louis Jordan was found on his sailboat, which was listing and in bad shape, rescuer says .\n",
      "He appears to be in good shape, physically and mentally .\n",
      "Generated: Man in bad shape , listing to one side , saw there was a boat wrecked . He ' d been drifting on the 35 - foot Pearson sail boat for more than two months . His father says he was expecting his son to look different.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 31/100 [01:42<03:27,  3.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 31:\n",
      "Article (truncated): (CNN)Police in the Indian city of Malegaon, in the western state of Maharashtra, are requiring identity cards for an unusual group of residents: Cattle. Following a recent state-wide ban on the sale a...\n",
      "Reference: Authorities in the Indian city of Malegaon have asked residents to take a 'mugshot' of their cattle .\n",
      "Cows are revered by the majority Hindu population, and many parts of the country have laws banning the slaughter of cattle .\n",
      "Officials in Malegaon believe this is the best way to solve cow slaughter cases and enforce the law .\n",
      "Generated: Indian police ask residents to take mug shot of their cattle and submit it to the police . C ows are holy and revered by that state ' s majority Hindu population . The ban on the sale and consumption of beef is still per missible . The slaughter of buff alo es is still a concern for the.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 41/100 [02:12<02:46,  2.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 41:\n",
      "Article (truncated): (CNN)A high temperature of 63.5 degrees Fahrenheit might sound like a pleasant day in early spring -- unless you're in Antarctica. The chilly continent recorded the temperature (15.5 degrees Celsius) ...\n",
      "Reference: High temperatures are recorded on the northern tip of the Antarctica Peninsula .\n",
      "The World Meteorological Organization will make the final determination .\n",
      "Generated: The temperature was recorded at Argentina ' s Es per anza Base on the northern tip of the Antarctica Peninsula . The agency is in the process of setting up an ad.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 51/100 [02:40<02:13,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 51:\n",
      "Article (truncated): (CNN)According to an outside review by Columbia Journalism School professors, \"(a)n institutional failure at Rolling Stone resulted in a deeply flawed article about a purported gang rape at the Univer...\n",
      "Reference: An outside review found that a Rolling Stone article about campus rape was \"deeply flawed\"\n",
      "Danny Cevallos says that there are obstacles to a successful libel case, should one be filed .\n",
      "Generated: Columbia Journal ism School professors say the university ' s \" failure \" was a n institutional failure . The university says it ' s a \" governmental entity \" to eliminate U VA . The publication of the article is.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 61/100 [03:14<02:13,  3.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 61:\n",
      "Article (truncated): Hong Kong (CNN)Six people were hurt after an explosion at a controversial chemical plant in China's southeastern Fujian province sparked a huge fire, provincial authorities told state media. The plant...\n",
      "Reference: A blast rocks a chemical plant in China's southeastern Fujian province for the second time in two years .\n",
      "Six were injured after the explosion and are being hospitalized .\n",
      "The explosion was triggered by an oil leak, though local media has not reported any toxic chemical spills .\n",
      "Generated: The blast occurred at an oil storage facility Monday night . Residents living close to the plant had heard the explosion . The plant was hit by another explosion in July 2013 . The explosion sparked a huge fire in China ' s southeastern Fuj ian province . The Zhang zhou plant produces par ax yl ene (.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 71/100 [03:42<01:18,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 71:\n",
      "Article (truncated): (CNN)A nuclear submarine being repaired at a Russian shipyard has caught on fire, according to a law enforcement source speaking to Russia's state-run news agency ITAR-Tass. \"The submarine is in a dry...\n",
      "Reference: Submarine is in Zvyozdochka shipyard, in northwestern Russia .\n",
      "No \"dangerous\" substances on the submarine, shipyard spokesman told ITAR-Tass .\n",
      "Generated: A nuclear submarine is being repaired at a Russian ship yard . The sub is being used as we lding work . The fire began on a sub in.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 81/100 [04:13<01:01,  3.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 81:\n",
      "Article (truncated): Cedar Falls, Iowa (CNN)As aides politely tried to rush Ted Cruz from an event in Cedar Falls to one in Cedar Rapids, Iowa, on Thursday, the presidential candidate continued shaking hands with anyone w...\n",
      "Reference: Ted Cruz has built a brand as a stalwart conservative on fiscal issues .\n",
      "But he's also eager to champion social issues at a time when many Republicans are eager to avoid them .\n",
      "Cruz says the GOP needs to unite young libertarian-minded voters and evangelicals .\n",
      "Generated: Ted Cruz drew crowds during his two - day swing across the state . The Iowa senator regularly avoids using a podium . He ' s the only official contender in the race , and is working a room . Cruz has built a brand as a stal wart conservative willing to buck GOP leadership .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 91/100 [04:51<00:31,  3.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 91:\n",
      "Article (truncated): (CNN)They're not gonna take it anymore. Really. Twisted Sister says that its 2016 tour will be its last, according to a press release. Next year marks the band's 40th anniversary, and to celebrate, th...\n",
      "Reference: Twisted Sister's 2016 tour will be its last .\n",
      "Band will celebrate 40 years in 2016 .\n",
      "Twisted Sister drummer A.J. Pero died in March .\n",
      "Generated: Tw isted Sister says 2016 tour will be its last , according to a press release . The tour is titled \" Forty and F * ck It \" The band will play with a.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [05:22<00:00,  3.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ROUGE Scores:\n",
      "ROUGE-1: 0.2483\n",
      "ROUGE-2: 0.0820\n",
      "ROUGE-L: 0.2310\n",
      "\n",
      "Additional Metrics:\n",
      "Average Reference Length: 34.46 words\n",
      "Average Summary Length: 42.62 words\n",
      "Results saved to evaluation_results.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Improved Generation and Evaluation with Length Control\n",
    "import nltk\n",
    "from rouge_score import rouge_scorer\n",
    "from rouge import Rouge\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "def generate_summary(model, tokenizer, article, max_length=64, beam_size=5, \n",
    "                     min_length=20, length_penalty=0.6, \n",
    "                     top_k=0, temperature=0.5, early_stopping=True):\n",
    "\n",
    "    # Prepare input\n",
    "    encoding = tokenizer.encode(article, max_length=MAX_INPUT_LENGTH, padding='max_length', truncation=True)\n",
    "    input_ids = torch.tensor(encoding['input_ids']).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Generate with beam search\n",
    "    with torch.no_grad():\n",
    "        # Initialize generation parameters\n",
    "        batch_size = input_ids.size(0)\n",
    "        src_mask = model.create_src_mask(input_ids)\n",
    "        encoder_output = model.encoder(input_ids, src_mask)\n",
    "        \n",
    "        # Initialize beams with SOS token\n",
    "        beams = [{'tokens': [model.sos_token_id], \n",
    "                 'score': 0.0, \n",
    "                 'finished': False} for _ in range(beam_size)]\n",
    "        \n",
    "        # Generate up to max_length tokens\n",
    "        for step in range(max_length):\n",
    "            new_beams = []\n",
    "            \n",
    "            # Process each active beam\n",
    "            for beam in beams:\n",
    "                if beam['finished']:\n",
    "                    new_beams.append(beam)\n",
    "                    continue\n",
    "                \n",
    "                # Convert beam tokens to tensor\n",
    "                curr_ids = torch.tensor([beam['tokens']], dtype=torch.long, device=device)\n",
    "                \n",
    "                # Forward pass through decoder\n",
    "                tgt_mask = model.create_tgt_mask(curr_ids)\n",
    "                decoder_output = model.decoder(curr_ids, encoder_output, src_mask, tgt_mask)\n",
    "                logits = model.final_layer(decoder_output[:, -1])\n",
    "                \n",
    "                # Apply temperature\n",
    "                logits = logits / temperature\n",
    "                \n",
    "                # Get top k tokens\n",
    "                if top_k > 0:\n",
    "                    topk_logits, topk_indices = torch.topk(logits, k=top_k)\n",
    "                    logits = torch.full_like(logits, float('-inf'))\n",
    "                    logits.scatter_(1, topk_indices, topk_logits)\n",
    "                \n",
    "                # Convert to probabilities\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "                \n",
    "                # Get top beam_size candidates\n",
    "                topk_probs, topk_ids = torch.topk(probs, k=beam_size * 2)\n",
    "                \n",
    "                # Create new candidates\n",
    "                for i in range(topk_ids.size(1)):\n",
    "                    token_id = topk_ids[0, i].item()\n",
    "                    token_prob = topk_probs[0, i].item()\n",
    "                    \n",
    "                    # Skip tokens that would create 3-gram repetitions\n",
    "                    if len(beam['tokens']) >= 3:\n",
    "                        last_3gram = beam['tokens'][-2:]\n",
    "                        skip = False\n",
    "                        for j in range(len(beam['tokens']) - 2):\n",
    "                            if beam['tokens'][j:j+2] == last_3gram and beam['tokens'][j+2] == token_id:\n",
    "                                skip = True\n",
    "                                break\n",
    "                        if skip:\n",
    "                            continue\n",
    "                    \n",
    "                    # Calculate new score\n",
    "                    # Length normalization: (5 + len)^length_penalty / (5^length_penalty)\n",
    "                    new_len = len(beam['tokens']) + 1\n",
    "                    length_norm = ((5 + new_len) ** length_penalty) / (5 ** length_penalty)\n",
    "                    new_score = beam['score'] - math.log(token_prob) / length_norm\n",
    "                    \n",
    "                    new_tokens = beam['tokens'] + [token_id]\n",
    "                    is_finished = token_id == model.eos_token_id or new_len >= max_length\n",
    "                    \n",
    "                    # Only add EOS if we're past min_length\n",
    "                    if token_id == model.eos_token_id and new_len < min_length:\n",
    "                        continue\n",
    "                        \n",
    "                    new_beams.append({\n",
    "                        'tokens': new_tokens,\n",
    "                        'score': new_score,\n",
    "                        'finished': is_finished\n",
    "                    })\n",
    "            \n",
    "            # Keep top beam_size beams\n",
    "            new_beams = sorted(new_beams, key=lambda x: x['score'])[:beam_size]\n",
    "            beams = new_beams\n",
    "            \n",
    "            # Early stopping: if all beams are finished\n",
    "            if early_stopping and all(beam['finished'] for beam in beams):\n",
    "                break\n",
    "        \n",
    "        # Select best beam\n",
    "        best_beam = min(beams, key=lambda x: x['score'])\n",
    "        generated_ids = best_beam['tokens']\n",
    "        \n",
    "        # Remove SOS and EOS tokens for decoding\n",
    "        if generated_ids[0] == model.sos_token_id:\n",
    "            generated_ids = generated_ids[1:]\n",
    "        if generated_ids and generated_ids[-1] == model.eos_token_id:\n",
    "            generated_ids = generated_ids[:-1]\n",
    "    \n",
    "    # Decode and post-process\n",
    "    generated_text = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "    \n",
    "    # Post-processing\n",
    "    generated_text = post_process_summary(generated_text)\n",
    "    \n",
    "    return generated_text\n",
    "\n",
    "def post_process_summary(text):\n",
    "    \"\"\"Improve summary quality through post-processing\"\"\"\n",
    "    # Fix spacing issues\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Remove duplicate sentences\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "    unique_sentences = []\n",
    "    seen = set()\n",
    "    for sent in sentences:\n",
    "        sent_lower = sent.lower()\n",
    "        # Skip empty, very short, or duplicate sentences\n",
    "        if sent and len(sent) > 10 and sent_lower not in seen:\n",
    "            unique_sentences.append(sent)\n",
    "            seen.add(sent_lower)\n",
    "    \n",
    "    # Join sentences\n",
    "    text = ' '.join(unique_sentences)\n",
    "    \n",
    "    # Make sure text ends with proper punctuation\n",
    "    if text and not text[-1] in '.!?':\n",
    "        text += '.'\n",
    "    \n",
    "    # Remove redundant references to \"NEW:\"\n",
    "    text = text.replace('NEW : ', '').replace('NEW: ', '')\n",
    "    \n",
    "    return text\n",
    "\n",
    "def evaluate_model(model, test_dataset, tokenizer, num_examples=100):\n",
    "\n",
    "    # Load the ROUGE scorer\n",
    "    rouge = Rouge()\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    \n",
    "    # Prepare lists to store results\n",
    "    references = []\n",
    "    summaries = []\n",
    "    \n",
    "    # Track additional metrics\n",
    "    avg_reference_length = 0\n",
    "    avg_summary_length = 0\n",
    "    \n",
    "    # Make sure the model is in evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Evaluate on a subset of test data\n",
    "    test_subset = test_dataset.select(range(min(num_examples, len(test_dataset))))\n",
    "    \n",
    "    # Generate summaries and calculate ROUGE scores\n",
    "    print(f\"Generating summaries for {len(test_subset)} examples...\")\n",
    "    for i, example in enumerate(tqdm(test_subset)):\n",
    "        article = example['article']\n",
    "        reference = example['highlights']\n",
    "        \n",
    "        # Get reference length to guide generation length\n",
    "        ref_length = len(reference.split())\n",
    "        target_length = min(ref_length + 10, 60)  # Target slightly longer than reference but capped\n",
    "        \n",
    "        generated_summary = generate_summary(\n",
    "            model, tokenizer, article, \n",
    "            max_length=target_length,\n",
    "            min_length=min(20, target_length-5),\n",
    "            beam_size=5,\n",
    "            length_penalty=0.7,  # Favor shorter summaries\n",
    "            top_k=50,\n",
    "            temperature=0.5,     # Lower temperature for more focused generation\n",
    "            early_stopping=True\n",
    "        )\n",
    "        \n",
    "        references.append(reference)\n",
    "        summaries.append(generated_summary)\n",
    "        \n",
    "        # Calculate additional metrics\n",
    "        avg_reference_length += len(reference.split())\n",
    "        avg_summary_length += len(generated_summary.split())\n",
    "        \n",
    "        # Print example every 10 items\n",
    "        if i % 10 == 0:\n",
    "            print(f\"\\nExample {i+1}:\")\n",
    "            print(f\"Article (truncated): {article[:200]}...\")\n",
    "            print(f\"Reference: {reference}\")\n",
    "            print(f\"Generated: {generated_summary}\")\n",
    "    \n",
    "    # Calculate average metrics\n",
    "    avg_reference_length /= len(test_subset)\n",
    "    avg_summary_length /= len(test_subset)\n",
    "    \n",
    "    # Calculate ROUGE scores\n",
    "    try:\n",
    "        # Try using the Rouge library first (faster for multiple examples)\n",
    "        rouge_scores = rouge.get_scores(summaries, references, avg=True)\n",
    "        \n",
    "        print(\"\\nROUGE Scores:\")\n",
    "        print(f\"ROUGE-1: {rouge_scores['rouge-1']['f']:.4f}\")\n",
    "        print(f\"ROUGE-2: {rouge_scores['rouge-2']['f']:.4f}\")\n",
    "        print(f\"ROUGE-L: {rouge_scores['rouge-l']['f']:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error with Rouge library: {e}\")\n",
    "        print(\"Falling back to rouge_scorer...\")\n",
    "        \n",
    "        # Use the rouge_scorer library as fallback\n",
    "        rouge1_scores = []\n",
    "        rouge2_scores = []\n",
    "        rougeL_scores = []\n",
    "        \n",
    "        for ref, hyp in zip(references, summaries):\n",
    "            try:\n",
    "                scores = scorer.score(ref, hyp)\n",
    "                rouge1_scores.append(scores['rouge1'].fmeasure)\n",
    "                rouge2_scores.append(scores['rouge2'].fmeasure)\n",
    "                rougeL_scores.append(scores['rougeL'].fmeasure)\n",
    "            except Exception as e:\n",
    "                print(f\"Error scoring example: {e}\")\n",
    "        \n",
    "        # Calculate average scores\n",
    "        avg_rouge1 = sum(rouge1_scores) / len(rouge1_scores) if rouge1_scores else 0\n",
    "        avg_rouge2 = sum(rouge2_scores) / len(rouge2_scores) if rouge2_scores else 0\n",
    "        avg_rougeL = sum(rougeL_scores) / len(rougeL_scores) if rougeL_scores else 0\n",
    "        \n",
    "        print(\"\\nROUGE Scores:\")\n",
    "        print(f\"ROUGE-1: {avg_rouge1:.4f}\")\n",
    "        print(f\"ROUGE-2: {avg_rouge2:.4f}\")\n",
    "        print(f\"ROUGE-L: {avg_rougeL:.4f}\")\n",
    "    \n",
    "    # Print additional metrics\n",
    "    print(\"\\nAdditional Metrics:\")\n",
    "    print(f\"Average Reference Length: {avg_reference_length:.2f} words\")\n",
    "    print(f\"Average Summary Length: {avg_summary_length:.2f} words\")\n",
    "    \n",
    "    # Save detailed results to CSV\n",
    "    results_df = pd.DataFrame({\n",
    "        'article': [example['article'][:500] + '...' for example in test_subset],\n",
    "        'reference': references,\n",
    "        'generated': summaries,\n",
    "        'summary_length': [len(s.split()) for s in summaries],\n",
    "        'reference_length': [len(r.split()) for r in references]\n",
    "    })\n",
    "    \n",
    "    # Save results\n",
    "    results_df.to_csv('evaluation_results.csv', index=False)\n",
    "    print(\"Results saved to evaluation_results.csv\")\n",
    "\n",
    "# Load best model\n",
    "try:\n",
    "    print(\"Loading best model for evaluation...\")\n",
    "    best_model_path = r\"C:\\Users\\nisha\\Downloads\\best_model (1).pt\"\n",
    "    checkpoint = torch.load(best_model_path, map_location=device)\n",
    "    \n",
    "    # Check if we need to recreate the model from config\n",
    "    if 'model_config' in checkpoint:\n",
    "        config = checkpoint['model_config']\n",
    "        print(f\"Recreating model from saved config: {config}\")\n",
    "        \n",
    "        # Create model with saved config\n",
    "        model = ImprovedTransformer(\n",
    "            src_vocab_size=config['vocab_size'],\n",
    "            tgt_vocab_size=config['vocab_size'],\n",
    "            d_model=config['d_model'],\n",
    "            num_heads=config['num_heads'],\n",
    "            d_ff=config['d_ff'],\n",
    "            num_layers=config['num_layers'],\n",
    "            dropout=config['dropout'],\n",
    "            activation=config['activation']\n",
    "        ).to(device)\n",
    "    \n",
    "    # Load state dict\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(\"Model loaded successfully!\")\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    evaluate_model(model, cnn_dailymail['test'], tokenizer, num_examples=100)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error during evaluation: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
