{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\nisha\\anaconda3\\lib\\site-packages (3.5.0)\n",
      "Requirement already satisfied: transformers in c:\\users\\nisha\\anaconda3\\lib\\site-packages (4.51.3)\n",
      "Requirement already satisfied: tokenizers in c:\\users\\nisha\\anaconda3\\lib\\site-packages (0.21.1)\n",
      "Requirement already satisfied: torch in c:\\users\\nisha\\anaconda3\\lib\\site-packages (2.6.0)\n",
      "Requirement already satisfied: nltk in c:\\users\\nisha\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: rouge_score in c:\\users\\nisha\\anaconda3\\lib\\site-packages (0.1.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\nisha\\anaconda3\\lib\\site-packages (2.1.4)\n",
      "Requirement already satisfied: numpy in c:\\users\\nisha\\anaconda3\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\nisha\\anaconda3\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: wandb in c:\\users\\nisha\\anaconda3\\lib\\site-packages (0.19.9)\n",
      "Requirement already satisfied: tensorboardX in c:\\users\\nisha\\anaconda3\\lib\\site-packages (2.6.2.2)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\nisha\\anaconda3\\lib\\site-packages (0.2.0)\n",
      "Requirement already satisfied: einops in c:\\users\\nisha\\anaconda3\\lib\\site-packages (0.8.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\nisha\\anaconda3\\lib\\site-packages (3.8.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: xxhash in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from datasets) (3.9.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from datasets) (0.30.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from datasets) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: click in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: absl-py in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from rouge_score) (2.2.2)\n",
      "Requirement already satisfied: six>=1.14.0 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from rouge_score) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from wandb) (3.1.37)\n",
      "Requirement already satisfied: platformdirs in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from wandb) (3.10.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from wandb) (3.20.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from wandb) (5.9.0)\n",
      "Requirement already satisfied: pydantic<3 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from wandb) (2.10.3)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from wandb) (2.26.1)\n",
      "Requirement already satisfied: setproctitle in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from wandb) (1.3.5)\n",
      "Requirement already satisfied: setuptools in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from wandb) (68.2.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from matplotlib) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from matplotlib) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.9.3)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.7)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from pydantic<3->wandb) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from pydantic<3->wandb) (2.27.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: smmap<5,>=3.0.1 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (4.0.0)\n",
      "Collecting accelerate\n",
      "  Using cached accelerate-1.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: regex in c:\\users\\nisha\\anaconda3\\lib\\site-packages (2023.10.3)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from accelerate) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from accelerate) (23.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=2.0.0 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from accelerate) (2.6.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from accelerate) (0.30.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from accelerate) (0.5.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (2023.10.0)\n",
      "Requirement already satisfied: requests in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.1.3)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.21.0->accelerate) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from jinja2->torch>=2.0.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2025.1.31)\n",
      "Using cached accelerate-1.6.0-py3-none-any.whl (354 kB)\n",
      "Installing collected packages: accelerate\n",
      "Successfully installed accelerate-1.6.0\n",
      "Collecting gdown\n",
      "  Using cached gdown-5.2.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting langdetect\n",
      "  Using cached langdetect-1.0.9-py3-none-any.whl\n",
      "Requirement already satisfied: ipywidgets in c:\\users\\nisha\\anaconda3\\lib\\site-packages (7.6.5)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from gdown) (4.12.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from gdown) (3.13.1)\n",
      "Requirement already satisfied: requests[socks] in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from gdown) (2.32.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from gdown) (4.67.1)\n",
      "Requirement already satisfied: six in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from langdetect) (1.16.0)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from ipywidgets) (6.28.0)\n",
      "Requirement already satisfied: ipython-genutils~=0.2.0 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from ipywidgets) (5.7.1)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from ipywidgets) (5.9.2)\n",
      "Requirement already satisfied: widgetsnbextension~=3.5.0 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from ipywidgets) (3.5.2)\n",
      "Requirement already satisfied: ipython>=4.0.0 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from ipywidgets) (8.20.0)\n",
      "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from ipywidgets) (3.0.9)\n",
      "Requirement already satisfied: comm>=0.1.1 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.2)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets) (1.6.7)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets) (8.6.0)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets) (5.5.0)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: nest-asyncio in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets) (1.6.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets) (23.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets) (5.9.0)\n",
      "Requirement already satisfied: pyzmq>=24 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets) (25.1.2)\n",
      "Requirement already satisfied: tornado>=6.1 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets) (6.3.3)\n",
      "Requirement already satisfied: decorator in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from ipython>=4.0.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from ipython>=4.0.0->ipywidgets) (0.18.1)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from ipython>=4.0.0->ipywidgets) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from ipython>=4.0.0->ipywidgets) (2.15.1)\n",
      "Requirement already satisfied: stack-data in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from ipython>=4.0.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from ipython>=4.0.0->ipywidgets) (0.4.6)\n",
      "Requirement already satisfied: fastjsonschema in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from nbformat>=4.2.0->ipywidgets) (2.16.2)\n",
      "Requirement already satisfied: jsonschema>=2.6 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from nbformat>=4.2.0->ipywidgets) (4.19.2)\n",
      "Requirement already satisfied: notebook>=4.4.1 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from widgetsnbextension~=3.5.0->ipywidgets) (7.0.8)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from beautifulsoup4->gdown) (2.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from requests[socks]->gdown) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from requests[socks]->gdown) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from requests[socks]->gdown) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from requests[socks]->gdown) (2025.1.31)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from requests[socks]->gdown) (1.7.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets) (0.10.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (2.8.2)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel>=4.5.1->ipywidgets) (3.10.0)\n",
      "Requirement already satisfied: pywin32>=300 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel>=4.5.1->ipywidgets) (305.1)\n",
      "Requirement already satisfied: jupyter-server<3,>=2.4.0 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.10.0)\n",
      "Requirement already satisfied: jupyterlab-server<3,>=2.22.1 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.25.1)\n",
      "Requirement already satisfied: jupyterlab<4.1,>=4.0.2 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (4.0.11)\n",
      "Requirement already satisfied: notebook-shim<0.3,>=0.2 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=4.0.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: executing in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from stack-data->ipython>=4.0.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: asttokens in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from stack-data->ipython>=4.0.0->ipywidgets) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from stack-data->ipython>=4.0.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: anyio>=3.1.0 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (4.2.0)\n",
      "Requirement already satisfied: argon2-cffi in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (21.3.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (3.1.3)\n",
      "Requirement already satisfied: jupyter-events>=0.6.0 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.8.0)\n",
      "Requirement already satisfied: jupyter-server-terminals in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.4.4)\n",
      "Requirement already satisfied: nbconvert>=6.4.4 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (7.10.0)\n",
      "Requirement already satisfied: overrides in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (7.4.0)\n",
      "Requirement already satisfied: prometheus-client in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.14.1)\n",
      "Requirement already satisfied: pywinpty in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.0.10)\n",
      "Requirement already satisfied: send2trash>=1.8.2 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.8.2)\n",
      "Requirement already satisfied: terminado>=0.8.3 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.17.1)\n",
      "Requirement already satisfied: websocket-client in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.58.0)\n",
      "Requirement already satisfied: async-lru>=1.0.0 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from jupyterlab<4.1,>=4.0.2->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.0.4)\n",
      "Requirement already satisfied: jupyter-lsp>=2.0.0 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from jupyterlab<4.1,>=4.0.2->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.2.0)\n",
      "Requirement already satisfied: babel>=2.10 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from jupyterlab-server<3,>=2.22.1->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.11.0)\n",
      "Requirement already satisfied: json5>=0.9.0 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from jupyterlab-server<3,>=2.22.1->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.9.6)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.3.0)\n",
      "Requirement already satisfied: pytz>=2015.7 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from babel>=2.10->jupyterlab-server<3,>=2.22.1->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2023.3.post1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from jinja2->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.1.3)\n",
      "Requirement already satisfied: python-json-logger>=2.0.4 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.0.7)\n",
      "Requirement already satisfied: pyyaml>=5.3 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (6.0.1)\n",
      "Requirement already satisfied: rfc3339-validator in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.1.4)\n",
      "Requirement already satisfied: rfc3986-validator>=0.1.1 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.1.1)\n",
      "Requirement already satisfied: bleach!=5.0.0 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (4.1.0)\n",
      "Requirement already satisfied: defusedxml in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.7.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.1.2)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.0.4)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.8.0)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.5.0)\n",
      "Requirement already satisfied: tinycss2 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.2.1)\n",
      "Requirement already satisfied: argon2-cffi-bindings in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from argon2-cffi->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (21.2.0)\n",
      "Requirement already satisfied: webencodings in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from bleach!=5.0.0->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.5.1)\n",
      "Collecting fqdn (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets)\n",
      "  Using cached fqdn-1.5.1-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting isoduration (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets)\n",
      "  Using cached isoduration-20.11.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: jsonpointer>1.13 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.1)\n",
      "Collecting uri-template (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets)\n",
      "  Using cached uri_template-1.3.0-py3-none-any.whl.metadata (8.8 kB)\n",
      "Collecting webcolors>=1.11 (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets)\n",
      "  Downloading webcolors-24.11.1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: cffi>=1.0.1 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from argon2-cffi-bindings->argon2-cffi->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.16.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.21)\n",
      "Requirement already satisfied: arrow>=0.15.0 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.2.3)\n",
      "Using cached gdown-5.2.0-py3-none-any.whl (18 kB)\n",
      "Downloading webcolors-24.11.1-py3-none-any.whl (14 kB)\n",
      "Using cached fqdn-1.5.1-py3-none-any.whl (9.1 kB)\n",
      "Using cached isoduration-20.11.0-py3-none-any.whl (11 kB)\n",
      "Using cached uri_template-1.3.0-py3-none-any.whl (11 kB)\n",
      "Installing collected packages: webcolors, uri-template, langdetect, fqdn, isoduration, gdown\n",
      "Successfully installed fqdn-1.5.1 gdown-5.2.0 isoduration-20.11.0 langdetect-1.0.9 uri-template-1.3.0 webcolors-24.11.1\n"
     ]
    }
   ],
   "source": [
    "# Install Required Packages\n",
    "!pip install datasets transformers tokenizers torch nltk rouge_score pandas numpy tqdm wandb tensorboardX sentencepiece einops matplotlib\n",
    "!pip install accelerate regex\n",
    "!pip install gdown langdetect ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from tokenizers import ByteLevelBPETokenizer, Tokenizer\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "import wandb  # Optional for experiment tracking\n",
    "from tensorboardX import SummaryWriter\n",
    "import logging\n",
    "import regex as re\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.cuda.amp import autocast, GradScaler  # For mixed precision training\n",
    "from torch.nn.utils import clip_grad_norm_  # For gradient clipping\n",
    "import torch.cuda.amp as amp  # For mixed precision\n",
    "from torch.utils.checkpoint import checkpoint  # For gradient checkpointing\n",
    "import time  # For timing training\n",
    "import json  # For saving configs\n",
    "import matplotlib.pyplot as plt  # For plotting\n",
    "import shutil  # For disk usage information\n",
    "import random  # For seed setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Seed Initialization\n",
    "\n",
    "The code below sets fixed random seeds across all libraries used in this project. This is critical for:\n",
    "\n",
    "- **Reproducibility**: Ensures the same results can be obtained across different runs\n",
    "- **Consistent evaluation**: Guarantees that the generated summaries remain consistent for proper analysis and comparison\n",
    "- **Reliable generation**: With our temperature-based sampling approach, fixed seeds ensure consistent token selection during text generation\n",
    "- **Deterministic behavior**: Makes debugging and validation possible by eliminating randomness as a variable\n",
    "\n",
    "For academic and research contexts, reproducibility is a fundamental requirement. Without these seeds, the model would produce different summaries each time, making proper analysis and comparison impossible.\n",
    "\n",
    "The `deterministic` and `benchmark` settings specifically configure CUDA operations to prioritize consistent results over performance optimizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Disk space - Total: 372.8 GB, Used: 343.5 GB, Free: 29.4 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nisha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Check GPU specs if available\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Total GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "\n",
    "# Check disk space\n",
    "total, used, free = shutil.disk_usage(\"/\")\n",
    "print(f\"Disk space - Total: {total/1e9:.1f} GB, Used: {used/1e9:.1f} GB, Free: {free/1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully!\n",
      "Dataset structure: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['article', 'highlights', 'id'],\n",
      "        num_rows: 287113\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['article', 'highlights', 'id'],\n",
      "        num_rows: 13368\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['article', 'highlights', 'id'],\n",
      "        num_rows: 11490\n",
      "    })\n",
      "})\n",
      "Number of training examples: 287113\n",
      "Number of validation examples: 13368\n",
      "Number of test examples: 11490\n",
      "\n",
      "Sample article (first 300 chars):\n",
      "LONDON, England (Reuters) -- Harry Potter star Daniel Radcliffe gains access to a reported £20 million ($41.1 million) fortune as he turns 18 on Monday, but he insists the money won't cast a spell on him. Daniel Radcliffe as Harry Potter in \"Harry Potter and the Order of the Phoenix\" To the disappoi...\n",
      "\n",
      "Sample highlights (summary):\n",
      "Harry Potter star Daniel Radcliffe gets £20M fortune as he turns 18 Monday .\n",
      "Young actor says he has no plans to fritter his cash away .\n",
      "Radcliffe's earnings from first five Potter films have been held in trust fund .\n"
     ]
    }
   ],
   "source": [
    "# Load CNN/DailyMail dataset\n",
    "cnn_dailymail = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
    "\n",
    "# info\n",
    "print(\"Dataset loaded successfully!\")\n",
    "print(f\"Dataset structure: {cnn_dailymail}\")\n",
    "print(f\"Number of training examples: {len(cnn_dailymail['train'])}\")\n",
    "print(f\"Number of validation examples: {len(cnn_dailymail['validation'])}\")\n",
    "print(f\"Number of test examples: {len(cnn_dailymail['test'])}\")\n",
    "\n",
    "# example\n",
    "sample = cnn_dailymail['train'][0]\n",
    "print(\"\\nSample article (first 300 chars):\")\n",
    "print(sample['article'][:300] + \"...\")\n",
    "print(\"\\nSample highlights (summary):\")\n",
    "print(sample['highlights'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Tokenization Strategy for Abstractive Summarization\n",
    "\n",
    "## The Tokenization Architecture\n",
    "\n",
    "The project implements a custom `OptimizedTokenizer` class that wraps around Hugging Face's Rust-based tokenizers library instead of the more commonly used SentencePiece. This choice was deliberate and offers several advantages for our summarization task.\n",
    "\n",
    "### Key Components of the Implementation:\n",
    "\n",
    "1. **ByteLevelBPE Tokenizer**: The implementation uses Byte-Level Byte-Pair Encoding (BPE) algorithm for tokenization, which works by iteratively merging the most frequent pairs of bytes in the text.\n",
    "\n",
    "2. **Special Tokens Management**: Four special tokens are explicitly defined with assigned IDs:\n",
    "   - `<pad>` (ID 0): For padding sequences to uniform length\n",
    "   - `<sos>` (ID 1): Start of sequence marker\n",
    "   - `<eos>` (ID 2): End of sequence marker\n",
    "   - `<unk>` (ID 3): For handling unknown tokens\n",
    "\n",
    "3. **Fast Training Process**: The tokenizer is trained on a sampled subset of the data (around 200,000 texts including both articles and summaries).\n",
    "\n",
    "4. **Vocabulary Size**: A vocabulary size of 32,000 tokens is used, which is large enough to capture the diversity of news language.\n",
    "\n",
    "## Rationale for Avoiding SentencePiece\n",
    "\n",
    "The implementation deliberately avoids SentencePiece for several reasons:\n",
    "\n",
    "1. **Performance**: ByteLevelBPE with the Rust implementation provides significant performance gains during both training and inference compared to SentencePiece.\n",
    "\n",
    "2. **Efficiency**: Training the tokenizer takes minutes rather than hours, making development iterations faster.\n",
    "\n",
    "3. **Memory Usage**: The Rust-based implementation is more memory-efficient, allowing for processing larger batches during training.\n",
    "\n",
    "4. **Fine-grained Control**: The implementation provides explicit control over special token IDs, which is important for the transformer architecture.\n",
    "\n",
    "5. **HuggingFace Integration**: Using `PreTrainedTokenizerFast` wrapper ensures compatibility with the broader ecosystem.\n",
    "\n",
    "## Implementation Details\n",
    "\n",
    "The implementation includes methods for:\n",
    "- Training the tokenizer on a corpus (`train`)\n",
    "- Encoding single texts (`encode`)\n",
    "- Batch encoding multiple texts (`batch_encode`)\n",
    "- Decoding token IDs back to text (`decode`)\n",
    "- Saving and loading the tokenizer (`save`, `load`)\n",
    "\n",
    "When used in the pipeline, the tokenizer truncates inputs to 512 tokens and targets to 128 tokens, which aligns with the characteristics of CNN/DailyMail articles and summaries.\n",
    "\n",
    "For a text summarization task like ours, having an efficient tokenization process is crucial due to the large amount of text data being processed. The chosen approach optimizes for both training speed and runtime performance. both training speed and runtime performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer Class\n",
    "class OptimizedTokenizer:\n",
    "    def __init__(self, vocab_size=32000):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.tokenizer = None\n",
    "        self.special_tokens = [\"<pad>\", \"<sos>\", \"<eos>\", \"<unk>\"]\n",
    "        self.special_token_ids = {\n",
    "            \"<pad>\": 0,\n",
    "            \"<sos>\": 1,\n",
    "            \"<eos>\": 2,\n",
    "            \"<unk>\": 3\n",
    "        }\n",
    "        \n",
    "    def train(self, texts, model_prefix=\"tokenizer\", num_samples=None):\n",
    "\n",
    "        os.makedirs(model_prefix, exist_ok=True)\n",
    "        \n",
    "        # Sample texts to speed up training if needed\n",
    "        if num_samples and len(texts) > num_samples:\n",
    "            import random\n",
    "            random.seed(42)\n",
    "            texts = random.sample(texts, num_samples)\n",
    "        \n",
    "        # Write sample texts to file\n",
    "        corpus_path = \"corpus.txt\"\n",
    "        print(f\"Writing {len(texts)} texts to file...\")\n",
    "        with open(corpus_path, 'w', encoding='utf-8') as f:\n",
    "            for text in texts:\n",
    "                f.write(text + '\\n')\n",
    "        \n",
    "        # Initialize and train the tokenizer \n",
    "        print(\"Training tokenizer...\")\n",
    "        from tokenizers import Tokenizer\n",
    "        from tokenizers.models import BPE\n",
    "        from tokenizers.trainers import BpeTrainer\n",
    "        from tokenizers.pre_tokenizers import Whitespace\n",
    "        \n",
    "        # Create a new BPE tokenizer\n",
    "        tokenizer = Tokenizer(BPE(unk_token=\"<unk>\"))\n",
    "        tokenizer.pre_tokenizer = Whitespace()\n",
    "        \n",
    "        # Prepare the trainer\n",
    "        trainer = BpeTrainer(\n",
    "            vocab_size=self.vocab_size,\n",
    "            special_tokens=self.special_tokens,\n",
    "            min_frequency=2\n",
    "        )\n",
    "        \n",
    "        # Train the tokenizer\n",
    "        tokenizer.train(files=[corpus_path], trainer=trainer)\n",
    "        \n",
    "        # Save the tokenizer\n",
    "        tokenizer_path = os.path.join(model_prefix, \"tokenizer.json\")\n",
    "        tokenizer.save(tokenizer_path)\n",
    "        print(f\"Tokenizer saved to {tokenizer_path}\")\n",
    "        \n",
    "        # Load the tokenizer\n",
    "        from transformers import PreTrainedTokenizerFast\n",
    "        self.tokenizer = PreTrainedTokenizerFast(\n",
    "            tokenizer_file=tokenizer_path,\n",
    "            bos_token=\"<sos>\",\n",
    "            eos_token=\"<eos>\",\n",
    "            pad_token=\"<pad>\",\n",
    "            unk_token=\"<unk>\"\n",
    "        )\n",
    "        \n",
    "        # Set special token IDs explicitly\n",
    "        self.tokenizer.pad_token_id = 0\n",
    "        self.tokenizer.bos_token_id = 1\n",
    "        self.tokenizer.eos_token_id = 2\n",
    "        self.tokenizer.unk_token_id = 3\n",
    "        \n",
    "        # Clean up\n",
    "        if os.path.exists(corpus_path):\n",
    "            os.remove(corpus_path)\n",
    "            \n",
    "        print(f\"Tokenizer training complete!\")\n",
    "        print(f\"Vocabulary size: {self.tokenizer.vocab_size}\")\n",
    "        \n",
    "    def encode(self, text, max_length=None, padding=\"max_length\", truncation=True):\n",
    "        if self.tokenizer is None:\n",
    "            raise ValueError(\"Tokenizer not trained. Call train() first.\")\n",
    "        \n",
    "        # Use the HuggingFace tokenizer\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_length,\n",
    "            padding=padding,\n",
    "            truncation=truncation,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors=None\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"],\n",
    "            \"attention_mask\": encoding[\"attention_mask\"]\n",
    "        }\n",
    "    \n",
    "    def batch_encode(self, texts, max_length=None, padding=\"max_length\", truncation=True):\n",
    "        if self.tokenizer is None:\n",
    "            raise ValueError(\"Tokenizer not trained. Call train() first.\")\n",
    "        \n",
    "        # Batch encode\n",
    "        encodings = self.tokenizer(\n",
    "            texts,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_length,\n",
    "            padding=padding,\n",
    "            truncation=truncation,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors=None\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": encodings[\"input_ids\"],\n",
    "            \"attention_mask\": encodings[\"attention_mask\"]\n",
    "        }\n",
    "    \n",
    "    def decode(self, token_ids, skip_special_tokens=True):\n",
    "        if self.tokenizer is None:\n",
    "            raise ValueError(\"Tokenizer not trained. Call train() first.\")\n",
    "            \n",
    "        if isinstance(token_ids, torch.Tensor):\n",
    "            token_ids = token_ids.cpu().tolist()\n",
    "            \n",
    "        return self.tokenizer.decode(token_ids, skip_special_tokens=skip_special_tokens)\n",
    "    \n",
    "    def save(self, path):\n",
    "        if self.tokenizer is None:\n",
    "            raise ValueError(\"Tokenizer not trained. Call train() first.\")\n",
    "        \n",
    "        os.makedirs(path, exist_ok=True)\n",
    "        self.tokenizer.save_pretrained(path)\n",
    "        \n",
    "        # Save the special token mapping separately\n",
    "        with open(os.path.join(path, \"special_tokens.json\"), \"w\") as f:\n",
    "            json.dump(self.special_token_ids, f)\n",
    "    \n",
    "    def load(self, path):\n",
    "        from transformers import PreTrainedTokenizerFast\n",
    "        \n",
    "        tokenizer_path = os.path.join(path, \"tokenizer.json\")\n",
    "        if os.path.exists(tokenizer_path):\n",
    "            self.tokenizer = PreTrainedTokenizerFast(\n",
    "                tokenizer_file=tokenizer_path,\n",
    "                bos_token=\"<sos>\",\n",
    "                eos_token=\"<eos>\",\n",
    "                pad_token=\"<pad>\",\n",
    "                unk_token=\"<unk>\"\n",
    "            )\n",
    "        else:\n",
    "            # Try loading as a pretrained tokenizer\n",
    "            self.tokenizer = PreTrainedTokenizerFast.from_pretrained(path)\n",
    "        \n",
    "        # Load special token mapping if it exists\n",
    "        special_tokens_path = os.path.join(path, \"special_tokens.json\")\n",
    "        if os.path.exists(special_tokens_path):\n",
    "            with open(special_tokens_path, \"r\") as f:\n",
    "                self.special_token_ids = json.load(f)\n",
    "        \n",
    "        # Ensure the tokenizer has the correct special tokens\n",
    "        self.tokenizer.pad_token = \"<pad>\"\n",
    "        self.tokenizer.bos_token = \"<sos>\"\n",
    "        self.tokenizer.eos_token = \"<eos>\"\n",
    "        self.tokenizer.unk_token = \"<unk>\"\n",
    "        \n",
    "        # Set special token IDs explicitly\n",
    "        self.tokenizer.pad_token_id = self.special_token_ids[\"<pad>\"]\n",
    "        self.tokenizer.bos_token_id = self.special_token_ids[\"<sos>\"]\n",
    "        self.tokenizer.eos_token_id = self.special_token_ids[\"<eos>\"]\n",
    "        self.tokenizer.unk_token_id = self.special_token_ids[\"<unk>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing tokenizer...\n",
      "Tokenizer loaded with vocabulary size: 32000\n"
     ]
    }
   ],
   "source": [
    "# Initialize tokenizer\n",
    "MAX_VOCAB_SIZE = 32000\n",
    "tokenizer = OptimizedTokenizer(vocab_size=MAX_VOCAB_SIZE)\n",
    "\n",
    "# Check if tokenizer already exists\n",
    "if os.path.exists(r\"D:\\NLP-Project\\processed_text_summarization_data\\tokenizer\\tokenizer.json\"):\n",
    "    print(\"Loading existing tokenizer...\")\n",
    "    tokenizer.load(r\"D:\\NLP-Project\\processed_text_summarization_data\\tokenizer\")\n",
    "    print(f\"Tokenizer loaded with vocabulary size: {tokenizer.tokenizer.vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Architecture: Advanced Components for Summarization\n",
    "\n",
    "## Positional Encoding\n",
    "\n",
    "The positional encoding component addresses a fundamental limitation of transformer models: they have no inherent understanding of sequence order. Unlike RNNs, which process tokens sequentially, transformers process all tokens in parallel.\n",
    "\n",
    "This implementation uses sinusoidal positional encodings, which add position-dependent patterns to each embedding. The mathematical properties of these sine/cosine functions allow the model to attend to relative positions, making it possible to understand the sequential nature of text while retaining the benefits of parallel processing.\n",
    "\n",
    "## Enhanced Multi-Head Attention\n",
    "\n",
    "The `ImprovedMultiHeadAttention` class represents a refined implementation of the attention mechanism that forms the core of the transformer. Key enhancements include:\n",
    "\n",
    "- **Numerical Stability**: Uses a smaller negative value (-1e4 instead of -1e9) for masked positions to prevent overflow in mixed precision training\n",
    "- **Proper Initialization**: Weight matrices are initialized with Xavier uniform distribution to ensure stable gradient flow\n",
    "- **Flexible Masking**: Supports multiple mask dimensions for different attention patterns\n",
    "\n",
    "Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. This enables capturing different types of dependencies in the text - some heads might focus on local syntactic patterns while others capture document-level semantic relationships.\n",
    "\n",
    "## Position-wise Feed-Forward Networks\n",
    "\n",
    "The feed-forward networks apply two linear transformations with a GELU activation in between. This creates a component that can model complex token-level transformations. Key features:\n",
    "\n",
    "- **GELU Activation**: Uses Gaussian Error Linear Unit instead of ReLU, providing smoother gradients\n",
    "- **Proper Dropout**: Applied after activation to improve regularization\n",
    "- **Careful Initialization**: Parameter initialization designed to prevent vanishing/exploding gradients\n",
    "\n",
    "## Encoder and Decoder Architecture\n",
    "\n",
    "The encoder and decoder follow the classic transformer design but with several optimizations:\n",
    "\n",
    "- **Pre-Layer Normalization**: Unlike the original transformer's post-layer norm, this implementation applies normalization before each sub-layer, significantly improving training stability\n",
    "- **Residual Connections**: Carefully implemented skip connections help maintain gradient flow through deep networks\n",
    "- **Shared Embeddings**: Input and output embeddings are shared to reduce parameters and improve regularization\n",
    "\n",
    "## Complete Transformer Model\n",
    "\n",
    "The `ImprovedTransformer` class brings everything together with several enhancements:\n",
    "\n",
    "- **Efficient Masking**: Optimized logic for creating source and target masks\n",
    "- **Three-Way Weight Tying**: Shares weights between encoder embeddings, decoder embeddings, and the output projection layer\n",
    "- **Beam Search Generation**: Implements a sophisticated beam search algorithm with top-k sampling for better summary quality\n",
    "- **Temperature Control**: Allows controlling the randomness in the generation process\n",
    "\n",
    "These architectural choices reflect both the original transformer design principles and more recent improvements developed by the NLP community, creating a model particularly well-suited for abstractive summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length=5000, dropout=0.1):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        \n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1), :].to(x.device)\n",
    "        return self.dropout(x)\n",
    "\n",
    "class ImprovedMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super(ImprovedMultiHeadAttention, self).__init__()\n",
    "        \n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        for module in [self.W_q, self.W_k, self.W_v, self.W_o]:\n",
    "            nn.init.xavier_uniform_(module.weight)\n",
    "            nn.init.zeros_(module.bias)\n",
    "            \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None, return_attention=False):\n",
    "        batch_size = query.size(0)\n",
    "        \n",
    "        Q = self.W_q(query)\n",
    "        K = self.W_k(key)\n",
    "        V = self.W_v(value)\n",
    "        \n",
    "        Q = Q.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = K.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = V.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        if mask is not None:\n",
    "            if mask.dim() == 2:\n",
    "                mask = mask.unsqueeze(1).unsqueeze(2)\n",
    "            elif mask.dim() == 3:\n",
    "                mask = mask.unsqueeze(1)\n",
    "            \n",
    "            scores = scores.masked_fill(mask == 0, -1e4)\n",
    "        \n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        context = torch.matmul(attn_weights, V)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "        \n",
    "        output = self.W_o(context)\n",
    "        \n",
    "        if return_attention:\n",
    "            return output, attn_weights\n",
    "        else:\n",
    "            return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1, activation='gelu'):\n",
    "        super(FeedForward, self).__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        if activation == 'relu':\n",
    "            self.activation = F.relu\n",
    "        elif activation == 'gelu':\n",
    "            self.activation = F.gelu\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation: {activation}\")\n",
    "        \n",
    "        nn.init.xavier_uniform_(self.linear1.weight)\n",
    "        nn.init.zeros_(self.linear1.bias)\n",
    "        nn.init.xavier_uniform_(self.linear2.weight)\n",
    "        nn.init.zeros_(self.linear2.bias)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear2(self.dropout(self.activation(self.linear1(x))))\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1, activation='gelu'):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        \n",
    "        self.self_attn = ImprovedMultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.feed_forward = FeedForward(d_model, d_ff, dropout, activation)\n",
    "        self.norm1 = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        self.norm2 = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        norm_x = self.norm1(x)\n",
    "        attn_output = self.self_attn(norm_x, norm_x, norm_x, mask)\n",
    "        x = x + self.dropout1(attn_output)\n",
    "        \n",
    "        norm_x = self.norm2(x)\n",
    "        ff_output = self.feed_forward(norm_x)\n",
    "        x = x + self.dropout2(ff_output)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1, activation='gelu'):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        \n",
    "        self.self_attn = ImprovedMultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.cross_attn = ImprovedMultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.feed_forward = FeedForward(d_model, d_ff, dropout, activation)\n",
    "        self.norm1 = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        self.norm2 = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        self.norm3 = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, enc_output, src_mask=None, tgt_mask=None):\n",
    "        norm_x = self.norm1(x)\n",
    "        self_attn_output = self.self_attn(norm_x, norm_x, norm_x, tgt_mask)\n",
    "        x = x + self.dropout1(self_attn_output)\n",
    "        \n",
    "        norm_x = self.norm2(x)\n",
    "        cross_attn_output = self.cross_attn(norm_x, enc_output, enc_output, src_mask)\n",
    "        x = x + self.dropout2(cross_attn_output)\n",
    "        \n",
    "        norm_x = self.norm3(x)\n",
    "        ff_output = self.feed_forward(norm_x)\n",
    "        x = x + self.dropout3(ff_output)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder and Decoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, d_ff, num_layers, dropout=0.1, activation='gelu'):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=0)\n",
    "        self.pos_encoding = PositionalEncoding(d_model, dropout=dropout)\n",
    "        \n",
    "        self.layers = nn.ModuleList([\n",
    "            EncoderLayer(d_model, num_heads, d_ff, dropout, activation)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.norm = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        \n",
    "        nn.init.normal_(self.embedding.weight, mean=0, std=d_model**-0.5)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        x = self.embedding(x) * math.sqrt(self.embedding.embedding_dim)\n",
    "        x = self.pos_encoding(x)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "            \n",
    "        x = self.norm(x)\n",
    "            \n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, d_ff, num_layers, dropout=0.1, activation='gelu'):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=0)\n",
    "        self.pos_encoding = PositionalEncoding(d_model, dropout=dropout)\n",
    "        \n",
    "        self.layers = nn.ModuleList([\n",
    "            DecoderLayer(d_model, num_heads, d_ff, dropout, activation)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.norm = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        \n",
    "        nn.init.normal_(self.embedding.weight, mean=0, std=d_model**-0.5)\n",
    "        \n",
    "    def forward(self, x, enc_output, src_mask=None, tgt_mask=None):\n",
    "        x = self.embedding(x) * math.sqrt(self.embedding.embedding_dim)\n",
    "        x = self.pos_encoding(x)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = layer(x, enc_output, src_mask, tgt_mask)\n",
    "            \n",
    "        x = self.norm(x)\n",
    "            \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer Model\n",
    "class ImprovedTransformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=512, num_heads=8,\n",
    "                 d_ff=2048, num_layers=6, dropout=0.1, activation='gelu',\n",
    "                 share_embeddings=True):\n",
    "        super(ImprovedTransformer, self).__init__()\n",
    "        \n",
    "        self.pad_token_id = 0\n",
    "        self.sos_token_id = 1\n",
    "        self.eos_token_id = 2\n",
    "        \n",
    "        self.encoder = Encoder(src_vocab_size, d_model, num_heads, d_ff, num_layers, dropout, activation)\n",
    "        self.decoder = Decoder(tgt_vocab_size, d_model, num_heads, d_ff, num_layers, dropout, activation)\n",
    "        \n",
    "        self.final_layer = nn.Linear(d_model, tgt_vocab_size, bias=False)\n",
    "        \n",
    "        if share_embeddings:\n",
    "            self.encoder.embedding.weight = self.decoder.embedding.weight\n",
    "            \n",
    "        self.final_layer.weight = self.decoder.embedding.weight\n",
    "        \n",
    "    def create_src_mask(self, src):\n",
    "        src_mask = (src != self.pad_token_id).unsqueeze(1).unsqueeze(2)\n",
    "        return src_mask\n",
    "    \n",
    "    def create_tgt_mask(self, tgt):\n",
    "        tgt_pad_mask = (tgt != self.pad_token_id).unsqueeze(1).unsqueeze(3)\n",
    "        \n",
    "        tgt_len = tgt.size(1)\n",
    "        tgt_sub_mask = torch.tril(torch.ones((tgt_len, tgt_len), device=tgt.device)).bool()\n",
    "        tgt_sub_mask = tgt_sub_mask.unsqueeze(0).unsqueeze(1)\n",
    "        \n",
    "        tgt_mask = tgt_pad_mask & tgt_sub_mask\n",
    "        return tgt_mask\n",
    "    \n",
    "    def forward(self, src, tgt):\n",
    "        src_mask = self.create_src_mask(src)\n",
    "        tgt_mask = self.create_tgt_mask(tgt)\n",
    "        \n",
    "        enc_output = self.encoder(src, src_mask)\n",
    "        dec_output = self.decoder(tgt, enc_output, src_mask, tgt_mask)\n",
    "        \n",
    "        output = self.final_layer(dec_output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded with vocabulary size: 32000\n"
     ]
    }
   ],
   "source": [
    "# Setup constants\n",
    "MAX_INPUT_LENGTH = 512\n",
    "MAX_OUTPUT_LENGTH = 128\n",
    "\n",
    "# Load the tokenizer (use existing class from Cell 4)\n",
    "tokenizer = OptimizedTokenizer(vocab_size=32000)\n",
    "tokenizer.load(r\"D:\\NLP-Project\\processed_text_summarization_data\\tokenizer\")\n",
    "print(f\"Tokenizer loaded with vocabulary size: {tokenizer.tokenizer.vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model config from checkpoint: {'vocab_size': 32000, 'd_model': 768, 'num_heads': 12, 'd_ff': 3072, 'num_layers': 6, 'dropout': 0.1, 'activation': 'gelu'}\n",
      "Model loaded successfully!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ImprovedTransformer(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(32000, 768, padding_idx=0)\n",
       "    (pos_encoding): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x EncoderLayer(\n",
       "        (self_attn): ImprovedMultiHeadAttention(\n",
       "          (W_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (W_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (W_v): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (W_o): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (linear1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (linear2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(32000, 768, padding_idx=0)\n",
       "    (pos_encoding): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x DecoderLayer(\n",
       "        (self_attn): ImprovedMultiHeadAttention(\n",
       "          (W_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (W_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (W_v): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (W_o): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (cross_attn): ImprovedMultiHeadAttention(\n",
       "          (W_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (W_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (W_v): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (W_o): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (linear1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (linear2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  )\n",
       "  (final_layer): Linear(in_features=768, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model from checkpoint\n",
    "best_model_path = r\"D:\\NLP-Project\\processed_text_summarization_data\\best_model (1).pt\"\n",
    "checkpoint = torch.load(best_model_path, map_location=device)\n",
    "\n",
    "# Get model config from checkpoint\n",
    "if 'model_config' in checkpoint:\n",
    "    config = checkpoint['model_config']\n",
    "    print(f\"Model config from checkpoint: {config}\")\n",
    "    \n",
    "    # Recreate the model with the same architecture\n",
    "    model = ImprovedTransformer(\n",
    "        src_vocab_size=config['vocab_size'],\n",
    "        tgt_vocab_size=config['vocab_size'],\n",
    "        d_model=config['d_model'],\n",
    "        num_heads=config['num_heads'],\n",
    "        d_ff=config['d_ff'],\n",
    "        num_layers=config['num_layers'],\n",
    "        dropout=config['dropout'],\n",
    "        activation=config['activation']\n",
    "    ).to(device)\n",
    "else:\n",
    "    # Default configuration if not found in checkpoint\n",
    "    print(\"Model config not found in checkpoint, using default values\")\n",
    "    model = ImprovedTransformer(\n",
    "        src_vocab_size=32000,\n",
    "        tgt_vocab_size=32000,\n",
    "        d_model=768,\n",
    "        num_heads=12,\n",
    "        d_ff=3072,\n",
    "        num_layers=6,\n",
    "        dropout=0.1,\n",
    "        activation='gelu'\n",
    "    ).to(device)\n",
    "\n",
    "# Load the model weights\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File nsw-project-tweet-datasets.zip already exists. Skipping download.\n"
     ]
    }
   ],
   "source": [
    "import gdown\n",
    "\n",
    "url = \"https://drive.google.com/file/d/1RDfNmQroAwyrtQL3SExGAvrYIEqBARa0/view\"\n",
    "output_file = 'nsw-project-tweet-datasets.zip'\n",
    "\n",
    "# Check if file already exists\n",
    "if os.path.exists(output_file):\n",
    "    print(f\"File {output_file} already exists. Skipping download.\")\n",
    "else:\n",
    "    print(f\"Downloading dataset from Google Drive...\")\n",
    "    # Convert to direct download link\n",
    "    file_id = url.split('/')[-2]\n",
    "    direct_url = f'https://drive.google.com/uc?id={file_id}'\n",
    "    \n",
    "    gdown.download(direct_url, output_file, quiet=False)\n",
    "    print(f\"Download complete: {output_file}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting nsw-project-tweet-datasets.zip to twitter_data...\n",
      "Extracted 9 tweet files and 9 network files\n",
      "\n",
      "Tweet files:\n",
      " - twitter_data\\#AYODHYAVERDICT_tweets.csv\n",
      " - twitter_data\\#cancelallBlueTicksinIndia_tweets.csv\n",
      " - twitter_data\\#HistoryOfAyodhya_tweets.csv\n",
      " - twitter_data\\#jnuprotest_tweets.csv\n",
      " - twitter_data\\#Kashmir_tweets.csv\n",
      " - twitter_data\\#KejriwalMustResign_tweets.csv\n",
      " - twitter_data\\#ShivSenaCheatsMaharashtra_tweets.csv\n",
      " - twitter_data\\#ShutDownJNU_tweets.csv\n",
      " - twitter_data\\#WhereIsAmitShah_tweets.csv\n"
     ]
    }
   ],
   "source": [
    "# Extract the downloaded zip file\n",
    "import zipfile\n",
    "\n",
    "output_dir = 'twitter_data'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Extract zip file\n",
    "zip_path = 'nsw-project-tweet-datasets.zip'\n",
    "print(f\"Extracting {zip_path} to {output_dir}...\")\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(output_dir)\n",
    "    \n",
    "# List the extracted files to verify\n",
    "tweet_files = []\n",
    "network_files = []\n",
    "\n",
    "for root, dirs, files in os.walk(output_dir):\n",
    "    for file in files:\n",
    "        if file.endswith('_tweets.csv'):\n",
    "            tweet_files.append(os.path.join(root, file))\n",
    "        elif file.endswith('_network.csv'):\n",
    "            network_files.append(os.path.join(root, file))\n",
    "\n",
    "print(f\"Extracted {len(tweet_files)} tweet files and {len(network_files)} network files\")\n",
    "print(\"\\nTweet files:\")\n",
    "for file in tweet_files:\n",
    "    print(f\" - {file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Data Preprocessing\n",
    "\n",
    "The code processes Twitter data to make it suitable for summarization. It takes tweets organized by hashtags and transforms them into clean documents by:\n",
    "\n",
    "1. Reading CSV files containing tweets from different political hashtags in India\n",
    "2. Cleaning each tweet by removing URLs, mentions, and extra spaces\n",
    "3. Checking if tweets are in English using language detection\n",
    "4. Combining all English tweets from the same hashtag into one document\n",
    "\n",
    "This approach makes sense because:\n",
    "- Raw tweets contain a lot of noise (links, @mentions, etc.) that would confuse the model\n",
    "- By filtering for English only, we ensure the model can properly understand the content\n",
    "- Grouping by hashtag creates natural topic boundaries that make summarization more meaningful\n",
    "- The preprocessing reduces the data to its essential content, making it easier for the model to identify key themes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ##AYODHYAVERDICT...\n",
      "  - Loaded 200 tweets\n",
      "  - Found 128 English tweets out of 200 total\n",
      "  - Created document with 28997 characters\n",
      "Processing ##cancelallBlueTicksinIndia...\n",
      "  - Loaded 198 tweets\n",
      "  - Found 63 English tweets out of 198 total\n",
      "  - Created document with 6376 characters\n",
      "Processing ##HistoryOfAyodhya...\n",
      "  - Loaded 200 tweets\n",
      "  - Found 83 English tweets out of 200 total\n",
      "  - Created document with 17866 characters\n",
      "Processing ##jnuprotest...\n",
      "  - Loaded 200 tweets\n",
      "  - Found 120 English tweets out of 200 total\n",
      "  - Created document with 27980 characters\n",
      "Processing ##Kashmir...\n",
      "  - Loaded 200 tweets\n",
      "  - Found 172 English tweets out of 200 total\n",
      "  - Created document with 28219 characters\n",
      "Processing ##KejriwalMustResign...\n",
      "  - Loaded 200 tweets\n",
      "  - Found 73 English tweets out of 200 total\n",
      "  - Created document with 10098 characters\n",
      "Processing ##ShivSenaCheatsMaharashtra...\n",
      "  - Loaded 200 tweets\n",
      "  - Found 117 English tweets out of 200 total\n",
      "  - Created document with 22514 characters\n",
      "Processing ##ShutDownJNU...\n",
      "  - Loaded 200 tweets\n",
      "  - Found 143 English tweets out of 200 total\n",
      "  - Created document with 27741 characters\n",
      "Processing ##WhereIsAmitShah...\n",
      "  - Loaded 200 tweets\n",
      "  - Found 96 English tweets out of 200 total\n",
      "  - Created document with 15021 characters\n",
      "\n",
      "Created 9 documents\n"
     ]
    }
   ],
   "source": [
    "# Preprocess Twitter data into documents by hashtag\n",
    "from langdetect import detect\n",
    "import re\n",
    "\n",
    "# Create directory for processed documents\n",
    "processed_dir = 'processed_data'\n",
    "os.makedirs(processed_dir, exist_ok=True)\n",
    "\n",
    "def clean_tweet(text):\n",
    "    \"\"\"Clean the tweet text\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # Remove user mentions\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    \n",
    "    # Remove hashtags symbol (but keep the text)\n",
    "    text = re.sub(r'#', '', text)\n",
    "    \n",
    "    # Remove RT symbol\n",
    "    text = re.sub(r'RT\\s+', '', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def is_english(text):\n",
    "    \"\"\"Check if text is in English\"\"\"\n",
    "    if pd.isna(text) or len(text) < 10:\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        return detect(text) == 'en'\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "# Process each tweet file\n",
    "hashtag_documents = {}\n",
    "\n",
    "for file_path in tweet_files:\n",
    "    # Extract hashtag name from file path\n",
    "    hashtag = os.path.basename(file_path).replace(\"_tweets.csv\", \"\")\n",
    "    print(f\"Processing #{hashtag}...\")\n",
    "    \n",
    "    # Load the tweets\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(f\"  - Loaded {len(df)} tweets\")\n",
    "    \n",
    "    # Find the text column\n",
    "    text_column = None\n",
    "    for col in ['text', 'tweet', 'content', 'status', 'full_text']:\n",
    "        if col in df.columns:\n",
    "            text_column = col\n",
    "            break\n",
    "    \n",
    "    if text_column is None:\n",
    "        print(f\"  - Warning: No text column found in {hashtag}\")\n",
    "        continue\n",
    "    \n",
    "    # Clean and filter tweets\n",
    "    df['cleaned_text'] = df[text_column].apply(clean_tweet)\n",
    "    \n",
    "    # Filter for English tweets\n",
    "    english_tweets = df[df['cleaned_text'].apply(is_english)]\n",
    "    print(f\"  - Found {len(english_tweets)} English tweets out of {len(df)} total\")\n",
    "    \n",
    "    if len(english_tweets) > 0:\n",
    "        # Create a document for this hashtag\n",
    "        document = \"\\n\".join(english_tweets['cleaned_text'].tolist())\n",
    "        hashtag_documents[hashtag] = document\n",
    "        \n",
    "        # Save to file\n",
    "        doc_path = os.path.join(processed_dir, f\"{hashtag}_document.txt\")\n",
    "        with open(doc_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(document)\n",
    "        \n",
    "        print(f\"  - Created document with {len(document)} characters\")\n",
    "    else:\n",
    "        print(f\"  - No English tweets found for #{hashtag}\")\n",
    "\n",
    "print(f\"\\nCreated {len(hashtag_documents)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##AYODHYAVERDICT (28997 chars):\n",
      "Ahead of AyodhyaVerdict, appeal to everyone to accept the judgement of Honourable SupremeCourt. Let us continue to live in peace and harmony. The spirit of brotherhood is the hallmark of our secular fabric.\n",
      "We respect the Hon’ble Supreme Court’s AyodhyaVerdict. It reaffirms the secular values of our country. All should join together in harmony and brotherhood to carry out this immensely important judicial order.\n",
      "This is the last prayer offered at BabriMasjid Image speaks a thousand words... 💔 AYODHYAVERDICT\n",
      "\"There is a profound reason Maharshi Valmiki titled his immortal work Ramayana, the Journey of Sri Ramachandra. One wonders whether it was an exile or an enduring national pilgrimage of which he was the pioneer. \" ------ My essay on the AYODHYAVERDICT\n",
      "I welcome Hon Supreme Court's AyodhyaVerdict on Sri Ram Janmbhoomi. I appeal to people of all religions to honour the verdict and maintain peace and harmony. We remain committed to the ideals of India. EkBharatShresthBharat AyodhyaCase...\n",
      "\n",
      "##cancelallBlueTicksinIndia (6376 chars):\n",
      "cancelallBlueTicksinIndia has been the top trend on Twitter on Wednesday. The trend surfaced in connection with allegations of Twitter encouraging caste discrimination by suppressing the voices of the marginalised.\n",
      "Dear You are very biased with lower caste account holders Plz give reservation here too🙄 While creating account plz give caste option.😒 or make it compulsory to add category in username अब तो आदत सी है उनको ऐसे जीने की 🎶🎶🎵🎵🎼 cancelallBlueTicksinIndia\n",
      "cancelallBlueTicksinIndia Now\n",
      "Why Dalits are accusing Twitter of casteism and communal bigotry? Scroll_story. cancelallBlueTicksinIndia\n",
      "Twitter caught in caste controversy again, users say it discriminates against SC, ST, OBCs. ThePrint cancelallBlueTicksinIndia\n",
      "Absolutely right this is new form of discrimination. cancelallBlueTicksinIndia\n",
      "cancelallBlueTicksinIndia\n",
      "I request you when anybody open account on twitter you tell which documents required if all documents right than you give blue ticks otherwise don't open account on t...\n",
      "\n",
      "##HistoryOfAyodhya (17866 chars):\n",
      "•The struggle for liberation of Ramjanmabhoomi continued even during the reigns of Jahangir {1559-1627} and Shah Jahan [1592-1666} AYODHYAVERDICT HistoryOfAyodhya HistoryOfRamMandir\n",
      "•In 1556 AD After Humayun's death, Akbar ascended the throne. In 1580 AD, Akbar divided his empire into 12 subas. One of these 12 subas was Awadh, whose capital was Ayodhya. HistoryOfAyodhya\n",
      "•Respecting the faith of the majority Hindu society, Todarmal and Birbal got the Ram platform built near the structure and allowed it to be worshiped. HistoryOfRamMandir HistoryOfAyodhya\n",
      "•To resolve the conflicts that had been going on for years for the Ram Janmabhoomi in Ayodhya, Akbar sent Todarmal and Birbal to negotiate and mediate between the two communities. HistoryOfRamMandir HistoryOfAyodhya\n",
      "•Even during the time of Akbar, the devotees under the leadership of Swami Balramacharya fought with the Mughals for the liberation of the birthplace for over many times. HistoryOfRamMandir HistoryOfAyodhya\n",
      "•After being defe...\n",
      "\n",
      "##jnuprotest (27980 chars):\n",
      "Statue of Unity - 3000 cr Cost of printing new notes after demo b/w 2016-18 - almost 13,000 cr. Money spent by BJP on ads since 2014-2018 - over 5,200 cr But, public univs like JNU are a waste of tax payers money because poor can study there. JNUProtest *Official Figures\n",
      "\"JNU gets more funds than any other universities in India and yet it produces disloyal students\" JNU hostels almost like a 'Dharamshala', why the taxpayers should to be wasted on this anarchist hub JNU JNUFreebies jnuprotest JNUFeeHike\n",
      "Statue of Unity - 3000 cr Cost of printing new notes after demo b/w 2016-18 - almost 13,000 cr. Money spent by BJP on ads since 2014-2018 - over 5,200 cr But, public univs like JNU are a waste of tax payers money because poor can study there. JNUProtest *Official Figures\n",
      "Came to Delhi in 2004..worked in a call centre before taking admission in JNU. Had I not cracked the JNU entrance test, I might not have got higher education coz it was unaffordable for me. Even 5k/month was too much. JN...\n",
      "\n",
      "##Kashmir (28219 chars):\n",
      "First forceful occupation of Kashmir and now they have released illegal map of India officially, ignoring the restrictions imposed by . KashmirIssue KashmirBleeds IOK POK Jammu PNA JKPNA IndianMapsRejected\n",
      "Snowfall in Kashmir AJK 🍁\n",
      "Snowfall in Kashmir AJK 🍁\n",
      "Can india deny that the situation in Kashmir is an internationally recognized dispute on which there are number of resolutions? Can india deny that the has expressly called for the holding of an impartial referendum to ascertain the wishes of the Kashmiri people? 1\n",
      "If you have any doubt who is killing innocent laborers in kashmir and why. Read this for a glimpse. rss_hindutva_extremists IndianMapsRejectd KashmirBleeds RAW ex-chief Dulat says intel agencies bribe J-K militants, parties via\n",
      ". 450 thousand muslims were killed by RSS terrorists in Jammu region of Kashmir 1.8 million were forced to migrant from Jammu region of Indian Administrated Kashmir They have right to come back to their homes HitlerModi Jammu_Massacres Kashmir Kash...\n",
      "\n",
      "##KejriwalMustResign (10098 chars):\n",
      "Why no-body is talking or writing ✍️ about solutions rather than simply blame game. KejriwalMustResign\n",
      "KejriwalMustResign, if that solves all the problems.\n",
      "KejriwalMustResign is not the not the solution of our problems (in Delhi).\n",
      "KejriwalMustResign BJP IT cell is trending paid hashtag.\n",
      "KejriwalMustResign for using taxpayers money for taxpayers\n",
      "Truth: Uttar Pradesh town Baghpat most polluted in India, 9 out of worst ten cities of India too in BJP ruled UP. Bhakt: KejriwalMustResign.\n",
      "KejriwalMustResign ...hence KejriwalMustResign 😀\n",
      "KejriwalMustResign for not buying 191 cr worth jet for his movements.\n",
      "Kejriwal Govt is done and doing well for Delhi comparing to all states of india. but dalal Bhakt are trending it KejriwalMustResign unnecessarily Get well soon Bhakts.\n",
      "1) KejriwalMustResign Bcoz Kejriwal Stopped Modiji as CM of Gujrat from fulfilling his promises to Gujrat People . Therefore HE MUST RESIGN! 2)He must Resign क्योंकि केजरीवाल ने मोदीजी की नाक में दम कर रखा है अपने सारे वादे प...\n",
      "\n",
      "##ShivSenaCheatsMaharashtra (22514 chars):\n",
      "Uddhav Thackrey’s 4 wrong assumptions: 1. He is Balasaheb Thackrey 2. PM Narendra Modi Is PM Atal Bihari Vajpayee 3. Shiv Sena is bigger party than BJP 4. Sanjay Raut is Shiv Sena’s Amit Shah. ShivSenaCheatsMaharashtra\n",
      "Oh my goodness! Sheikh Hasina the Islamophobe. What is she talking about Rohingyas! 😂😂😂 Waiting for JNU, AMU, Jadhavpur LeLi snowflakes to meltdown. I hope they get triggered! JNU shivsenacheatsbalasaheb ShivSenaCheatsMaharashtra ShivaSena ShivaSena\n",
      "Shiv Sena quit NDA in the center even before securing support from Cong-NCP. That's like resigning from your current job before you get the offer letter for the next, and now the HR is telling you that \"Senior management is traveling\" 🤣 Maharashtra ShivSenaCheatsMaharashtra\n",
      "If BJP decides to abstain from voting, INC+NCP can form the govt even without having to entertain SS. In which case 2 more rooms can be booked at Lilawati. ShivSenaCheatsMaharashtra\n",
      "I was very excited about metro project in Pune due traffic issue in the ci...\n",
      "\n",
      "##ShutDownJNU (27741 chars):\n",
      "We are poor people. We cannot afford the hike in fee. We tweet from iPhone. All in all, we are hypocrites. jnuprotest ShutDownJNU\n",
      "Public won't mind funding college subsidies if it's actual useful courses like medicine, engineering, pure sciences etc. that actually contribute value to society instead of useless crap like gender studies and African studies and lesbian dance theory ShutDownJNU\n",
      "□ 473 Teachers □ 8309 Students □ 1276 Admin Staff □ 405 Hectares Campus □ 400 Cr/Year From Government To Produce Such Anti National goondas ??? ○ [ShutDownJNU] ॥ॐ॥\n",
      "Public won't mind funding college subsidies if it's actual useful courses like medicine, engineering, pure sciences etc. that actually contribute value to society instead of useless crap like gender studies and African studies and lesbian dance theory ShutDownJNU\n",
      "For all those running the hashtag ShutDownJNU They should hear these voices first\n",
      "JNU stands for nothing other than Sharia inspired Communism. It needs to be shit down and the la...\n",
      "\n",
      "##WhereIsAmitShah (15021 chars):\n",
      "New Chair for Maharashtra Chief Minister!😆 WhereIsAmitShah MaharashtraPoliticalCrisis MaharashtraNeedsDevendra bjp ShivSena\n",
      "For the first time in 72 years, the situation in the national capital is such that law &amp; order has become disreputable. We are seeing unprecedented protests from the police force. But HM Amit Shah is mute &amp; missing. WhereIsAmitShah\n",
      "For the first time in 72 years, the situation in the national capital is such that law &amp; order has become disreputable. We are seeing unprecedented protests from the police force. But HM Amit Shah is mute &amp; missing. WhereIsAmitShah\n",
      "WhereIsAmitShah Here is he ordering a pulis ka jawaan to sit down.\n",
      "Mr Amit Shah has completely failed the law and order situation in Delhi. Law and Order situation in Delhi is at its worst in last 70 years. WhereIsAmitShah\n",
      "Listen how uncultured Amit Shah is ordering a \"Police ka jawaan\" to sit down as if Police forces are his slaves. Shameful. WhereIsAmitShah\n",
      "For the first time in 72 years, th...\n"
     ]
    }
   ],
   "source": [
    "# Display a preview of each document\n",
    "for hashtag, doc in hashtag_documents.items():\n",
    "    preview = doc[:1000] + \"...\" if len(doc) > 200 else doc\n",
    "    print(f\"\\n#{hashtag} ({len(doc)} chars):\")\n",
    "    print(preview)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixed Language Tweets\r\n",
    "\r\n",
    "When dealing with tweets from multilingual regions like India, the language detection process faces significant challenges with mixed-language content. Here's r explanation of why Hindi phrases might slip through an English filter:\r\n",
    "\r\n",
    "## How Language Detection Works\r\n",
    "\r\n",
    "Most language detection libraries (including `langdetect` used in the notebook) work by analyzing character and word n-grams (sequences) and comparing them against statistical models of different languages. They typically:\r\n",
    "\r\n",
    "1. Look at character and word patterns\r\n",
    "2. Calculate probability scores for each language\r\n",
    "3. Choose the language with the highest probabihed text common in the Indian social media context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation Function\n",
    "\n",
    "The summary generation function takes a document and creates a concise summary using the transformer model:\n",
    "\n",
    "1. It first prepares the input by trimming very long documents and converting text to tokens\n",
    "2. The document gets processed by the encoder part of the model just once\n",
    "3. The function then generates the summary one word at a time:\n",
    "   - It looks at what it's generated so far\n",
    "   - Predicts what should come next\n",
    "   - Adds some randomness through \"temperature\" to make it more creative\n",
    "   - Adds the new word to the growing summary\n",
    "   - Continues until it decides it's complete or hits the length limit\n",
    "\n",
    "This approach is sensible because:\n",
    "- Running the encoder just once saves a lot of computation time\n",
    "- Adding words one by one mimics how humans write summaries\n",
    "- The temperature setting balances between boring, predictable text and completely random text\n",
    "- The process stops naturally when the model determines the summary is complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative batch implementation for faster generation\n",
    "def generate_summary_fast(model, tokenizer, text, max_length=50, temperature=0.7, method=\"sampling\"):\n",
    "\n",
    "    # Truncate input text if too long to avoid OOM errors\n",
    "    if len(text) > 10000:\n",
    "        text = text[:10000]\n",
    "    \n",
    "    # Tokenize the input text\n",
    "    encoding = tokenizer.encode(text, max_length=MAX_INPUT_LENGTH, padding=\"max_length\", truncation=True)\n",
    "    input_ids = torch.tensor([encoding[\"input_ids\"]]).to(device)\n",
    "    \n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Generate decoder start token\n",
    "        decoder_input_ids = torch.tensor([[tokenizer.tokenizer.bos_token_id]]).to(device)\n",
    "        \n",
    "        # Create source mask\n",
    "        src_mask = model.create_src_mask(input_ids)\n",
    "        \n",
    "        # Generate the encoding once\n",
    "        encoder_output = model.encoder(input_ids, src_mask)\n",
    "        \n",
    "        # Generate tokens step by step\n",
    "        for _ in range(max_length):\n",
    "            # Create target mask\n",
    "            tgt_mask = model.create_tgt_mask(decoder_input_ids)\n",
    "            \n",
    "            # Forward pass through the decoder\n",
    "            decoder_output = model.decoder(decoder_input_ids, encoder_output, src_mask, tgt_mask)\n",
    "            \n",
    "            # Get logits for the next token\n",
    "            next_token_logits = model.final_layer(decoder_output[:, -1])\n",
    "            \n",
    "            # Apply temperature\n",
    "            next_token_logits = next_token_logits / temperature\n",
    "            \n",
    "            # Sample from the distribution for creative generation\n",
    "            probs = F.softmax(next_token_logits, dim=-1)\n",
    "            next_token_id = torch.multinomial(probs, 1)\n",
    "            \n",
    "            # Add to sequence\n",
    "            decoder_input_ids = torch.cat([decoder_input_ids, next_token_id], dim=1)\n",
    "            \n",
    "            # Stop if EOS token is generated\n",
    "            if next_token_id.item() == tokenizer.tokenizer.eos_token_id:\n",
    "                break\n",
    "                \n",
    "        # Decode the sequence\n",
    "        output_ids = decoder_input_ids[0].cpu().tolist()\n",
    "        summary = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "        \n",
    "        return summary\n",
    "\n",
    "# Define optimized document summarization function\n",
    "def summarize_documents_optimized(hashtag_documents, model, tokenizer, save_path=\"summaries_optimized\"):\n",
    "\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    \n",
    "    # Only parameters\n",
    "    params = [\n",
    "        {\"name\": \"creative\", \"method\": \"sampling\", \"max_length\": 50, \"temperature\": 0.7}\n",
    "    ]\n",
    "    \n",
    "    # Check for existing progress\n",
    "    progress_file = os.path.join(save_path, \"progress.json\")\n",
    "    completed = {}\n",
    "    if os.path.exists(progress_file):\n",
    "        with open(progress_file, \"r\") as f:\n",
    "            completed = json.load(f)\n",
    "    \n",
    "    # Process documents\n",
    "    all_results = {}\n",
    "    \n",
    "    for i, (hashtag, document) in enumerate(hashtag_documents.items()):\n",
    "        print(f\"\\nProcessing #{hashtag} ({i+1}/{len(hashtag_documents)})...\")\n",
    "        \n",
    "        # Skip if already completed\n",
    "        if hashtag in completed and \"creative\" in completed[hashtag]:\n",
    "            print(f\"  Skipping #{hashtag} - already processed\")\n",
    "            # Load existing results\n",
    "            result_file = os.path.join(save_path, f\"{hashtag}_summaries.json\")\n",
    "            if os.path.exists(result_file):\n",
    "                with open(result_file, \"r\") as f:\n",
    "                    hashtag_results = json.load(f)\n",
    "                    all_results[hashtag] = hashtag_results\n",
    "            continue\n",
    "        \n",
    "        # Initialize results for this hashtag\n",
    "        hashtag_results = {}\n",
    "        if hashtag in completed:\n",
    "            # Load partial results\n",
    "            result_file = os.path.join(save_path, f\"{hashtag}_summaries.json\")\n",
    "            if os.path.exists(result_file):\n",
    "                with open(result_file, \"r\") as f:\n",
    "                    hashtag_results = json.load(f)\n",
    "        \n",
    "        # Process with creative parameters\n",
    "        param = params[0]  # Only one parameter set now\n",
    "        param_name = param[\"name\"]\n",
    "        \n",
    "        print(f\"  Generating {param_name} summary...\")\n",
    "        \n",
    "        try:\n",
    "            # Generate summary\n",
    "            summary = generate_summary_fast(\n",
    "                model,\n",
    "                tokenizer,\n",
    "                document,\n",
    "                max_length=param[\"max_length\"],\n",
    "                temperature=param[\"temperature\"],\n",
    "                method=param[\"method\"]\n",
    "            )\n",
    "            \n",
    "            hashtag_results[param_name] = summary\n",
    "            print(f\"    Generated summary: {len(summary.split())} words\")\n",
    "            \n",
    "            # Update progress\n",
    "            if hashtag not in completed:\n",
    "                completed[hashtag] = []\n",
    "            if param_name not in completed[hashtag]:\n",
    "                completed[hashtag].append(param_name)\n",
    "            \n",
    "            # Save progress\n",
    "            with open(progress_file, \"w\") as f:\n",
    "                json.dump(completed, f)\n",
    "            \n",
    "            # Save results for this hashtag\n",
    "            with open(os.path.join(save_path, f\"{hashtag}_summaries.json\"), \"w\") as f:\n",
    "                json.dump(hashtag_results, f, indent=2)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"    Error generating summary: {str(e)}\")\n",
    "        \n",
    "        all_results[hashtag] = hashtag_results\n",
    "    \n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting summarization process...\n",
      "\n",
      "Processing ##AYODHYAVERDICT (1/9)...\n",
      "  Skipping ##AYODHYAVERDICT - already processed\n",
      "\n",
      "Processing ##cancelallBlueTicksinIndia (2/9)...\n",
      "  Skipping ##cancelallBlueTicksinIndia - already processed\n",
      "\n",
      "Processing ##HistoryOfAyodhya (3/9)...\n",
      "  Skipping ##HistoryOfAyodhya - already processed\n",
      "\n",
      "Processing ##jnuprotest (4/9)...\n",
      "  Skipping ##jnuprotest - already processed\n",
      "\n",
      "Processing ##Kashmir (5/9)...\n",
      "  Skipping ##Kashmir - already processed\n",
      "\n",
      "Processing ##KejriwalMustResign (6/9)...\n",
      "  Skipping ##KejriwalMustResign - already processed\n",
      "\n",
      "Processing ##ShivSenaCheatsMaharashtra (7/9)...\n",
      "  Skipping ##ShivSenaCheatsMaharashtra - already processed\n",
      "\n",
      "Processing ##ShutDownJNU (8/9)...\n",
      "  Skipping ##ShutDownJNU - already processed\n",
      "\n",
      "Processing ##WhereIsAmitShah (9/9)...\n",
      "  Skipping ##WhereIsAmitShah - already processed\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting summarization process...\")\n",
    "summaries_optimized = summarize_documents_optimized(hashtag_documents, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Creative Summaries:\n",
      "\n",
      "\n",
      "==================================================\n",
      "HASHTAG: ##AYODHYAVERDICT\n",
      "==================================================\n",
      "\n",
      "--- Summary ---\n",
      "We continue to live in peace and harmony ; we respect the Hon our able Supreme Court . This can happen only in India . We pray for peace , love and harmony A YO D H Y AV D IC T ' There is a profound reason Mah ar\n",
      "[Word count: 50]\n",
      "\n",
      "\n",
      "==================================================\n",
      "HASHTAG: ##cancelallBlueTicksinIndia\n",
      "==================================================\n",
      "\n",
      "--- Summary ---\n",
      "Facebook has been accused of encouraging cast e discrimination by sup pressing the voices of the marginal ised . The trend surfaced in connection with allegations of Twitter encouraging cast e discrimination by sup pressing the voices of the marginal ised . The issue is being brought in in by\n",
      "[Word count: 50]\n",
      "\n",
      "\n",
      "==================================================\n",
      "HASHTAG: ##HistoryOfAyodhya\n",
      "==================================================\n",
      "\n",
      "--- Summary ---\n",
      "History Of Ram Mand ir History Of Ay od hy a fought with the M ugh als . After losing his empire in 15 80 AD , Akbar ' s empire divided into 12 sub as . Temple of Sh ri in Ram Hidden , was destroyed by Ma ula\n",
      "[Word count: 50]\n",
      "\n",
      "\n",
      "==================================================\n",
      "HASHTAG: ##jnuprotest\n",
      "==================================================\n",
      "\n",
      "--- Summary ---\n",
      "B JP on ads spent by B JP on ads since 2014 - over 5 , 200 cr But , public un iv s like J N U are a waste of tax payers money . J N U is a waste of tax payers money because poor can study\n",
      "[Word count: 50]\n",
      "\n",
      "\n",
      "==================================================\n",
      "HASHTAG: ##Kashmir\n",
      "==================================================\n",
      "\n",
      "--- Summary ---\n",
      "Kashmir is an internationally recognized dispute on which there are number of resolutions . The dispute has outraged Indian AirAsia and Indian military forces . Kashmir is under siege and curfew , according to court papers . The clamp down is nearing 3 months . Kashmir has refused to face\n",
      "[Word count: 50]\n",
      "\n",
      "\n",
      "==================================================\n",
      "HASHTAG: ##KejriwalMustResign\n",
      "==================================================\n",
      "\n",
      "--- Summary ---\n",
      "Ke j ri wal Must Res ign has been trending on paid hashtag . Ke j ri wal Must Res ign is the only one affected by the problems . Delhi C in fin a is also a state of issues affecting the country . Delhi is the worst affected\n",
      "[Word count: 50]\n",
      "\n",
      "\n",
      "==================================================\n",
      "HASHTAG: ##ShivSenaCheatsMaharashtra\n",
      "==================================================\n",
      "\n",
      "--- Summary ---\n",
      "J N aren dra Modi is asking you Sh iv Sen a Che ats Mah ar ash tra to end his career . He is the leader of the Islamic State of the Islamic Union . Sh iv Sen a is bigger than B JP 4 . He b Sh\n",
      "[Word count: 50]\n",
      "\n",
      "\n",
      "==================================================\n",
      "HASHTAG: ##ShutDownJNU\n",
      "==================================================\n",
      "\n",
      "--- Summary ---\n",
      "Shut Down J N U Public won ' t mind funding college subsidies for university subsidies . Shut Down J N U Public Public Hut 8 30 9 Students 12 76 Ad min Staff 405 He ct ares Campus 400 Cr / Year From Government\n",
      "[Word count: 45]\n",
      "\n",
      "\n",
      "==================================================\n",
      "HASHTAG: ##WhereIsAmitShah\n",
      "==================================================\n",
      "\n",
      "--- Summary ---\n",
      "HM A mit Shah Where Is A mit Shah has completely failed the law and order situation in Delhi . ' Police ka j awa an \" to sit down as if police forces are his slaves . Where Is A mit Shah ? Take this quiz and find out\n",
      "[Word count: 50]\n",
      "All creative summaries saved to creative_summaries.txt\n"
     ]
    }
   ],
   "source": [
    "def display_optimized_summaries(summaries):\n",
    "\n",
    "    for hashtag, hashtag_summaries in summaries.items():\n",
    "        print(f\"\\n\\n{'=' * 50}\")\n",
    "        print(f\"HASHTAG: #{hashtag}\")\n",
    "        print(f\"{'=' * 50}\")\n",
    "        \n",
    "        if \"creative\" in hashtag_summaries:\n",
    "            summary = hashtag_summaries[\"creative\"]\n",
    "            print(f\"\\n--- Summary ---\")\n",
    "            print(f\"{summary}\")\n",
    "            print(f\"[Word count: {len(summary.split())}]\")\n",
    "\n",
    "print(\"\\nGenerated Creative Summaries:\")\n",
    "display_optimized_summaries(summaries_optimized)\n",
    "\n",
    "\n",
    "def save_combined_summaries(summaries, filename=\"creative_summaries.txt\"):\n",
    "\n",
    "    with open(filename, \"w\") as f:\n",
    "        for hashtag, hashtag_summaries in summaries.items():\n",
    "            f.write(f\"\\n\\n{'=' * 50}\\n\")\n",
    "            f.write(f\"HASHTAG: #{hashtag}\\n\")\n",
    "            f.write(f\"{'=' * 50}\\n\\n\")\n",
    "            \n",
    "            if \"creative\" in hashtag_summaries:\n",
    "                summary = hashtag_summaries[\"creative\"]\n",
    "                f.write(f\"\\n--- Creative Summary ---\\n\")\n",
    "                f.write(f\"{summary}\\n\")\n",
    "                f.write(f\"[Word count: {len(summary.split())}]\\n\")\n",
    "    \n",
    "    print(f\"All creative summaries saved to {filename}\")\n",
    "\n",
    "save_combined_summaries(summaries_optimized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Token Splitting Explanation\n",
    "\n",
    "The weird spacing in summaries (like \"A YO D H Y A\" instead of \"AYODHYA\") happens because:\n",
    "\n",
    "1. The model doesn't understand words as whole units - it breaks them into pieces called \"tokens\"\n",
    "2. Common words like \"the\" or \"and\" get their own tokens\n",
    "3. Uncommon words, especially hashtags like \"AYODHYAVERDICT,\" get broken into smaller chunks\n",
    "4. When converting the model's output back to text, each token gets separated by a space\n",
    "\n",
    "This happens because:\n",
    "- The model has a limited vocabulary of 32,000 tokens\n",
    "- It can't possibly include every possible word, especially unusual hashtags\n",
    "- Breaking words into smaller pieces lets it handle any word it encounters\n",
    "- This approach is more efficient than trying to include every possible word\n",
    "- The downside is that when these pieces get put back together, they show up with spaces between them\n",
    "\n",
    "This is particularly noticeable with hashtags and unusual proper nouns that weren't common in the model's training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creative Summary Statistics:\n",
      "Average Word Count: 49.4\n",
      "Min Word Count: 45\n",
      "Max Word Count: 50\n",
      "Average Character Length: 225.3\n"
     ]
    }
   ],
   "source": [
    "def analyze_optimized_summaries(summaries):\n",
    "    \n",
    "    # Calculate length statistics\n",
    "    creative_stats = {\n",
    "        \"word_counts\": [],\n",
    "        \"char_lengths\": []\n",
    "    }\n",
    "    \n",
    "    # Collect data\n",
    "    for hashtag, hashtag_summaries in summaries.items():\n",
    "        if \"creative\" in hashtag_summaries:\n",
    "            summary = hashtag_summaries[\"creative\"]\n",
    "            word_count = len(summary.split())\n",
    "            char_length = len(summary)\n",
    "            creative_stats[\"word_counts\"].append(word_count)\n",
    "            creative_stats[\"char_lengths\"].append(char_length)\n",
    "    \n",
    "    # Calculate statistics\n",
    "    if creative_stats[\"word_counts\"]:\n",
    "        avg_words = sum(creative_stats[\"word_counts\"]) / len(creative_stats[\"word_counts\"])\n",
    "        avg_chars = sum(creative_stats[\"char_lengths\"]) / len(creative_stats[\"char_lengths\"])\n",
    "        min_words = min(creative_stats[\"word_counts\"])\n",
    "        max_words = max(creative_stats[\"word_counts\"])\n",
    "    else:\n",
    "        avg_words = 0\n",
    "        avg_chars = 0\n",
    "        min_words = 0\n",
    "        max_words = 0\n",
    "    \n",
    "    # Print statistics\n",
    "    print(\"\\nCreative Summary Statistics:\")\n",
    "    print(f\"Average Word Count: {avg_words:.1f}\")\n",
    "    print(f\"Min Word Count: {min_words}\")\n",
    "    print(f\"Max Word Count: {max_words}\")\n",
    "    print(f\"Average Character Length: {avg_chars:.1f}\")\n",
    "    \n",
    "    return {\n",
    "        \"avg_words\": avg_words,\n",
    "        \"min_words\": min_words,\n",
    "        \"max_words\": max_words,\n",
    "        \"avg_chars\": avg_chars\n",
    "    }\n",
    "\n",
    "if summaries_optimized:\n",
    "    summary_stats = analyze_optimized_summaries(summaries_optimized)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "isSourceIdPinned": true,
     "modelId": 308155,
     "modelInstanceId": 287353,
     "sourceId": 343622,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
